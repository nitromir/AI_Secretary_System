# app/routers/llm.py
"""LLM configuration router - backend switching, personas, providers, params."""

import asyncio
import logging
import os
from pathlib import Path
from typing import Dict, Optional

import httpx
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel

from app.dependencies import get_container
from auth_manager import User, get_current_user, require_admin, require_not_guest
from cloud_llm_service import PROVIDER_TYPES, CloudLLMService, GeminiProvider
from db.integration import async_audit_logger, async_cloud_provider_manager
from llm_service import LLMService
from service_manager import get_service_manager


logger = logging.getLogger(__name__)

router = APIRouter(prefix="/admin/llm", tags=["llm"])


# ============== Pydantic Models ==============


class AdminLLMPromptRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""

    prompt: str


class AdminLLMModelRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ LLM"""

    model: str  # gemini-2.5-flash, gemini-2.5-pro


class AdminBackendRequest(BaseModel):
    backend: str  # "vllm", "gemini", or "cloud:{provider_id}"
    stop_unused: bool = False  # –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è GPU


class CloudProviderCreate(BaseModel):
    """Create cloud LLM provider"""

    name: str
    provider_type: str  # gemini, kimi, openai, claude, deepseek, custom
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    model_name: str = ""
    enabled: bool = True
    is_default: bool = False
    config: Optional[Dict] = None
    description: Optional[str] = None


class CloudProviderUpdate(BaseModel):
    """Update cloud LLM provider"""

    name: Optional[str] = None
    provider_type: Optional[str] = None
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    model_name: Optional[str] = None
    enabled: Optional[bool] = None
    is_default: Optional[bool] = None
    config: Optional[Dict] = None
    description: Optional[str] = None


class AdminPersonaRequest(BaseModel):
    persona: str  # "anna" or "marina"


class AdminLLMParamsRequest(BaseModel):
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    repetition_penalty: Optional[float] = None


class ProxyTestRequest(BaseModel):
    """Request to test VLESS proxy connection"""

    vless_url: str


class LLMPresetCreate(BaseModel):
    """Create LLM preset"""

    id: str
    name: str
    description: Optional[str] = None
    system_prompt: Optional[str] = None
    temperature: float = 0.7
    max_tokens: int = 512
    top_p: float = 0.9
    repetition_penalty: float = 1.1


class LLMPresetUpdate(BaseModel):
    """Update LLM preset"""

    name: Optional[str] = None
    description: Optional[str] = None
    system_prompt: Optional[str] = None
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    repetition_penalty: Optional[float] = None


class VLLMModelRequest(BaseModel):
    """Request to change vLLM model"""

    model: str  # Full model name, e.g., "Qwen/Qwen2.5-7B-Instruct-AWQ"


# ============== Helper Functions ==============


async def _switch_to_cloud_provider(provider_id: str, stop_unused: bool, user: User):
    """Helper function to switch to a cloud provider"""
    container = get_container()

    provider_config = await async_cloud_provider_manager.get_provider_with_key(provider_id)
    if not provider_config:
        raise HTTPException(status_code=404, detail=f"Provider {provider_id} not found")

    if not provider_config.get("enabled"):
        raise HTTPException(status_code=400, detail=f"Provider {provider_id} is disabled")

    is_bridge = provider_config.get("provider_type") == "claude_bridge"

    # Bridge doesn't need API key (uses local claude CLI auth)
    if not is_bridge and not provider_config.get("api_key"):
        raise HTTPException(
            status_code=400, detail=f"Provider {provider_id} has no API key configured"
        )

    try:
        # Auto-start bridge if provider_type is claude_bridge
        if is_bridge:
            from bridge_manager import bridge_manager

            if not bridge_manager.is_running:
                config = provider_config.get("config") or {}
                port = config.get("bridge_port", 8787)
                permission = config.get("permission_level", "chat")
                result = await bridge_manager.start(port=port, permission_level=permission)
                if result.get("status") != "ok":
                    raise HTTPException(
                        status_code=503,
                        detail=f"Bridge failed to start: {result.get('error')}",
                    )

            # Override base_url to actual bridge URL
            provider_config["base_url"] = bridge_manager.get_base_url()
            # Bridge doesn't need API key ‚Äî set dummy for OpenAICompatibleProvider
            if not provider_config.get("api_key"):
                provider_config["api_key"] = "bridge-local"

        new_service = CloudLLMService(provider_config)
        if not new_service.is_available():
            raise HTTPException(status_code=503, detail=f"Provider {provider_id} is not responding")

        # Stop bridge if switching away from claude_bridge
        current_service = container.llm_service
        if (
            current_service
            and getattr(current_service, "provider_type", None) == "claude_bridge"
            and not is_bridge
        ):
            from bridge_manager import bridge_manager

            if bridge_manager.is_running:
                logger.info("üõë Stopping bridge (switching to another provider)...")
                await bridge_manager.stop()

        container.llm_service = new_service
        os.environ["LLM_BACKEND"] = f"cloud:{provider_id}"

        # Optionally stop vLLM to free GPU
        if stop_unused:
            manager = get_service_manager()
            vllm_status = manager.get_service_status("vllm")
            if vllm_status.get("is_running"):
                logger.info("üõë Stopping vLLM to free GPU memory...")
                await manager.stop_service("vllm")

        logger.info(f"‚úÖ Switched to cloud provider: {provider_config.get('name')}")

        # Audit log
        await async_audit_logger.log(
            action="update",
            resource="config",
            resource_id="llm_backend",
            user_id=user.username,
            details={
                "backend": f"cloud:{provider_id}",
                "provider_type": provider_config.get("provider_type"),
                "model": provider_config.get("model_name"),
            },
        )

        return {
            "status": "ok",
            "backend": f"cloud:{provider_id}",
            "provider_id": provider_id,
            "provider_type": provider_config.get("provider_type"),
            "model": provider_config.get("model_name"),
            "message": f"Switched to cloud provider: {provider_config.get('name')}",
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error switching to cloud provider: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============== Prompt Endpoints ==============


@router.get("/prompt")
async def admin_get_llm_prompt(user: User = Depends(get_current_user)):
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç LLM"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service:
        persona = getattr(llm_service, "current_persona", None) or os.getenv(
            "SECRETARY_PERSONA", "anna"
        )
        return {
            "prompt": llm_service.system_prompt,
            "model": llm_service.model_name,
            "persona": persona,
        }
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@router.post("/prompt")
async def admin_set_llm_prompt(
    request: AdminLLMPromptRequest, user: User = Depends(require_not_guest)
):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–æ–≤—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç LLM"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service:
        llm_service.set_system_prompt(request.prompt)
        return {
            "status": "ok",
            "prompt": request.prompt[:100] + "..." if len(request.prompt) > 100 else request.prompt,
        }
    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== Model Endpoints ==============


@router.get("/model")
async def admin_get_llm_model(user: User = Depends(get_current_user)):
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å LLM"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service:
        return {"model": llm_service.model_name}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@router.post("/model")
async def admin_set_llm_model(
    request: AdminLLMModelRequest, user: User = Depends(require_not_guest)
):
    """–ò–∑–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å LLM"""
    allowed_models = ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash"]

    if request.model not in allowed_models:
        raise HTTPException(
            status_code=400,
            detail=f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {request.model}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {allowed_models}",
        )

    container = get_container()
    llm_service = container.llm_service
    if llm_service:
        try:
            llm_service.set_model(request.model)
            return {"status": "ok", "model": request.model}
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== History Endpoints ==============


@router.delete("/history")
async def admin_clear_llm_history(user: User = Depends(require_not_guest)):
    """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ LLM"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service:
        count = len(llm_service.conversation_history)
        llm_service.reset_conversation()
        return {"status": "ok", "cleared_messages": count}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@router.get("/history")
async def admin_get_llm_history(user: User = Depends(get_current_user)):
    """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ LLM"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service:
        return {
            "history": llm_service.conversation_history,
            "count": len(llm_service.conversation_history),
        }
    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== Backend Endpoints ==============


@router.get("/backend")
async def admin_get_llm_backend(user: User = Depends(get_current_user)):
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π LLM backend"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service:
        # Detect backend type
        if isinstance(llm_service, CloudLLMService):
            backend = f"cloud:{llm_service.provider_id}"
        elif hasattr(llm_service, "api_url"):
            backend = "vllm"
        else:
            backend = "gemini"

        return {
            "backend": backend,
            "model": getattr(llm_service, "model_name", "unknown"),
            "api_url": getattr(llm_service, "api_url", None),
            "provider_type": getattr(llm_service, "provider_type", None),
        }
    return {"backend": "none", "error": "LLM service not initialized"}


@router.get("/models")
async def admin_get_llm_models(user: User = Depends(get_current_user)):
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π vLLM –∏ —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ Qwen, Llama, DeepSeek –∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª—è—Ö.
    """
    from vllm_llm_service import AVAILABLE_MODELS

    container = get_container()
    llm_service = container.llm_service
    result = {
        "available_models": AVAILABLE_MODELS,
        "current_model": None,
        "loaded_models": [],
        "backend": "none",
    }

    if llm_service:
        is_vllm = hasattr(llm_service, "api_url")
        result["backend"] = "vllm" if is_vllm else "gemini"

        if is_vllm and hasattr(llm_service, "get_current_model_info"):
            result["current_model"] = llm_service.get_current_model_info()
            result["loaded_models"] = llm_service.get_loaded_models()
        elif not is_vllm:
            # Gemini backend
            result["current_model"] = {
                "id": "gemini",
                "name": getattr(llm_service, "model_name", "gemini-2.0-flash"),
                "description": "Google Gemini API",
                "available": True,
            }

    return result


@router.post("/backend")
async def admin_set_llm_backend(
    request: AdminBackendRequest, user: User = Depends(get_current_user)
):
    """–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å LLM backend —Å –≥–æ—Ä—è—á–µ–π –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–æ–π —Å–µ—Ä–≤–∏—Å–∞"""
    container = get_container()

    # Stop bridge if currently running and switching away from it
    current_service = container.llm_service
    if current_service and getattr(current_service, "provider_type", None) == "claude_bridge":
        from bridge_manager import bridge_manager

        if bridge_manager.is_running:
            logger.info("üõë Stopping bridge (switching backend)...")
            await bridge_manager.stop()

    # Check if it's a cloud provider
    if request.backend.startswith("cloud:"):
        provider_id = request.backend.split(":", 1)[1]
        return await _switch_to_cloud_provider(provider_id, request.stop_unused, user)

    if request.backend not in ["vllm", "gemini"]:
        raise HTTPException(
            status_code=400,
            detail="Invalid backend. Use 'vllm', 'gemini', or 'cloud:{provider_id}'",
        )

    llm_service = container.llm_service

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –±—ç–∫–µ–Ω–¥ –ø—Ä–∞–≤–∏–ª—å–Ω–æ (cloud, vllm, gemini)
    if llm_service and getattr(llm_service, "backend_type", None) == "cloud":
        current_backend = f"cloud:{getattr(llm_service, 'provider_id', 'unknown')}"
    elif (
        llm_service and hasattr(llm_service, "api_url") and not hasattr(llm_service, "backend_type")
    ):
        current_backend = "vllm"
    else:
        current_backend = "gemini"

    if request.backend == current_backend:
        return {
            "status": "ok",
            "backend": request.backend,
            "message": f"–£–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {request.backend}",
        }

    manager = get_service_manager()

    try:
        if request.backend == "vllm":
            # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ vLLM
            logger.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ vLLM...")

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º URL –¥–ª—è vLLM (–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º - —É–¥–∞–ª—è–µ–º trailing /v1)
            vllm_url = os.getenv("VLLM_API_URL", "http://localhost:11434").rstrip("/")
            if vllm_url.endswith("/v1"):
                vllm_url = vllm_url[:-3]
            is_docker = Path("/.dockerenv").exists() or os.getenv("DOCKER_CONTAINER") == "1"

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å vLLM
            async def check_vllm_health() -> bool:
                try:
                    async with httpx.AsyncClient() as client:
                        # vLLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç /v1/models –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
                        try:
                            resp = await client.get(f"{vllm_url}/v1/models", timeout=5.0)
                            if resp.status_code == 200:
                                return True
                        except Exception:
                            pass
                except Exception:
                    pass
                return False

            vllm_accessible = await check_vllm_health()

            if not vllm_accessible:
                # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å vLLM (–∏ –≤ Docker, –∏ –ª–æ–∫–∞–ª—å–Ω–æ)
                vllm_status = manager.get_service_status("vllm")

                if not vllm_status.get("is_running"):
                    logger.info("üöÄ –ó–∞–ø—É—Å–∫ vLLM...")
                    start_result = await manager.start_service("vllm")
                    if start_result.get("status") != "ok":
                        raise HTTPException(
                            status_code=503,
                            detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å vLLM: {start_result.get('message', 'Unknown error')}",
                        )

                # –ñ–¥—ë–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ vLLM (–¥–æ 180 —Å–µ–∫—É–Ω–¥ –¥–ª—è Docker, —Ç.–∫. –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏)
                max_attempts = 90 if is_docker else 60  # 180 –∏–ª–∏ 120 —Å–µ–∫—É–Ω–¥
                logger.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ vLLM (–¥–æ {max_attempts * 2} —Å–µ–∫)...")

                for i in range(max_attempts):
                    await asyncio.sleep(2)
                    if await check_vllm_health():
                        logger.info(f"‚úÖ vLLM –≥–æ—Ç–æ–≤ (–ø–æ–ø—ã—Ç–∫–∞ {i + 1})")
                        break
                else:
                    raise HTTPException(
                        status_code=503,
                        detail=f"vLLM –Ω–µ —Å—Ç–∞–ª –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É {vllm_url}. "
                        "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞: docker logs ai-secretary-vllm",
                    )

            # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π vLLM —Å–µ—Ä–≤–∏—Å
            try:
                from vllm_llm_service import VLLMLLMService

                new_service = VLLMLLMService()
                if not new_service.is_available():
                    raise HTTPException(
                        status_code=503, detail="vLLM –∑–∞–ø—É—â–µ–Ω, –Ω–æ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ API"
                    )
            except ImportError:
                raise HTTPException(status_code=503, detail="VLLMLLMService –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")

            container.llm_service = new_service
            os.environ["LLM_BACKEND"] = "vllm"

            logger.info("‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ vLLM")

            # Audit log
            await async_audit_logger.log(
                action="update",
                resource="config",
                resource_id="llm_backend",
                user_id=user.username,
                details={"backend": "vllm", "model": getattr(new_service, "model_name", "unknown")},
            )

            return {
                "status": "ok",
                "backend": "vllm",
                "model": getattr(new_service, "model_name", "unknown"),
                "message": "–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ vLLM",
            }

        else:
            # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ Gemini
            logger.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ Gemini...")

            new_service = LLMService()
            container.llm_service = new_service
            os.environ["LLM_BACKEND"] = "gemini"

            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º vLLM –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è GPU
            if request.stop_unused:
                vllm_status = manager.get_service_status("vllm")
                if vllm_status.get("is_running"):
                    logger.info("üõë –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º vLLM –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è GPU...")
                    await manager.stop_service("vllm")

            logger.info("‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ Gemini")

            # Audit log
            await async_audit_logger.log(
                action="update",
                resource="config",
                resource_id="llm_backend",
                user_id=user.username,
                details={
                    "backend": "gemini",
                    "model": getattr(new_service, "model_name", "unknown"),
                },
            )

            return {
                "status": "ok",
                "backend": "gemini",
                "model": getattr(new_service, "model_name", "unknown"),
                "message": "–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ Gemini"
                + (" (vLLM –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)" if request.stop_unused else ""),
            }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –±—ç–∫–µ–Ω–¥–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============== vLLM Model Selection ==============


@router.get("/vllm-model")
async def admin_get_vllm_model(user: User = Depends(require_admin)):
    """
    –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å vLLM.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ –ë–î –∏–ª–∏ env var.
    """
    from db.database import get_session_context
    from db.repositories.config import ConfigRepository

    # –ü—Ä–æ–±—É–µ–º –∏–∑ –ë–î
    try:
        async with get_session_context() as session:
            config_repo = ConfigRepository(session)
            llm_config = await config_repo.get_llm_config()
            db_model = llm_config.get("vllm_model")
    except Exception:
        db_model = None

    # Fallback –Ω–∞ env var
    env_model = os.getenv("VLLM_MODEL", "Qwen/Qwen2.5-7B-Instruct-AWQ")

    return {
        "model": db_model or env_model,
        "source": "db" if db_model else "env",
        "env_model": env_model,
    }


@router.post("/vllm-model")
async def admin_set_vllm_model(request: VLLMModelRequest, user: User = Depends(require_admin)):
    """
    –ò–∑–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å vLLM –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä.

    –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ –ë–î –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–ø—É—Å–∫–∞—Ö.
    –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä vLLM —É–¥–∞–ª—è–µ—Ç—Å—è –∏ —Å–æ–∑–¥–∞—ë—Ç—Å—è –∑–∞–Ω–æ–≤–æ —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é.

    –ü—Ä–∏–º–µ—Ä—ã –º–æ–¥–µ–ª–µ–π:
    - Qwen/Qwen2.5-7B-Instruct-AWQ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    - deepseek-ai/deepseek-llm-7b-chat
    - meta-llama/Llama-3.1-8B-Instruct
    """
    manager = get_service_manager()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º—ã –≤ Docker —Ä–µ–∂–∏–º–µ
    is_docker = Path("/.dockerenv").exists() or os.getenv("DOCKER_CONTAINER") == "1"
    if not is_docker:
        raise HTTPException(
            status_code=400,
            detail="–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ vLLM –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ Docker —Ä–µ–∂–∏–º–µ",
        )

    logger.info(f"üîÑ –ó–∞–ø—Ä–æ—Å –Ω–∞ —Å–º–µ–Ω—É –º–æ–¥–µ–ª–∏ vLLM: {request.model}")

    try:
        result = await manager.switch_vllm_model(request.model)

        if result.get("status") == "ok":
            # Audit log
            await async_audit_logger.log(
                action="update",
                resource="config",
                resource_id="vllm_model",
                user_id=user.username,
                details={"model": request.model},
            )

            return {
                "status": "ok",
                "model": request.model,
                "message": f"–ú–æ–¥–µ–ª—å vLLM –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞ {request.model}. –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–π–º—ë—Ç 2-3 –º–∏–Ω—É—Ç—ã.",
                "container_id": result.get("container_id"),
            }
        else:
            raise HTTPException(
                status_code=500, detail=result.get("message", "–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è")
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–º–µ–Ω—ã –º–æ–¥–µ–ª–∏ vLLM: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============== Cloud Providers Endpoints ==============


@router.get("/providers")
async def admin_list_cloud_providers(
    enabled_only: bool = False, user: User = Depends(get_current_user)
):
    """List all cloud LLM providers"""
    owner_id = None if user.role == "admin" else user.id
    providers = await async_cloud_provider_manager.list_providers(enabled_only, owner_id=owner_id)
    return {
        "providers": providers,
        "provider_types": PROVIDER_TYPES,
    }


@router.get("/providers/{provider_id}")
async def admin_get_cloud_provider(
    provider_id: str, include_key: bool = False, user: User = Depends(get_current_user)
):
    """Get cloud provider by ID"""
    owner_id = None if user.role == "admin" else user.id
    if include_key:
        provider = await async_cloud_provider_manager.get_provider_with_key(provider_id)
    else:
        provider = await async_cloud_provider_manager.get_provider(provider_id, owner_id=owner_id)

    if not provider:
        raise HTTPException(status_code=404, detail="Provider not found")
    return {"provider": provider}


@router.post("/providers")
async def admin_create_cloud_provider(
    data: CloudProviderCreate, user: User = Depends(get_current_user)
):
    """Create new cloud LLM provider"""
    owner_id = None if user.role == "admin" else user.id
    try:
        provider = await async_cloud_provider_manager.create_provider(
            name=data.name,
            provider_type=data.provider_type,
            api_key=data.api_key,
            base_url=data.base_url,
            model_name=data.model_name,
            enabled=data.enabled,
            is_default=data.is_default,
            config=data.config,
            description=data.description,
            owner_id=owner_id,
        )

        # Audit log
        await async_audit_logger.log(
            action="create",
            resource="cloud_provider",
            resource_id=provider["id"],
            user_id=user.username,
            details={"name": data.name, "provider_type": data.provider_type},
        )

        return {"status": "ok", "provider": provider}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.put("/providers/{provider_id}")
async def admin_update_cloud_provider(
    provider_id: str, data: CloudProviderUpdate, user: User = Depends(get_current_user)
):
    """Update cloud LLM provider"""
    # Verify ownership
    owner_id = None if user.role == "admin" else user.id
    existing = await async_cloud_provider_manager.get_provider(provider_id, owner_id=owner_id)
    if not existing:
        raise HTTPException(status_code=404, detail="Provider not found")

    # Filter out None values
    update_data = {k: v for k, v in data.model_dump().items() if v is not None}

    provider = await async_cloud_provider_manager.update_provider(provider_id, **update_data)
    if not provider:
        raise HTTPException(status_code=404, detail="Provider not found")

    # Audit log
    await async_audit_logger.log(
        action="update",
        resource="cloud_provider",
        resource_id=provider_id,
        user_id=user.username,
        details=update_data,
    )

    return {"status": "ok", "provider": provider}


@router.delete("/providers/{provider_id}")
async def admin_delete_cloud_provider(provider_id: str, user: User = Depends(get_current_user)):
    """Delete cloud LLM provider"""
    owner_id = None if user.role == "admin" else user.id
    if not await async_cloud_provider_manager.delete_provider(provider_id, owner_id=owner_id):
        raise HTTPException(status_code=404, detail="Provider not found")

    # Audit log
    await async_audit_logger.log(
        action="delete",
        resource="cloud_provider",
        resource_id=provider_id,
        user_id=user.username,
    )

    return {"status": "ok", "message": f"Provider {provider_id} deleted"}


@router.post("/providers/{provider_id}/test")
async def admin_test_cloud_provider(provider_id: str, user: User = Depends(get_current_user)):
    """Test cloud provider connection"""
    # Verify ownership before testing
    owner_id = None if user.role == "admin" else user.id
    if not await async_cloud_provider_manager.get_provider(provider_id, owner_id=owner_id):
        raise HTTPException(status_code=404, detail="Provider not found")
    provider_config = await async_cloud_provider_manager.get_provider_with_key(provider_id)
    if not provider_config:
        raise HTTPException(status_code=404, detail="Provider not found")

    is_bridge = provider_config.get("provider_type") == "claude_bridge"

    if not is_bridge and not provider_config.get("api_key"):
        return {
            "status": "error",
            "available": False,
            "message": "No API key configured",
        }

    # For bridge: ensure it's running and set URL
    if is_bridge:
        from bridge_manager import bridge_manager

        if not bridge_manager.is_running:
            return {
                "status": "error",
                "available": False,
                "message": "Bridge is not running. Start it first.",
            }
        provider_config["base_url"] = bridge_manager.get_base_url()
        if not provider_config.get("api_key"):
            provider_config["api_key"] = "bridge-local"

    try:
        service = CloudLLMService(provider_config)
        is_available = service.is_available()

        if is_available:
            # Quick test generation
            test_response = service.generate_response("–°–∫–∞–∂–∏ '—Ç–µ—Å—Ç –æ–∫'", use_history=False)
            return {
                "status": "ok",
                "available": True,
                "test_response": test_response[:200] if test_response else "",
            }
        else:
            return {
                "status": "error",
                "available": False,
                "message": "Provider not responding",
            }
    except Exception as e:
        return {
            "status": "error",
            "available": False,
            "message": str(e),
        }


@router.post("/providers/{provider_id}/set-default")
async def admin_set_default_cloud_provider(
    provider_id: str, user: User = Depends(get_current_user)
):
    """Set cloud provider as default"""
    # Verify ownership
    owner_id = None if user.role == "admin" else user.id
    if not await async_cloud_provider_manager.get_provider(provider_id, owner_id=owner_id):
        raise HTTPException(status_code=404, detail="Provider not found")

    if not await async_cloud_provider_manager.set_default(provider_id):
        raise HTTPException(status_code=404, detail="Provider not found or disabled")

    await async_audit_logger.log(
        action="update",
        resource="cloud_provider",
        resource_id=provider_id,
        user_id=user.username,
        details={"is_default": True},
    )

    return {"status": "ok", "message": f"Provider {provider_id} set as default"}


# ============== Persona Endpoints ==============


@router.get("/personas")
async def admin_get_personas(user: User = Depends(get_current_user)):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω (–∏–∑ –ë–î)"""
    from db.database import get_session_context
    from db.repositories.llm_preset import LLMPresetRepository

    async with get_session_context() as session:
        repo = LLMPresetRepository(session)
        await repo.ensure_defaults()
        presets = await repo.get_all_enabled()
        return {
            "personas": {
                p.id: {"name": p.name, "full_name": p.name, "description": p.description}
                for p in presets
            }
        }


@router.get("/persona")
async def admin_get_current_persona(user: User = Depends(get_current_user)):
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –ø–µ—Ä—Å–æ–Ω—É (–∏–∑ –ë–î)"""
    from db.database import get_session_context
    from db.repositories.llm_preset import LLMPresetRepository

    container = get_container()
    llm_service = container.llm_service
    current_id = "anna"
    if llm_service and hasattr(llm_service, "persona_id"):
        current_id = llm_service.persona_id

    async with get_session_context() as session:
        repo = LLMPresetRepository(session)
        await repo.ensure_defaults()
        preset = await repo.get_by_id(current_id)
        if preset:
            return {"id": preset.id, "name": preset.name}

        # Fallback to default
        default = await repo.get_default()
        if default:
            return {"id": default.id, "name": default.name}

    return {"id": "none", "error": "No preset found"}


@router.post("/persona")
async def admin_set_persona(request: AdminPersonaRequest, user: User = Depends(get_current_user)):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–µ—Ä—Å–æ–Ω—É (–∏–∑ –ë–î)"""
    from db.database import get_session_context
    from db.repositories.llm_preset import LLMPresetRepository

    async with get_session_context() as session:
        repo = LLMPresetRepository(session)
        preset = await repo.get_by_id(request.persona)
        if not preset:
            raise HTTPException(status_code=404, detail=f"Persona not found: {request.persona}")

        # Set as default in DB
        await repo.set_default(request.persona)

        # Update llm_service with DB data
        container = get_container()
        llm_service = container.llm_service
        if llm_service and hasattr(llm_service, "set_persona"):
            persona_data = {
                "name": preset.name,
                "description": preset.description,
                "system_prompt": preset.system_prompt,
                "temperature": preset.temperature,
                "max_tokens": preset.max_tokens,
                "top_p": preset.top_p,
                "repetition_penalty": preset.repetition_penalty,
            }
            llm_service.set_persona(request.persona, persona_data=persona_data)

        # Audit log
        await async_audit_logger.log(
            action="update",
            resource="config",
            resource_id="llm_persona",
            user_id=user.username,
            details={"persona": request.persona},
        )
        return {"status": "ok", "persona": request.persona}


# ============== Params Endpoints ==============


@router.get("/params")
async def admin_get_llm_params(user: User = Depends(get_current_user)):
    """–ü–æ–ª—É—á–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service and hasattr(llm_service, "runtime_params"):
        return {"params": llm_service.runtime_params}

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    return {
        "params": {"temperature": 0.7, "max_tokens": 512, "top_p": 0.9, "repetition_penalty": 1.1}
    }


@router.post("/params")
async def admin_set_llm_params(
    request: AdminLLMParamsRequest, user: User = Depends(require_not_guest)
):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service and hasattr(llm_service, "set_params"):
        params = {k: v for k, v in request.model_dump().items() if v is not None}
        llm_service.set_params(**params)
        return {"status": "ok", "params": llm_service.runtime_params}

    # –î–ª—è vLLM —Å–µ—Ä–≤–∏—Å–∞ –±–µ–∑ set_params - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∞—Ç—Ä–∏–±—É—Ç–µ
    if llm_service:
        if not hasattr(llm_service, "runtime_params"):
            llm_service.runtime_params = {}
        params = {k: v for k, v in request.model_dump().items() if v is not None}
        llm_service.runtime_params.update(params)
        return {"status": "ok", "params": llm_service.runtime_params}

    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== Persona Prompt Endpoints ==============


@router.get("/prompt/{persona}")
async def admin_get_persona_prompt(persona: str, user: User = Depends(get_current_user)):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω—ã (–∏–∑ –ë–î)"""
    from db.database import get_session_context
    from db.repositories.llm_preset import LLMPresetRepository

    async with get_session_context() as session:
        repo = LLMPresetRepository(session)
        preset = await repo.get_by_id(persona)
        if not preset:
            raise HTTPException(status_code=404, detail=f"Persona not found: {persona}")
        return {"persona": persona, "prompt": preset.system_prompt or ""}


@router.post("/prompt/{persona}")
async def admin_set_persona_prompt(
    persona: str, request: AdminLLMPromptRequest, user: User = Depends(require_not_guest)
):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω—ã (–≤ –ë–î)"""
    from db.database import get_session_context
    from db.repositories.llm_preset import LLMPresetRepository

    async with get_session_context() as session:
        repo = LLMPresetRepository(session)
        preset = await repo.update_prompt(persona, request.prompt)
        if not preset:
            raise HTTPException(status_code=404, detail=f"Persona not found: {persona}")

        # –ï—Å–ª–∏ —ç—Ç–æ —Ç–µ–∫—É—â–∞—è –ø–µ—Ä—Å–æ–Ω–∞ - –æ–±–Ω–æ–≤–ª—è–µ–º –≤ —Å–µ—Ä–≤–∏—Å–µ
        container = get_container()
        llm_service = container.llm_service
        if llm_service and hasattr(llm_service, "persona_id") and llm_service.persona_id == persona:
            llm_service.system_prompt = request.prompt

        return {"status": "ok", "persona": persona}


@router.post("/prompt/{persona}/reset")
async def admin_reset_persona_prompt(persona: str, user: User = Depends(get_current_user)):
    """–°–±—Ä–æ—Å–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –ø–µ—Ä—Å–æ–Ω—ã –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
    from db.database import get_session_context
    from db.models import DEFAULT_LLM_PRESETS
    from db.repositories.llm_preset import LLMPresetRepository

    # –ò—â–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    default_data = next((p for p in DEFAULT_LLM_PRESETS if p["id"] == persona), None)
    if not default_data:
        raise HTTPException(status_code=400, detail=f"No default prompt for persona: {persona}")

    async with get_session_context() as session:
        repo = LLMPresetRepository(session)
        preset = await repo.update_prompt(persona, default_data["system_prompt"])
        if not preset:
            raise HTTPException(status_code=404, detail=f"Persona not found: {persona}")

        # –û–±–Ω–æ–≤–ª—è–µ–º –≤ —Å–µ—Ä–≤–∏—Å–µ –µ—Å–ª–∏ —Ç–µ–∫—É—â–∞—è –ø–µ—Ä—Å–æ–Ω–∞
        container = get_container()
        llm_service = container.llm_service
        if llm_service and hasattr(llm_service, "persona_id") and llm_service.persona_id == persona:
            llm_service.system_prompt = default_data["system_prompt"]

        await async_audit_logger.log(
            action="reset",
            resource="llm_preset",
            resource_id=persona,
            user_id=user.username,
        )

        return {"status": "ok", "persona": persona, "prompt": default_data["system_prompt"]}


# ============== Bridge Endpoints ==============


@router.get("/bridge/status")
async def admin_get_bridge_status(user: User = Depends(require_not_guest)):
    """Get CLI-OpenAI Bridge process status."""
    from bridge_manager import bridge_manager

    return bridge_manager.get_status()


@router.post("/bridge/start")
async def admin_start_bridge(user: User = Depends(require_not_guest)):
    """Manually start the CLI-OpenAI Bridge."""
    from bridge_manager import bridge_manager

    if bridge_manager.is_running:
        return {
            "status": "ok",
            "message": "Bridge is already running",
            **bridge_manager.get_status(),
        }

    result = await bridge_manager.start()
    if result.get("status") != "ok":
        raise HTTPException(
            status_code=503, detail=f"Bridge failed to start: {result.get('error')}"
        )

    await async_audit_logger.log(
        action="create",
        resource="bridge",
        resource_id="bridge",
        user_id=user.username,
        details={"action": "start"},
    )

    return result


@router.post("/bridge/stop")
async def admin_stop_bridge(user: User = Depends(require_not_guest)):
    """Manually stop the CLI-OpenAI Bridge."""
    from bridge_manager import bridge_manager

    if not bridge_manager.is_running:
        return {"status": "ok", "message": "Bridge is not running"}

    result = await bridge_manager.stop()

    await async_audit_logger.log(
        action="delete",
        resource="bridge",
        resource_id="bridge",
        user_id=user.username,
        details={"action": "stop"},
    )

    return result


# ============== VLESS Proxy Endpoints ==============


@router.get("/proxy/status")
async def admin_get_proxy_status(user: User = Depends(require_not_guest)):
    """
    Get VLESS proxy status for current Gemini provider.

    Returns xray availability, running state, and configuration.
    """
    container = get_container()
    llm_service = container.llm_service

    # Check if using Gemini provider with proxy
    if isinstance(llm_service, CloudLLMService):
        provider = llm_service.provider
        if isinstance(provider, GeminiProvider):
            return {"status": "ok", "proxy": provider.get_proxy_status()}

    # Check if xray is available globally
    try:
        from xray_proxy_manager import get_proxy_manager

        manager = get_proxy_manager()
        return {
            "status": "ok",
            "proxy": {
                "xray_available": manager.is_xray_available(),
                "xray_path": manager._xray_path,
                "configured": False,
                "is_running": False,
            },
        }
    except ImportError:
        return {
            "status": "ok",
            "proxy": {
                "xray_available": False,
                "configured": False,
                "is_running": False,
            },
        }


@router.post("/proxy/test")
async def admin_test_proxy(request: ProxyTestRequest, user: User = Depends(require_admin)):
    """
    Test VLESS proxy connection to Google Gemini API.

    Args:
        request: Contains vless_url to test

    Returns:
        Test result with success status and details
    """
    try:
        from xray_proxy_manager import XrayProxyManager, validate_vless_url
    except ImportError:
        raise HTTPException(status_code=503, detail="xray_proxy_manager module not available")

    # Validate URL first
    is_valid, error = validate_vless_url(request.vless_url)
    if not is_valid:
        return {
            "status": "error",
            "success": False,
            "error": f"Invalid VLESS URL: {error}",
        }

    # Create temporary proxy manager for testing
    manager = XrayProxyManager()
    if not manager.is_xray_available():
        return {
            "status": "error",
            "success": False,
            "error": "xray binary not found. Install xray-core first.",
        }

    if not manager.configure(request.vless_url):
        return {
            "status": "error",
            "success": False,
            "error": "Failed to configure VLESS proxy",
        }

    # Test connection
    result = manager.test_connection("https://generativelanguage.googleapis.com")

    # Stop the test proxy
    manager.stop()

    if result.get("success"):
        # Audit log
        await async_audit_logger.log(
            action="test",
            resource="vless_proxy",
            user_id=user.username,
            details={"target": result.get("target"), "status_code": result.get("status_code")},
        )
        return {
            "status": "ok",
            "success": True,
            "message": f"Connection successful (HTTP {result.get('status_code')})",
            "details": result,
        }
    else:
        return {
            "status": "error",
            "success": False,
            "error": result.get("error", "Unknown error"),
        }


@router.get("/proxy/validate")
async def admin_validate_vless_url(vless_url: str, user: User = Depends(require_admin)):
    """
    Validate a VLESS URL without testing the connection.

    Args:
        vless_url: VLESS URL to validate

    Returns:
        Validation result with parsed configuration if valid
    """
    try:
        from xray_proxy_manager import parse_vless_url, validate_vless_url
    except ImportError:
        raise HTTPException(status_code=503, detail="xray_proxy_manager module not available")

    is_valid, error = validate_vless_url(vless_url)
    if not is_valid:
        return {
            "valid": False,
            "error": error,
        }

    # Parse and return safe info (without exposing full UUID)
    try:
        config = parse_vless_url(vless_url)
        return {
            "valid": True,
            "config": {
                "server": f"{config.address}:{config.port}",
                "security": config.security,
                "transport": config.transport_type,
                "remark": config.remark,
            },
        }
    except Exception as e:
        return {
            "valid": False,
            "error": str(e),
        }


class ProxyTestMultipleRequest(BaseModel):
    """Request to test multiple VLESS proxy URLs"""

    vless_urls: list[str]


@router.post("/proxy/test-multiple")
async def admin_test_multiple_proxies(
    request: ProxyTestMultipleRequest, user: User = Depends(require_admin)
):
    """
    Test multiple VLESS proxy URLs.

    Args:
        request: Contains list of vless_urls to test

    Returns:
        Test results for each proxy
    """
    try:
        from xray_proxy_manager import XrayProxyManagerWithFallback
    except ImportError:
        raise HTTPException(status_code=503, detail="xray_proxy_manager module not available")

    manager = XrayProxyManagerWithFallback(socks_port=10818, http_port=10819)
    if not manager.is_xray_available():
        return {
            "status": "error",
            "error": "xray binary not found. Install xray-core first.",
            "results": [],
        }

    count = manager.configure_proxies(request.vless_urls)
    if count == 0:
        return {
            "status": "error",
            "error": "No valid VLESS URLs provided",
            "results": [],
        }

    # Test all proxies
    results = manager.test_all_proxies()
    manager.stop()

    # Audit log
    successful = sum(1 for r in results if r.get("success"))
    await async_audit_logger.log(
        action="test",
        resource="vless_proxy_multiple",
        user_id=user.username,
        details={"total": len(results), "successful": successful},
    )

    return {
        "status": "ok",
        "total": len(results),
        "successful": successful,
        "results": results,
    }


@router.post("/proxy/reset")
async def admin_reset_proxies(user: User = Depends(require_admin)):
    """
    Reset all proxies to enabled state (for fallback mode).
    """
    container = get_container()
    llm_service = container.llm_service

    if isinstance(llm_service, CloudLLMService):
        provider = llm_service.provider
        if isinstance(provider, GeminiProvider):
            provider.reset_proxies()

            await async_audit_logger.log(
                action="reset",
                resource="vless_proxy",
                user_id=user.username,
            )

            return {"status": "ok", "message": "All proxies reset to enabled"}

    return {"status": "error", "message": "No Gemini provider with proxy configured"}


@router.post("/proxy/switch-next")
async def admin_switch_to_next_proxy(user: User = Depends(require_admin)):
    """
    Manually switch to the next proxy in fallback chain.
    """
    container = get_container()
    llm_service = container.llm_service

    if isinstance(llm_service, CloudLLMService):
        provider = llm_service.provider
        if isinstance(provider, GeminiProvider):
            provider.mark_proxy_failed()
            status = provider.get_proxy_status()

            await async_audit_logger.log(
                action="switch",
                resource="vless_proxy",
                user_id=user.username,
                details={"new_proxy": status.get("current_proxy")},
            )

            return {"status": "ok", "proxy": status}

    return {"status": "error", "message": "No Gemini provider with fallback proxy configured"}


# ============== LLM Presets Endpoints ==============


@router.get("/presets")
async def admin_get_llm_presets(user: User = Depends(get_current_user)):
    """Get all LLM presets"""
    from db.database import get_session_context
    from db.repositories.llm_preset import LLMPresetRepository

    async with get_session_context() as session:
        repo = LLMPresetRepository(session)
        # Ensure default presets exist
        await repo.ensure_defaults()
        presets = await repo.get_all_enabled()
        return {"presets": [p.to_dict() for p in presets]}


@router.get("/presets/{preset_id}")
async def admin_get_llm_preset(preset_id: str, user: User = Depends(get_current_user)):
    """Get a specific LLM preset"""
    from db.database import get_session_context
    from db.repositories.llm_preset import LLMPresetRepository

    async with get_session_context() as session:
        repo = LLMPresetRepository(session)
        preset = await repo.get_by_id(preset_id)
        if not preset:
            raise HTTPException(status_code=404, detail=f"Preset not found: {preset_id}")
        return preset.to_dict()


@router.post("/presets")
async def admin_create_llm_preset(request: LLMPresetCreate, user: User = Depends(get_current_user)):
    """Create a new LLM preset"""
    from db.database import get_session_context
    from db.models import LLMPreset
    from db.repositories.llm_preset import LLMPresetRepository

    async with get_session_context() as session:
        repo = LLMPresetRepository(session)

        # Check if preset already exists
        existing = await repo.get_by_id(request.id)
        if existing:
            raise HTTPException(status_code=400, detail=f"Preset already exists: {request.id}")

        preset = LLMPreset(**request.model_dump())
        session.add(preset)
        await session.commit()
        await session.refresh(preset)

        await async_audit_logger.log(
            action="create",
            resource="llm_preset",
            resource_id=preset.id,
            user_id=user.username,
            details={"name": preset.name},
        )

        return preset.to_dict()


@router.put("/presets/{preset_id}")
async def admin_update_llm_preset(
    preset_id: str, request: LLMPresetUpdate, user: User = Depends(get_current_user)
):
    """Update an existing LLM preset"""
    from db.database import get_session_context
    from db.repositories.llm_preset import LLMPresetRepository

    async with get_session_context() as session:
        repo = LLMPresetRepository(session)
        preset = await repo.get_by_id(preset_id)
        if not preset:
            raise HTTPException(status_code=404, detail=f"Preset not found: {preset_id}")

        # Update fields
        update_data = request.model_dump(exclude_unset=True)
        for field, value in update_data.items():
            if value is not None:
                setattr(preset, field, value)

        await session.commit()
        await session.refresh(preset)

        # Update llm_service if this is the current preset
        container = get_container()
        llm_service = container.llm_service
        if (
            llm_service
            and hasattr(llm_service, "persona_id")
            and llm_service.persona_id == preset_id
        ):
            # Update runtime params
            if hasattr(llm_service, "runtime_params"):
                llm_service.runtime_params["temperature"] = preset.temperature
                llm_service.runtime_params["max_tokens"] = preset.max_tokens
                llm_service.runtime_params["top_p"] = preset.top_p
                llm_service.runtime_params["repetition_penalty"] = preset.repetition_penalty
            # Update system prompt
            if preset.system_prompt:
                llm_service.system_prompt = preset.system_prompt

        await async_audit_logger.log(
            action="update",
            resource="llm_preset",
            resource_id=preset_id,
            user_id=user.username,
            details=update_data,
        )

        return preset.to_dict()


@router.delete("/presets/{preset_id}")
async def admin_delete_llm_preset(preset_id: str, user: User = Depends(get_current_user)):
    """Delete an LLM preset"""
    from db.database import get_session_context
    from db.repositories.llm_preset import LLMPresetRepository

    # Prevent deletion of default presets
    if preset_id in ["anna", "marina"]:
        raise HTTPException(status_code=400, detail="Cannot delete default presets")

    async with get_session_context() as session:
        repo = LLMPresetRepository(session)
        preset = await repo.get_by_id(preset_id)
        if not preset:
            raise HTTPException(status_code=404, detail=f"Preset not found: {preset_id}")

        await repo.delete(preset)

        await async_audit_logger.log(
            action="delete",
            resource="llm_preset",
            resource_id=preset_id,
            user_id=user.username,
        )

        return {"status": "ok", "deleted": preset_id}


@router.post("/presets/{preset_id}/activate")
async def admin_activate_llm_preset(preset_id: str, user: User = Depends(get_current_user)):
    """
    Activate an LLM preset - load its parameters into the current vLLM service.
    Also sets it as the default preset.
    """
    from db.database import get_session_context
    from db.repositories.llm_preset import LLMPresetRepository

    async with get_session_context() as session:
        repo = LLMPresetRepository(session)
        preset = await repo.get_by_id(preset_id)
        if not preset:
            raise HTTPException(status_code=404, detail=f"Preset not found: {preset_id}")

        # Set as default
        await repo.set_default(preset_id)

        # Update llm_service
        container = get_container()
        llm_service = container.llm_service

        if llm_service and hasattr(llm_service, "runtime_params"):
            # Update runtime params
            llm_service.runtime_params["temperature"] = preset.temperature
            llm_service.runtime_params["max_tokens"] = preset.max_tokens
            llm_service.runtime_params["top_p"] = preset.top_p
            llm_service.runtime_params["repetition_penalty"] = preset.repetition_penalty

            # Update system prompt
            if preset.system_prompt and hasattr(llm_service, "system_prompt"):
                llm_service.system_prompt = preset.system_prompt

            # Update persona_id for tracking
            if hasattr(llm_service, "persona_id"):
                llm_service.persona_id = preset_id
            if hasattr(llm_service, "persona"):
                llm_service.persona = {
                    "name": preset.name,
                    "full_name": preset.name,
                    "description": preset.description or "",
                    "prompt": preset.system_prompt or "",
                }

            logger.info(f"‚úÖ LLM Preset activated: {preset.name} ({preset_id})")

        await async_audit_logger.log(
            action="activate",
            resource="llm_preset",
            resource_id=preset_id,
            user_id=user.username,
            details={"name": preset.name, "params": preset.get_params()},
        )

        return {
            "status": "ok",
            "preset": preset.to_dict(),
            "message": f"Preset '{preset.name}' activated",
        }


@router.get("/presets/current")
async def admin_get_current_preset(user: User = Depends(get_current_user)):
    """Get the currently active LLM preset"""
    from db.database import get_session_context
    from db.repositories.llm_preset import LLMPresetRepository

    container = get_container()
    llm_service = container.llm_service

    # Get current preset_id from llm_service
    current_id = "anna"  # Default
    if llm_service and hasattr(llm_service, "persona_id"):
        current_id = llm_service.persona_id

    async with get_session_context() as session:
        repo = LLMPresetRepository(session)
        await repo.ensure_defaults()
        preset = await repo.get_by_id(current_id)

        if preset:
            return {"current": preset.to_dict()}

        # Fallback to default
        default = await repo.get_default()
        if default:
            return {"current": default.to_dict()}

        return {"current": None, "error": "No preset found"}
