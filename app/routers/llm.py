# app/routers/llm.py
"""LLM configuration router - backend switching, personas, providers, params."""

import asyncio
import logging
import os
from pathlib import Path
from typing import Dict, Optional

import httpx
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel

from app.dependencies import get_container
from auth_manager import User, get_current_user
from cloud_llm_service import PROVIDER_TYPES, CloudLLMService
from db.integration import async_audit_logger, async_cloud_provider_manager
from llm_service import LLMService
from service_manager import get_service_manager


logger = logging.getLogger(__name__)

router = APIRouter(prefix="/admin/llm", tags=["llm"])


# ============== Pydantic Models ==============


class AdminLLMPromptRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""

    prompt: str


class AdminLLMModelRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ LLM"""

    model: str  # gemini-2.5-flash, gemini-2.5-pro


class AdminBackendRequest(BaseModel):
    backend: str  # "vllm", "gemini", or "cloud:{provider_id}"
    stop_unused: bool = False  # –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è GPU


class CloudProviderCreate(BaseModel):
    """Create cloud LLM provider"""

    name: str
    provider_type: str  # gemini, kimi, openai, claude, deepseek, custom
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    model_name: str = ""
    enabled: bool = True
    is_default: bool = False
    config: Optional[Dict] = None
    description: Optional[str] = None


class CloudProviderUpdate(BaseModel):
    """Update cloud LLM provider"""

    name: Optional[str] = None
    provider_type: Optional[str] = None
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    model_name: Optional[str] = None
    enabled: Optional[bool] = None
    is_default: Optional[bool] = None
    config: Optional[Dict] = None
    description: Optional[str] = None


class AdminPersonaRequest(BaseModel):
    persona: str  # "gulya" or "lidia"


class AdminLLMParamsRequest(BaseModel):
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    repetition_penalty: Optional[float] = None


# ============== Helper Functions ==============


async def _switch_to_cloud_provider(provider_id: str, stop_unused: bool, user: User):
    """Helper function to switch to a cloud provider"""
    container = get_container()

    provider_config = await async_cloud_provider_manager.get_provider_with_key(provider_id)
    if not provider_config:
        raise HTTPException(status_code=404, detail=f"Provider {provider_id} not found")

    if not provider_config.get("enabled"):
        raise HTTPException(status_code=400, detail=f"Provider {provider_id} is disabled")

    if not provider_config.get("api_key"):
        raise HTTPException(
            status_code=400, detail=f"Provider {provider_id} has no API key configured"
        )

    try:
        new_service = CloudLLMService(provider_config)
        if not new_service.is_available():
            raise HTTPException(status_code=503, detail=f"Provider {provider_id} is not responding")

        container.llm_service = new_service
        os.environ["LLM_BACKEND"] = f"cloud:{provider_id}"

        # Optionally stop vLLM to free GPU
        if stop_unused:
            manager = get_service_manager()
            vllm_status = manager.get_service_status("vllm")
            if vllm_status.get("is_running"):
                logger.info("üõë Stopping vLLM to free GPU memory...")
                await manager.stop_service("vllm")

        logger.info(f"‚úÖ Switched to cloud provider: {provider_config.get('name')}")

        # Audit log
        await async_audit_logger.log(
            action="update",
            resource="config",
            resource_id="llm_backend",
            user_id=user.username,
            details={
                "backend": f"cloud:{provider_id}",
                "provider_type": provider_config.get("provider_type"),
                "model": provider_config.get("model_name"),
            },
        )

        return {
            "status": "ok",
            "backend": f"cloud:{provider_id}",
            "provider_id": provider_id,
            "provider_type": provider_config.get("provider_type"),
            "model": provider_config.get("model_name"),
            "message": f"Switched to cloud provider: {provider_config.get('name')}",
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error switching to cloud provider: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============== Prompt Endpoints ==============


@router.get("/prompt")
async def admin_get_llm_prompt():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç LLM"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service:
        persona = getattr(llm_service, "current_persona", None) or os.getenv(
            "SECRETARY_PERSONA", "gulya"
        )
        return {
            "prompt": llm_service.system_prompt,
            "model": llm_service.model_name,
            "persona": persona,
        }
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@router.post("/prompt")
async def admin_set_llm_prompt(request: AdminLLMPromptRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–æ–≤—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç LLM"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service:
        llm_service.set_system_prompt(request.prompt)
        return {
            "status": "ok",
            "prompt": request.prompt[:100] + "..." if len(request.prompt) > 100 else request.prompt,
        }
    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== Model Endpoints ==============


@router.get("/model")
async def admin_get_llm_model():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å LLM"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service:
        return {"model": llm_service.model_name}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@router.post("/model")
async def admin_set_llm_model(request: AdminLLMModelRequest):
    """–ò–∑–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å LLM"""
    allowed_models = ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash"]

    if request.model not in allowed_models:
        raise HTTPException(
            status_code=400,
            detail=f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {request.model}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {allowed_models}",
        )

    container = get_container()
    llm_service = container.llm_service
    if llm_service:
        try:
            llm_service.set_model(request.model)
            return {"status": "ok", "model": request.model}
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== History Endpoints ==============


@router.delete("/history")
async def admin_clear_llm_history():
    """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ LLM"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service:
        count = len(llm_service.conversation_history)
        llm_service.reset_conversation()
        return {"status": "ok", "cleared_messages": count}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@router.get("/history")
async def admin_get_llm_history():
    """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ LLM"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service:
        return {
            "history": llm_service.conversation_history,
            "count": len(llm_service.conversation_history),
        }
    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== Backend Endpoints ==============


@router.get("/backend")
async def admin_get_llm_backend():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π LLM backend"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service:
        # Detect backend type
        if isinstance(llm_service, CloudLLMService):
            backend = f"cloud:{llm_service.provider_id}"
        elif hasattr(llm_service, "api_url"):
            backend = "vllm"
        else:
            backend = "gemini"

        return {
            "backend": backend,
            "model": getattr(llm_service, "model_name", "unknown"),
            "api_url": getattr(llm_service, "api_url", None),
            "provider_type": getattr(llm_service, "provider_type", None),
        }
    return {"backend": "none", "error": "LLM service not initialized"}


@router.get("/models")
async def admin_get_llm_models():
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π vLLM –∏ —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ Qwen, Llama, DeepSeek –∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª—è—Ö.
    """
    from vllm_llm_service import AVAILABLE_MODELS

    container = get_container()
    llm_service = container.llm_service
    result = {
        "available_models": AVAILABLE_MODELS,
        "current_model": None,
        "loaded_models": [],
        "backend": "none",
    }

    if llm_service:
        is_vllm = hasattr(llm_service, "api_url")
        result["backend"] = "vllm" if is_vllm else "gemini"

        if is_vllm and hasattr(llm_service, "get_current_model_info"):
            result["current_model"] = llm_service.get_current_model_info()
            result["loaded_models"] = llm_service.get_loaded_models()
        elif not is_vllm:
            # Gemini backend
            result["current_model"] = {
                "id": "gemini",
                "name": getattr(llm_service, "model_name", "gemini-2.0-flash"),
                "description": "Google Gemini API",
                "available": True,
            }

    return result


@router.post("/backend")
async def admin_set_llm_backend(
    request: AdminBackendRequest, user: User = Depends(get_current_user)
):
    """–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å LLM backend —Å –≥–æ—Ä—è—á–µ–π –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–æ–π —Å–µ—Ä–≤–∏—Å–∞"""
    container = get_container()

    # Check if it's a cloud provider
    if request.backend.startswith("cloud:"):
        provider_id = request.backend.split(":", 1)[1]
        return await _switch_to_cloud_provider(provider_id, request.stop_unused, user)

    if request.backend not in ["vllm", "gemini"]:
        raise HTTPException(
            status_code=400,
            detail="Invalid backend. Use 'vllm', 'gemini', or 'cloud:{provider_id}'",
        )

    llm_service = container.llm_service

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –±—ç–∫–µ–Ω–¥ –ø—Ä–∞–≤–∏–ª—å–Ω–æ (cloud, vllm, gemini)
    if llm_service and getattr(llm_service, "backend_type", None) == "cloud":
        current_backend = f"cloud:{getattr(llm_service, 'provider_id', 'unknown')}"
    elif (
        llm_service and hasattr(llm_service, "api_url") and not hasattr(llm_service, "backend_type")
    ):
        current_backend = "vllm"
    else:
        current_backend = "gemini"

    if request.backend == current_backend:
        return {
            "status": "ok",
            "backend": request.backend,
            "message": f"–£–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {request.backend}",
        }

    manager = get_service_manager()

    try:
        if request.backend == "vllm":
            # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ vLLM
            logger.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ vLLM...")

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º URL –¥–ª—è vLLM
            vllm_url = os.getenv("VLLM_API_URL", "http://localhost:11434")
            is_docker = Path("/.dockerenv").exists() or os.getenv("DOCKER_CONTAINER") == "1"

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å vLLM
            async def check_vllm_health() -> bool:
                try:
                    async with httpx.AsyncClient() as client:
                        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ endpoints (v1/models –¥–ª—è OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ API)
                        for endpoint in ["/health", "/v1/models"]:
                            try:
                                resp = await client.get(f"{vllm_url}{endpoint}", timeout=5.0)
                                if resp.status_code == 200:
                                    return True
                            except Exception:
                                pass
                except Exception:
                    pass
                return False

            vllm_accessible = await check_vllm_health()

            if not vllm_accessible:
                # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å vLLM (–∏ –≤ Docker, –∏ –ª–æ–∫–∞–ª—å–Ω–æ)
                vllm_status = manager.get_service_status("vllm")

                if not vllm_status.get("is_running"):
                    logger.info("üöÄ –ó–∞–ø—É—Å–∫ vLLM...")
                    start_result = await manager.start_service("vllm")
                    if start_result.get("status") != "ok":
                        raise HTTPException(
                            status_code=503,
                            detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å vLLM: {start_result.get('message', 'Unknown error')}",
                        )

                # –ñ–¥—ë–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ vLLM (–¥–æ 180 —Å–µ–∫—É–Ω–¥ –¥–ª—è Docker, —Ç.–∫. –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏)
                max_attempts = 90 if is_docker else 60  # 180 –∏–ª–∏ 120 —Å–µ–∫—É–Ω–¥
                logger.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ vLLM (–¥–æ {max_attempts * 2} —Å–µ–∫)...")

                for i in range(max_attempts):
                    await asyncio.sleep(2)
                    if await check_vllm_health():
                        logger.info(f"‚úÖ vLLM –≥–æ—Ç–æ–≤ (–ø–æ–ø—ã—Ç–∫–∞ {i + 1})")
                        break
                else:
                    raise HTTPException(
                        status_code=503,
                        detail=f"vLLM –Ω–µ —Å—Ç–∞–ª –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É {vllm_url}. "
                        "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞: docker logs ai-secretary-vllm",
                    )

            # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π vLLM —Å–µ—Ä–≤–∏—Å
            try:
                from vllm_llm_service import VLLMLLMService

                new_service = VLLMLLMService()
                if not new_service.is_available():
                    raise HTTPException(
                        status_code=503, detail="vLLM –∑–∞–ø—É—â–µ–Ω, –Ω–æ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ API"
                    )
            except ImportError:
                raise HTTPException(status_code=503, detail="VLLMLLMService –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")

            container.llm_service = new_service
            os.environ["LLM_BACKEND"] = "vllm"

            logger.info("‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ vLLM")

            # Audit log
            await async_audit_logger.log(
                action="update",
                resource="config",
                resource_id="llm_backend",
                user_id=user.username,
                details={"backend": "vllm", "model": getattr(new_service, "model_name", "unknown")},
            )

            return {
                "status": "ok",
                "backend": "vllm",
                "model": getattr(new_service, "model_name", "unknown"),
                "message": "–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ vLLM",
            }

        else:
            # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ Gemini
            logger.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ Gemini...")

            new_service = LLMService()
            container.llm_service = new_service
            os.environ["LLM_BACKEND"] = "gemini"

            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º vLLM –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è GPU
            if request.stop_unused:
                vllm_status = manager.get_service_status("vllm")
                if vllm_status.get("is_running"):
                    logger.info("üõë –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º vLLM –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è GPU...")
                    await manager.stop_service("vllm")

            logger.info("‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ Gemini")

            # Audit log
            await async_audit_logger.log(
                action="update",
                resource="config",
                resource_id="llm_backend",
                user_id=user.username,
                details={
                    "backend": "gemini",
                    "model": getattr(new_service, "model_name", "unknown"),
                },
            )

            return {
                "status": "ok",
                "backend": "gemini",
                "model": getattr(new_service, "model_name", "unknown"),
                "message": "–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ Gemini"
                + (" (vLLM –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)" if request.stop_unused else ""),
            }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –±—ç–∫–µ–Ω–¥–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============== Cloud Providers Endpoints ==============


@router.get("/providers")
async def admin_list_cloud_providers(enabled_only: bool = False):
    """List all cloud LLM providers"""
    providers = await async_cloud_provider_manager.list_providers(enabled_only)
    return {
        "providers": providers,
        "provider_types": PROVIDER_TYPES,
    }


@router.get("/providers/{provider_id}")
async def admin_get_cloud_provider(
    provider_id: str, include_key: bool = False, user: User = Depends(get_current_user)
):
    """Get cloud provider by ID"""
    if include_key:
        provider = await async_cloud_provider_manager.get_provider_with_key(provider_id)
    else:
        provider = await async_cloud_provider_manager.get_provider(provider_id)

    if not provider:
        raise HTTPException(status_code=404, detail="Provider not found")
    return {"provider": provider}


@router.post("/providers")
async def admin_create_cloud_provider(
    data: CloudProviderCreate, user: User = Depends(get_current_user)
):
    """Create new cloud LLM provider"""
    try:
        provider = await async_cloud_provider_manager.create_provider(
            name=data.name,
            provider_type=data.provider_type,
            api_key=data.api_key,
            base_url=data.base_url,
            model_name=data.model_name,
            enabled=data.enabled,
            is_default=data.is_default,
            config=data.config,
            description=data.description,
        )

        # Audit log
        await async_audit_logger.log(
            action="create",
            resource="cloud_provider",
            resource_id=provider["id"],
            user_id=user.username,
            details={"name": data.name, "provider_type": data.provider_type},
        )

        return {"status": "ok", "provider": provider}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.put("/providers/{provider_id}")
async def admin_update_cloud_provider(
    provider_id: str, data: CloudProviderUpdate, user: User = Depends(get_current_user)
):
    """Update cloud LLM provider"""
    # Filter out None values
    update_data = {k: v for k, v in data.model_dump().items() if v is not None}

    provider = await async_cloud_provider_manager.update_provider(provider_id, **update_data)
    if not provider:
        raise HTTPException(status_code=404, detail="Provider not found")

    # Audit log
    await async_audit_logger.log(
        action="update",
        resource="cloud_provider",
        resource_id=provider_id,
        user_id=user.username,
        details=update_data,
    )

    return {"status": "ok", "provider": provider}


@router.delete("/providers/{provider_id}")
async def admin_delete_cloud_provider(provider_id: str, user: User = Depends(get_current_user)):
    """Delete cloud LLM provider"""
    if not await async_cloud_provider_manager.delete_provider(provider_id):
        raise HTTPException(status_code=404, detail="Provider not found")

    # Audit log
    await async_audit_logger.log(
        action="delete",
        resource="cloud_provider",
        resource_id=provider_id,
        user_id=user.username,
    )

    return {"status": "ok", "message": f"Provider {provider_id} deleted"}


@router.post("/providers/{provider_id}/test")
async def admin_test_cloud_provider(provider_id: str, user: User = Depends(get_current_user)):
    """Test cloud provider connection"""
    provider_config = await async_cloud_provider_manager.get_provider_with_key(provider_id)
    if not provider_config:
        raise HTTPException(status_code=404, detail="Provider not found")

    if not provider_config.get("api_key"):
        return {
            "status": "error",
            "available": False,
            "message": "No API key configured",
        }

    try:
        service = CloudLLMService(provider_config)
        is_available = service.is_available()

        if is_available:
            # Quick test generation
            test_response = service.generate_response("–°–∫–∞–∂–∏ '—Ç–µ—Å—Ç –æ–∫'", use_history=False)
            return {
                "status": "ok",
                "available": True,
                "test_response": test_response[:200] if test_response else "",
            }
        else:
            return {
                "status": "error",
                "available": False,
                "message": "Provider not responding",
            }
    except Exception as e:
        return {
            "status": "error",
            "available": False,
            "message": str(e),
        }


@router.post("/providers/{provider_id}/set-default")
async def admin_set_default_cloud_provider(
    provider_id: str, user: User = Depends(get_current_user)
):
    """Set cloud provider as default"""
    if not await async_cloud_provider_manager.set_default(provider_id):
        raise HTTPException(status_code=404, detail="Provider not found or disabled")

    await async_audit_logger.log(
        action="update",
        resource="cloud_provider",
        resource_id=provider_id,
        user_id=user.username,
        details={"is_default": True},
    )

    return {"status": "ok", "message": f"Provider {provider_id} set as default"}


# ============== Persona Endpoints ==============


@router.get("/personas")
async def admin_get_personas():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service and hasattr(llm_service, "get_available_personas"):
        return {"personas": llm_service.get_available_personas()}

    # Fallback –¥–ª—è Gemini LLM Service
    from vllm_llm_service import SECRETARY_PERSONAS

    return {
        "personas": {
            pid: {"name": p["name"], "full_name": p.get("full_name", p["name"])}
            for pid, p in SECRETARY_PERSONAS.items()
        }
    }


@router.get("/persona")
async def admin_get_current_persona():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –ø–µ—Ä—Å–æ–Ω—É"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service:
        persona_id = getattr(llm_service, "persona_id", "gulya")
        persona = getattr(llm_service, "persona", {})
        return {
            "id": persona_id,
            "name": persona.get("name", "Unknown"),
        }
    return {"id": "none", "error": "LLM service not initialized"}


@router.post("/persona")
async def admin_set_persona(request: AdminPersonaRequest, user: User = Depends(get_current_user)):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–µ—Ä—Å–æ–Ω—É"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service and hasattr(llm_service, "set_persona"):
        success = llm_service.set_persona(request.persona)
        if success:
            # Audit log
            await async_audit_logger.log(
                action="update",
                resource="config",
                resource_id="llm_persona",
                user_id=user.username,
                details={"persona": request.persona},
            )
            return {"status": "ok", "persona": request.persona}
        raise HTTPException(status_code=400, detail=f"Persona not found: {request.persona}")
    raise HTTPException(status_code=503, detail="LLM service does not support personas")


# ============== Params Endpoints ==============


@router.get("/params")
async def admin_get_llm_params():
    """–ü–æ–ª—É—á–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service and hasattr(llm_service, "runtime_params"):
        return {"params": llm_service.runtime_params}

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    return {
        "params": {"temperature": 0.7, "max_tokens": 512, "top_p": 0.9, "repetition_penalty": 1.1}
    }


@router.post("/params")
async def admin_set_llm_params(request: AdminLLMParamsRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM"""
    container = get_container()
    llm_service = container.llm_service
    if llm_service and hasattr(llm_service, "set_params"):
        params = {k: v for k, v in request.model_dump().items() if v is not None}
        llm_service.set_params(**params)
        return {"status": "ok", "params": llm_service.runtime_params}

    # –î–ª—è vLLM —Å–µ—Ä–≤–∏—Å–∞ –±–µ–∑ set_params - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∞—Ç—Ä–∏–±—É—Ç–µ
    if llm_service:
        if not hasattr(llm_service, "runtime_params"):
            llm_service.runtime_params = {}
        params = {k: v for k, v in request.model_dump().items() if v is not None}
        llm_service.runtime_params.update(params)
        return {"status": "ok", "params": llm_service.runtime_params}

    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== Persona Prompt Endpoints ==============


@router.get("/prompt/{persona}")
async def admin_get_persona_prompt(persona: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω—ã"""
    try:
        from vllm_llm_service import SECRETARY_PERSONAS

        if persona in SECRETARY_PERSONAS:
            return {"persona": persona, "prompt": SECRETARY_PERSONAS[persona]["prompt"]}
        raise HTTPException(status_code=404, detail=f"Persona not found: {persona}")
    except ImportError:
        raise HTTPException(status_code=503, detail="vLLM service not available")


@router.post("/prompt/{persona}")
async def admin_set_persona_prompt(persona: str, request: AdminLLMPromptRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω—ã"""
    try:
        from vllm_llm_service import SECRETARY_PERSONAS

        if persona not in SECRETARY_PERSONAS:
            raise HTTPException(status_code=404, detail=f"Persona not found: {persona}")

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–º–ø—Ç
        SECRETARY_PERSONAS[persona]["prompt"] = request.prompt

        # –ï—Å–ª–∏ —ç—Ç–æ —Ç–µ–∫—É—â–∞—è –ø–µ—Ä—Å–æ–Ω–∞ - –æ–±–Ω–æ–≤–ª—è–µ–º –≤ —Å–µ—Ä–≤–∏—Å–µ
        container = get_container()
        llm_service = container.llm_service
        if llm_service and hasattr(llm_service, "persona_id") and llm_service.persona_id == persona:
            llm_service.system_prompt = request.prompt

        return {"status": "ok", "persona": persona}
    except ImportError:
        raise HTTPException(status_code=503, detail="vLLM service not available")


@router.post("/prompt/{persona}/reset")
async def admin_reset_persona_prompt(persona: str):
    """–°–±—Ä–æ—Å–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –ø–µ—Ä—Å–æ–Ω—ã –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
    # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
    raise HTTPException(status_code=501, detail="Not implemented yet")
