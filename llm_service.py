#!/usr/bin/env python3
"""
–°–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Gemini API –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å–µ–∫—Ä–µ—Ç–∞—Ä—è
"""
import os
import logging
from typing import List, Dict, Optional
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLMService:
    def __init__(
        self,
        api_key: Optional[str] = None,
        model_name: str = "gemini-2.5-flash",
        system_prompt: Optional[str] = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ LLM

        Args:
            api_key: API –∫–ª—é—á Gemini (–µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω, –±–µ—Ä–µ—Ç—Å—è –∏–∑ .env)
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å–µ–∫—Ä–µ—Ç–∞—Ä—è
        """
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")

        self.model_name = model_name
        self.conversation_history: List[Dict[str, str]] = []

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Gemini API
        genai.configure(api_key=self.api_key)

        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.system_prompt = system_prompt or self._default_system_prompt()

        logger.info(f"ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM Service: {model_name}")

        try:
            self.model = genai.GenerativeModel(
                model_name=model_name,
                system_instruction=self.system_prompt
            )
            logger.info("‚úÖ Gemini API –ø–æ–¥–∫–ª—é—á–µ–Ω")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Gemini: {e}")
            raise

    def _default_system_prompt(self) -> str:
        """–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å–µ–∫—Ä–µ—Ç–∞—Ä—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return """–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π —Å–µ–∫—Ä–µ—Ç–∞—Ä—å –ø–æ –∏–º–µ–Ω–∏ –õ–∏–¥–∏—è.

–¢–≤–æ–∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏:
- –û—Ç–≤–µ—á–∞—Ç—å –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ –∑–≤–æ–Ω–∫–∏ –≤–µ–∂–ª–∏–≤–æ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ
- –ó–∞–ø–∏—Å—ã–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–≤–æ–Ω—è—â–µ–º (–∏–º—è, –∫–æ–Ω—Ç–∞–∫—Ç—ã, —Ü–µ–ª—å –∑–≤–æ–Ω–∫–∞)
- –û—Ç–≤–µ—á–∞—Ç—å –Ω–∞ —Ç–∏–ø–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –æ –≥—Ä–∞—Ñ–∏–∫–µ —Ä–∞–±–æ—Ç—ã, —É—Å–ª—É–≥–∞—Ö, –∫–æ–Ω—Ç–∞–∫—Ç–∞—Ö
- –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –≤—Å—Ç—Ä–µ—á—É –∏–ª–∏ –ø–µ—Ä–µ–∑–≤–æ–Ω–∏—Ç—å –ø–æ–∑–∂–µ
- –ì–æ–≤–æ—Ä–∏—Ç—å –∫—Ä–∞—Ç–∫–æ, –ø–æ –¥–µ–ª—É, –Ω–æ –¥—Ä—É–∂–µ–ª—é–±–Ω–æ

–ü—Ä–∞–≤–∏–ª–∞ –æ–±—â–µ–Ω–∏—è:
- –í—Å–µ–≥–¥–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–π—Å—è –≤ –Ω–∞—á–∞–ª–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
- –ë—É–¥—å –≤–µ–∂–ª–∏–≤–æ–π, –Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω–æ–π
- –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ –ø–µ—Ä–µ–¥–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—é
- –ü–µ—Ä–µ—Å–ø—Ä–∞—à–∏–≤–∞–π, –µ—Å–ª–∏ –Ω–µ —Ä–∞—Å—Å–ª—ã—à–∞–ª–∞ –∏–ª–∏ –Ω–µ –ø–æ–Ω—è–ª–∞
- –ó–∞–≤–µ—Ä—à–∞–π —Ä–∞–∑–≥–æ–≤–æ—Ä, —É—Ç–æ—á–Ω–∏–≤, —á–µ–º –µ—â–µ –º–æ–∂–Ω–æ –ø–æ–º–æ—á—å

–°—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è: –¥–µ–ª–æ–≤–æ–π, –Ω–æ –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π, –∫–∞–∫ –Ω–∞—Å—Ç–æ—è—â–∏–π —Å–µ–∫—Ä–µ—Ç–∞—Ä—å."""

    def generate_response(
        self,
        user_message: str,
        use_history: bool = True
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

        Args:
            user_message: –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            use_history: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞

        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        logger.info(f"üí¨ –ó–∞–ø—Ä–æ—Å –∫ LLM: '{user_message[:50]}...'")

        try:
            if use_history:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                chat = self.model.start_chat(history=[
                    {"role": msg["role"], "parts": [msg["content"]]}
                    for msg in self.conversation_history
                ])
                response = chat.send_message(user_message)
            else:
                # –ë–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏
                response = self.model.generate_content(user_message)

            assistant_message = response.text.strip()

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
            if use_history:
                self.conversation_history.append({
                    "role": "user",
                    "content": user_message
                })
                self.conversation_history.append({
                    "role": "model",
                    "content": assistant_message
                })

            logger.info(f"‚úÖ –û—Ç–≤–µ—Ç LLM: '{assistant_message[:50]}...'")
            return assistant_message

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            # Fallback –æ—Ç–≤–µ—Ç
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–æ–∑–Ω–∏–∫–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å."

    def reset_conversation(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞"""
        self.conversation_history = []
        logger.info("üîÑ –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ —Å–±—Ä–æ—à–µ–Ω–∞")

    def get_conversation_history(self) -> List[Dict[str, str]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞"""
        return self.conversation_history

    def set_system_prompt(self, new_prompt: str) -> None:
        """
        –ò–∑–º–µ–Ω—è–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏ –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å

        Args:
            new_prompt: –ù–æ–≤—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        """
        logger.info(f"üìù –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞...")
        self.system_prompt = new_prompt
        self.model = genai.GenerativeModel(
            model_name=self.model_name,
            system_instruction=self.system_prompt
        )
        logger.info("‚úÖ –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –æ–±–Ω–æ–≤–ª—ë–Ω")

    def set_model(self, new_model_name: str) -> None:
        """
        –ò–∑–º–µ–Ω—è–µ—Ç –º–æ–¥–µ–ª—å LLM

        Args:
            new_model_name: –ò–º—è –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä. gemini-2.5-pro, gemini-2.5-flash)
        """
        logger.info(f"üîÑ –°–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏ –Ω–∞: {new_model_name}")
        self.model_name = new_model_name
        self.model = genai.GenerativeModel(
            model_name=self.model_name,
            system_instruction=self.system_prompt
        )
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞: {new_model_name}")

    def get_config(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é LLM"""
        return {
            "model_name": self.model_name,
            "system_prompt": self.system_prompt,
            "history_length": len(self.conversation_history),
        }

    def generate_response_stream(
        self,
        user_message: str,
        use_history: bool = True
    ):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º —Ä–µ–∂–∏–º–µ (streaming)

        Args:
            user_message: –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            use_history: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞

        Yields:
            –ß–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        logger.info(f"üí¨ Streaming –∑–∞–ø—Ä–æ—Å –∫ LLM: '{user_message[:50]}...'")

        try:
            if use_history:
                chat = self.model.start_chat(history=[
                    {"role": msg["role"], "parts": [msg["content"]]}
                    for msg in self.conversation_history
                ])
                response = chat.send_message(user_message, stream=True)
            else:
                response = self.model.generate_content(user_message, stream=True)

            full_response = ""
            for chunk in response:
                if chunk.text:
                    full_response += chunk.text
                    yield chunk.text

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            if use_history:
                self.conversation_history.append({
                    "role": "user",
                    "content": user_message
                })
                self.conversation_history.append({
                    "role": "model",
                    "content": full_response
                })

            logger.info(f"‚úÖ Streaming –æ—Ç–≤–µ—Ç –∑–∞–≤–µ—Ä—à—ë–Ω: '{full_response[:50]}...'")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            yield "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–æ–∑–Ω–∏–∫–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞."

    def _convert_messages_to_gemini(self, messages: List[Dict[str, str]]):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç OpenAI —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –≤ Gemini —Ñ–æ—Ä–º–∞—Ç"""
        gemini_history = []
        last_user_message = ""

        for msg in messages:
            role = msg["role"]
            content = msg["content"]

            if role == "system":
                continue
            elif role == "user":
                last_user_message = content
                gemini_history.append({"role": "user", "parts": [content]})
            elif role == "assistant":
                gemini_history.append({"role": "model", "parts": [content]})

        # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
        if gemini_history and gemini_history[-1]["role"] == "user":
            gemini_history = gemini_history[:-1]

        return gemini_history, last_user_message

    def generate_response_from_messages(
        self,
        messages: List[Dict[str, str]],
        stream: bool = False
    ):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–∏—Å–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π OpenAI —Ñ–æ—Ä–º–∞—Ç–∞

        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π [{"role": "user/assistant", "content": "..."}]
            stream: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ—Ç–æ–∫–æ–≤—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é

        Returns/Yields:
            –û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–∫–∞ (–µ—Å–ª–∏ stream=False) –∏–ª–∏ generator (–µ—Å–ª–∏ stream=True)
        """
        if stream:
            return self._generate_response_stream(messages)
        else:
            return self._generate_response_sync(messages)

    def _generate_response_sync(self, messages: List[Dict[str, str]]) -> str:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
        gemini_history, last_user_message = self._convert_messages_to_gemini(messages)

        try:
            chat = self.model.start_chat(history=gemini_history)
            response = chat.send_message(last_user_message)
            return response.text.strip()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–æ–∑–Ω–∏–∫–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞."

    def _generate_response_stream(self, messages: List[Dict[str, str]]):
        """–ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (generator)"""
        gemini_history, last_user_message = self._convert_messages_to_gemini(messages)

        try:
            chat = self.model.start_chat(history=gemini_history)
            response = chat.send_message(last_user_message, stream=True)
            for chunk in response:
                if chunk.text:
                    yield chunk.text
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            yield "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–æ–∑–Ω–∏–∫–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞."


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    try:
        service = LLMService()

        # –¢–µ—Å—Ç–æ–≤—ã–π –¥–∏–∞–ª–æ–≥
        response1 = service.generate_response("–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, —ç—Ç–æ –∫–æ–º–ø–∞–Ω–∏—è XYZ?")
        print(f"–°–µ–∫—Ä–µ—Ç–∞—Ä—å: {response1}")

        response2 = service.generate_response("–ö–∞–∫–æ–π —É –≤–∞—Å –≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã?")
        print(f"–°–µ–∫—Ä–µ—Ç–∞—Ä—å: {response2}")

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        print("–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env —Å GEMINI_API_KEY –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
