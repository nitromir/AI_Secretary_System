# =============================================================================
# AI Secretary System - Docker Compose (GPU Mode)
# =============================================================================
#
# Usage:
#   docker compose up -d                    # Start all services
#   docker compose logs -f orchestrator     # View logs
#   docker compose down                     # Stop all services
#
# Requirements:
#   - NVIDIA GPU with 12GB+ VRAM
#   - NVIDIA Container Toolkit installed
#   - Docker Compose v2.x
#
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # Main Application (Orchestrator + Admin Panel + TTS)
  # ---------------------------------------------------------------------------
  orchestrator:
    build:
      context: .
      dockerfile: Dockerfile
      target: runtime
    image: ai-secretary:latest
    container_name: ai-secretary
    ports:
      - "${ORCHESTRATOR_PORT:-8002}:8002"
    volumes:
      # Persistent data
      - ./data:/app/data
      - ./logs:/app/logs
      - ./models:/app/models
      # Voice samples (read-only, already in image but can override)
      - ./Гуля:/app/Гуля:ro
      - ./Лидия:/app/Лидия:ro
      # Model caches (named volumes for persistence)
      - tts_cache:/root/.cache/tts_models
      - hf_cache:/root/.cache/huggingface
    environment:
      # LLM Configuration
      - LLM_BACKEND=${LLM_BACKEND:-vllm}
      - VLLM_API_URL=http://vllm:8000/v1
      - VLLM_MODEL_NAME=${VLLM_MODEL_NAME:-}
      - SECRETARY_PERSONA=${SECRETARY_PERSONA:-gulya}
      # Cloud LLM (optional fallback)
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      # Database
      - REDIS_URL=redis://redis:6379/0
      - DATABASE_URL=sqlite+aiosqlite:///data/secretary.db
      # Security
      - ADMIN_JWT_SECRET=${ADMIN_JWT_SECRET:-}
      # TTS
      - COQUI_TOS_AGREED=1
      - TTS_CACHE_PATH=/root/.cache/tts_models
    depends_on:
      redis:
        condition: service_healthy
      vllm:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - ai-secretary
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ---------------------------------------------------------------------------
  # vLLM - Local LLM Inference Server
  # ---------------------------------------------------------------------------
  vllm:
    image: vllm/vllm-openai:latest
    container_name: ai-secretary-vllm
    command: >
      --model ${VLLM_MODEL:-Qwen/Qwen2.5-7B-Instruct-AWQ}
      --gpu-memory-utilization 0.5
      --max-model-len 4096
      --dtype float16
      --max-num-seqs 32
      --enforce-eager
      --trust-remote-code
      --host 0.0.0.0
      --port 8000
    volumes:
      - hf_cache:/root/.cache/huggingface
      - ./finetune/adapters:/app/adapters:ro
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
      - VLLM_LOGGING_LEVEL=WARNING
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 180s
    restart: unless-stopped
    networks:
      - ai-secretary

  # ---------------------------------------------------------------------------
  # Redis - Caching and Rate Limiting
  # ---------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: ai-secretary-redis
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - ai-secretary

# =============================================================================
# Volumes
# =============================================================================
volumes:
  redis_data:
    driver: local
  tts_cache:
    driver: local
  hf_cache:
    driver: local

# =============================================================================
# Networks
# =============================================================================
networks:
  ai-secretary:
    driver: bridge
