# =============================================================================
# AI Secretary System - Docker Compose
# =============================================================================
#
# Usage:
#   docker compose up -d               # Start orchestrator + redis
#   docker compose logs -f orchestrator
#
# vLLM is started automatically from Admin Panel when switching LLM backend.
# First start downloads ~9GB vLLM image.
#
# For manual vLLM control:
#   docker compose up -d vllm          # Start vLLM manually
#   docker compose stop vllm           # Stop vLLM
#
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # Main Application (Orchestrator + Admin Panel + TTS)
  # ---------------------------------------------------------------------------
  orchestrator:
    build:
      context: .
      dockerfile: Dockerfile
      target: runtime
    image: ai-secretary:latest
    container_name: ai-secretary
    ports:
      - "${ORCHESTRATOR_PORT:-8002}:8002"
    volumes:
      # Persistent data
      - ./data:/app/data
      - ./logs:/app/logs
      - ./models:/app/models
      # Voice samples
      - ./Анна:/app/Анна:ro
      - ./Марина:/app/Марина:ro
      # Mount LOCAL caches (no re-download)
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
      - ${HOME}/.local/share/tts:/root/.local/share/tts
      # Docker socket for managing vLLM container
      - /var/run/docker.sock:/var/run/docker.sock
      # DEV: Mount source code for hot-reload (remove in production)
      - ./orchestrator.py:/app/orchestrator.py:ro
      - ./cloud_llm_service.py:/app/cloud_llm_service.py:ro
      - ./voice_clone_service.py:/app/voice_clone_service.py:ro
      - ./vllm_llm_service.py:/app/vllm_llm_service.py:ro
      - ./service_manager.py:/app/service_manager.py:ro
      - ./system_monitor.py:/app/system_monitor.py:ro
      - ./db:/app/db:ro
      - ./app:/app/app:ro
      - ./bridge_manager.py:/app/bridge_manager.py:ro
      - ./finetune_manager.py:/app/finetune_manager.py:ro
      - ./services/bridge:/app/services/bridge:ro
      # Claude CLI for bridge auto-start in Docker
      # If claude is installed elsewhere, adjust the path
      - ${HOME}/.local/bin/claude:/usr/local/bin/claude:ro
      - ${HOME}/.claude:/root/.claude
      - ./finetune:/app/finetune:rw
      - ./wiki-pages:/app/wiki-pages:ro
      - ./docs:/app/docs:ro
      - ./README.md:/app/README.md:ro
      - ./admin/dist:/app/admin/dist:ro
    environment:
      # Docker mode detection
      - DOCKER_CONTAINER=1
      - VLLM_CONTAINER_NAME=ai-secretary-vllm
      # LLM - connect to vLLM container
      - LLM_BACKEND=${LLM_BACKEND:-gemini}
      - VLLM_API_URL=http://ai-secretary-vllm:8000/v1
      - VLLM_MODEL_NAME=${VLLM_MODEL_NAME:-}
      - SECRETARY_PERSONA=${SECRETARY_PERSONA:-anna}
      # Cloud LLM fallback
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      # Database
      - REDIS_URL=redis://redis:6379/0
      - DATABASE_URL=sqlite+aiosqlite:///data/secretary.db
      # Security
      - ADMIN_JWT_SECRET=${ADMIN_JWT_SECRET:-}
      # amoCRM proxy (run scripts/amocrm_proxy.py on host)
      - AMOCRM_PROXY=${AMOCRM_PROXY:-http://host.docker.internal:8899}
      # TTS
      - COQUI_TOS_AGREED=1
      - TTS_CACHE_PATH=/root/.local/share/tts
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # All GPUs for monitoring; XTTS auto-selects GPU with CC >= 7.0
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - ai-secretary
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ---------------------------------------------------------------------------
  # Redis - Caching and Rate Limiting
  # ---------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: ai-secretary-redis
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - ai-secretary

  # ---------------------------------------------------------------------------
  # vLLM - Local LLM Inference Server (started on demand from Admin Panel)
  # ---------------------------------------------------------------------------
  vllm:
    image: vllm/vllm-openai:latest
    container_name: ai-secretary-vllm
    profiles: ["vllm"]  # Not started by default, use: docker compose --profile vllm up -d
    command: >
      --model ${VLLM_MODEL:-Qwen/Qwen2.5-7B-Instruct-AWQ}
      --gpu-memory-utilization 0.5
      --max-model-len 4096
      --dtype float16
      --max-num-seqs 32
      --enforce-eager
      --trust-remote-code
      --host 0.0.0.0
      --port 8000
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
      - ./finetune/adapters:/app/adapters:ro
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
      - VLLM_LOGGING_LEVEL=WARNING
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']  # Use first available GPU for vLLM
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 180s
    restart: unless-stopped
    networks:
      - ai-secretary

# =============================================================================
# Volumes
# =============================================================================
volumes:
  redis_data:
    driver: local

# =============================================================================
# Networks
# =============================================================================
networks:
  ai-secretary:
    driver: bridge
