# =============================================================================
# AI Secretary System - Docker Compose
# =============================================================================
#
# Default: Uses LOCAL vLLM (start with ./start_qwen.sh before docker compose)
#
# Usage:
#   ./start_qwen.sh                    # Start local vLLM first
#   docker compose up -d               # Start orchestrator + redis
#   docker compose logs -f orchestrator
#
# For FULL containerized setup (downloads ~9GB vLLM image):
#   docker compose -f docker-compose.yml -f docker-compose.full.yml up -d
#
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # Main Application (Orchestrator + Admin Panel + TTS)
  # ---------------------------------------------------------------------------
  orchestrator:
    build:
      context: .
      dockerfile: Dockerfile
      target: runtime
    image: ai-secretary:latest
    container_name: ai-secretary
    ports:
      - "${ORCHESTRATOR_PORT:-8002}:8002"
    volumes:
      # Persistent data
      - ./data:/app/data
      - ./logs:/app/logs
      - ./models:/app/models
      # Voice samples
      - ./Гуля:/app/Гуля:ro
      - ./Лидия:/app/Лидия:ro
      # Mount LOCAL caches (no re-download)
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
      - ${HOME}/.local/share/tts:/root/.local/share/tts
      # DEV: Mount source code for hot-reload (remove in production)
      - ./orchestrator.py:/app/orchestrator.py:ro
      - ./cloud_llm_service.py:/app/cloud_llm_service.py:ro
      - ./voice_clone_service.py:/app/voice_clone_service.py:ro
      - ./db:/app/db:ro
      - ./app:/app/app:ro
      - ./admin/dist:/app/admin/dist:ro
    environment:
      # LLM - connect to LOCAL vLLM on host
      - LLM_BACKEND=${LLM_BACKEND:-vllm}
      - VLLM_API_URL=http://host.docker.internal:11434
      - VLLM_MODEL_NAME=${VLLM_MODEL_NAME:-}
      - SECRETARY_PERSONA=${SECRETARY_PERSONA:-gulya}
      # Cloud LLM fallback
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      # Database
      - REDIS_URL=redis://redis:6379/0
      - DATABASE_URL=sqlite+aiosqlite:///data/secretary.db
      # Security
      - ADMIN_JWT_SECRET=${ADMIN_JWT_SECRET:-}
      # TTS
      - COQUI_TOS_AGREED=1
      - TTS_CACHE_PATH=/root/.local/share/tts
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # Use GPU 1 (RTX 3060, CC 8.6) instead of GPU 0 (P104-100, CC 6.1)
              # XTTS requires CC >= 7.0
              device_ids: ['1']
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - ai-secretary
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ---------------------------------------------------------------------------
  # Redis - Caching and Rate Limiting
  # ---------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: ai-secretary-redis
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - ai-secretary

# =============================================================================
# Volumes
# =============================================================================
volumes:
  redis_data:
    driver: local

# =============================================================================
# Networks
# =============================================================================
networks:
  ai-secretary:
    driver: bridge
