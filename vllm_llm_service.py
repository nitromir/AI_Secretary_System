#!/usr/bin/env python3
"""
–°–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å vLLM (OpenAI-compatible API) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å–µ–∫—Ä–µ—Ç–∞—Ä—è.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Qwen2.5-7B —Å LoRA, Llama-3.1-8B –∏ DeepSeek-LLM-7B —á–µ—Ä–µ–∑ vLLM.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–µ—Ä—Å–æ–Ω (–ì—É–ª—è, –õ–∏–¥–∏—è –∏ –¥—Ä.)
"""

import json
import logging
import os
from datetime import datetime
from typing import Dict, Generator, List, Optional

import httpx


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============== –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ vLLM ==============
AVAILABLE_MODELS = {
    "qwen": {
        "id": "qwen",
        "name": "Qwen2.5-7B-AWQ",
        "full_name": "Qwen/Qwen2.5-7B-Instruct-AWQ",
        "description": "–ö–∏—Ç–∞–π—Å–∫–∞—è –º–æ–¥–µ–ª—å –æ—Ç Alibaba. –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.",
        "size": "~4GB VRAM",
        "features": ["–†—É—Å—Å–∫–∏–π", "–ö–∏—Ç–∞–π—Å–∫–∏–π", "–ê–Ω–≥–ª–∏–π—Å–∫–∏–π", "–ö–æ–¥", "LoRA –ø–æ–¥–¥–µ—Ä–∂–∫–∞"],
        "start_flag": "",  # default
        "lora_support": True,
    },
    "llama": {
        "id": "llama",
        "name": "Llama-3.1-8B-GPTQ",
        "full_name": "meta-llama/Llama-3.1-8B-Instruct (GPTQ INT4)",
        "description": "–ú–æ–¥–µ–ª—å –æ—Ç Meta. –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ.",
        "size": "~5GB VRAM",
        "features": ["–ê–Ω–≥–ª–∏–π—Å–∫–∏–π", "–ö–æ–¥", "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"],
        "start_flag": "--llama",
        "lora_support": False,
    },
    "deepseek": {
        "id": "deepseek",
        "name": "DeepSeek-LLM-7B",
        "full_name": "deepseek-ai/deepseek-llm-7b-chat",
        "description": "–ö–∏—Ç–∞–π—Å–∫–∞—è –º–æ–¥–µ–ª—å –æ—Ç DeepSeek AI. –°–∏–ª—å–Ω–∞—è –≤ reasoning –∏ –∫–æ–¥–µ.",
        "size": "~5GB VRAM",
        "features": ["–†—É—Å—Å–∫–∏–π", "–ö–∏—Ç–∞–π—Å–∫–∏–π", "–ê–Ω–≥–ª–∏–π—Å–∫–∏–π", "–ö–æ–¥", "Reasoning"],
        "start_flag": "--deepseek",
        "lora_support": False,
    },
}


# ============== –ü–µ—Ä—Å–æ–Ω—ã —Å–µ–∫—Ä–µ—Ç–∞—Ä–µ–π ==============
SECRETARY_PERSONAS = {
    "gulya": {
        "name": "–ì—É–ª—è",
        "full_name": "–ì—É–ª—å–Ω–∞—Ä–∞",
        "company": "Shareware Digital",
        "boss": "–ê—Ä—Ç—ë–º–∞ –Æ—Ä—å–µ–≤–∏—á–∞",
        "prompt": """–¢—ã ‚Äî –ì—É–ª—è (–ì—É–ª—å–Ω–∞—Ä–∞), —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å–µ–∫—Ä–µ—Ç–∞—Ä—å –∫–æ–º–ø–∞–Ω–∏–∏ Shareware Digital –∏ –ª–∏—á–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –ê—Ä—Ç—ë–º–∞ –Æ—Ä—å–µ–≤–∏—á–∞.

–ü–†–ê–í–ò–õ–ê:
1. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º—É–º)
2. –ù–∏–∫–∞–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ - —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
3. –ò—Å–ø–æ–ª—å–∑—É–π –±—É–∫–≤—É "—ë" (–≤—Å—ë, –∏–¥—ë—Ç, –ø—Ä–∏—à–ª—ë—Ç)
4. –ß–∏—Å–ª–∞ –ø–∏—à–∏ —Å–ª–æ–≤–∞–º–∏ (–ø—è—Ç—å—Å–æ—Ç —Ä—É–±–ª–µ–π)
5. –û–û–û –ø—Ä–æ–∏–∑–Ω–æ—Å–∏ –∫–∞–∫ "–æ-–æ-–æ", IT –∫–∞–∫ "–∞–π-—Ç–∏"

–†–û–õ–¨:
- –§–∏–ª—å—Ç—Ä—É–π —Å–ø–∞–º –∏ –ø—Ä–æ–¥–∞–∂–∏
- –ó–∞–ø–∏—Å—ã–≤–∞–π —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –ê—Ä—Ç—ë–º–∞ –Æ—Ä—å–µ–≤–∏—á–∞
- –ë—É–¥—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –∏ –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–π

–ü–†–ò–ú–ï–†–´:
- "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ö–æ–º–ø–∞–Ω–∏—è –®—ç–∞—Ä–≤—ç–∞—Ä –î–∏–¥–∂–∏—Ç–∞–ª, –ø–æ–º–æ—â–Ω–∏–∫ –ê—Ä—Ç—ë–º–∞ –Æ—Ä—å–µ–≤–∏—á–∞, –ì—É–ª—è. –°–ª—É—à–∞—é –≤–∞—Å."
- "–ü—Ä–∏–Ω—è—Ç–æ. –Ø –ø–µ—Ä–µ–¥–∞–º –ê—Ä—Ç—ë–º—É –Æ—Ä—å–µ–≤–∏—á—É, —á—Ç–æ –≤—ã –∑–≤–æ–Ω–∏–ª–∏."
- "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —ç—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–µ–π—á–∞—Å –Ω–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ. –í—Å–µ–≥–æ –¥–æ–±—Ä–æ–≥–æ."
""",
    },
    "lidia": {
        "name": "–õ–∏–¥–∏—è",
        "full_name": "–õ–∏–¥–∏—è",
        "company": "Shareware Digital",
        "boss": "–ê—Ä—Ç—ë–º–∞ –Æ—Ä—å–µ–≤–∏—á–∞",
        "prompt": """–¢—ã ‚Äî –õ–∏–¥–∏—è, —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å–µ–∫—Ä–µ—Ç–∞—Ä—å –∫–æ–º–ø–∞–Ω–∏–∏ Shareware Digital –∏ –ª–∏—á–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –ê—Ä—Ç—ë–º–∞ –Æ—Ä—å–µ–≤–∏—á–∞.

–ü–†–ê–í–ò–õ–ê:
1. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º—É–º)
2. –ù–∏–∫–∞–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ - —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
3. –ò—Å–ø–æ–ª—å–∑—É–π –±—É–∫–≤—É "—ë" (–≤—Å—ë, –∏–¥—ë—Ç, –ø—Ä–∏—à–ª—ë—Ç)
4. –ß–∏—Å–ª–∞ –ø–∏—à–∏ —Å–ª–æ–≤–∞–º–∏ (–ø—è—Ç—å—Å–æ—Ç —Ä—É–±–ª–µ–π)
5. –û–û–û –ø—Ä–æ–∏–∑–Ω–æ—Å–∏ –∫–∞–∫ "–æ-–æ-–æ", IT –∫–∞–∫ "–∞–π-—Ç–∏"

–†–û–õ–¨:
- –§–∏–ª—å—Ç—Ä—É–π —Å–ø–∞–º –∏ –ø—Ä–æ–¥–∞–∂–∏
- –ó–∞–ø–∏—Å—ã–≤–∞–π —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –ê—Ä—Ç—ë–º–∞ –Æ—Ä—å–µ–≤–∏—á–∞
- –ë—É–¥—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –∏ –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–π

–ü–†–ò–ú–ï–†–´:
- "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ö–æ–º–ø–∞–Ω–∏—è –®—ç–∞—Ä–≤—ç–∞—Ä –î–∏–¥–∂–∏—Ç–∞–ª, –ø–æ–º–æ—â–Ω–∏–∫ –ê—Ä—Ç—ë–º–∞ –Æ—Ä—å–µ–≤–∏—á–∞, –õ–∏–¥–∏—è. –°–ª—É—à–∞—é –≤–∞—Å."
- "–ü—Ä–∏–Ω—è—Ç–æ. –Ø –ø–µ—Ä–µ–¥–∞–º –ê—Ä—Ç—ë–º—É –Æ—Ä—å–µ–≤–∏—á—É, —á—Ç–æ –≤—ã –∑–≤–æ–Ω–∏–ª–∏."
- "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —ç—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–µ–π—á–∞—Å –Ω–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ. –í—Å–µ–≥–æ –¥–æ–±—Ä–æ–≥–æ."
""",
    },
}

# –ü–µ—Ä—Å–æ–Ω–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–∏–∑ env –∏–ª–∏ gulya)
DEFAULT_PERSONA = os.getenv("SECRETARY_PERSONA", "gulya")


class VLLMLLMService:
    """
    LLM —Å–µ—Ä–≤–∏—Å —á–µ—Ä–µ–∑ vLLM (OpenAI-compatible API).
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - Qwen2.5-7B-Instruct + LoRA
    - Llama-3.1-8B-Instruct GPTQ
    - DeepSeek-LLM-7B-Chat
    - –ù–µ—Å–∫–æ–ª—å–∫–æ –ø–µ—Ä—Å–æ–Ω —Å–µ–∫—Ä–µ—Ç–∞—Ä–µ–π (–ì—É–ª—è, –õ–∏–¥–∏—è)
    """

    def __init__(
        self,
        api_url: Optional[str] = None,
        model_name: Optional[str] = None,
        system_prompt: Optional[str] = None,
        persona: Optional[str] = None,
        timeout: float = 60.0,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ vLLM

        Args:
            api_url: URL vLLM API (default: http://localhost:11434)
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (auto-detect from vLLM, –∏–ª–∏ VLLM_MODEL_NAME env)
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å–µ–∫—Ä–µ—Ç–∞—Ä—è (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–µ—Ä—Å–æ–Ω—É)
            persona: –ü–µ—Ä—Å–æ–Ω–∞ —Å–µ–∫—Ä–µ—Ç–∞—Ä—è (gulya, lidia). Default: SECRETARY_PERSONA env –∏–ª–∏ gulya
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        self.api_url = api_url or os.getenv("VLLM_API_URL", "http://localhost:11434")
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –∞—Ä–≥—É–º–µ–Ω—Ç > env var > auto-detect
        self.model_name = model_name or os.getenv("VLLM_MODEL_NAME", "")
        self.timeout = timeout
        self.conversation_history: List[Dict[str, str]] = []

        # HTTP –∫–ª–∏–µ–Ω—Ç
        self.client = httpx.Client(timeout=timeout)

        # Runtime –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–º–æ–≥—É—Ç –±—ã—Ç—å –∏–∑–º–µ–Ω–µ–Ω—ã —á–µ—Ä–µ–∑ API)
        self.runtime_params = {
            "temperature": 0.7,
            "max_tokens": 512,
            "top_p": 0.9,
            "repetition_penalty": 1.1,
        }

        # –ü–µ—Ä—Å–æ–Ω–∞ —Å–µ–∫—Ä–µ—Ç–∞—Ä—è
        self.persona_id = persona or DEFAULT_PERSONA
        if self.persona_id not in SECRETARY_PERSONAS:
            logger.warning(f"‚ö†Ô∏è –ü–µ—Ä—Å–æ–Ω–∞ '{self.persona_id}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 'gulya'")
            self.persona_id = "gulya"
        self.persona = SECRETARY_PERSONAS[self.persona_id]

        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (—è–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç > –ø–µ—Ä—Å–æ–Ω–∞)
        self.system_prompt = system_prompt or self.persona["prompt"]

        # FAQ (–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ reload_faq –∏–∑ –ë–î)
        self.faq: Dict[str, str] = {}

        logger.info(f"ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è vLLM Service: {self.api_url}")
        logger.info(f"üë§ –ü–µ—Ä—Å–æ–Ω–∞: {self.persona['name']} ({self.persona_id})")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∏ –ø–æ–ª—É—á–∞–µ–º/–ø—Ä–æ–≤–µ—Ä—è–µ–º –∏–º—è –º–æ–¥–µ–ª–∏
        self._check_connection()

    def _check_connection(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ vLLM –∏ –ø–æ–ª—É—á–∞–µ—Ç/–ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏–º—è –º–æ–¥–µ–ª–∏"""
        try:
            response = self.client.get(f"{self.api_url}/v1/models")
            response.raise_for_status()
            models = response.json()

            available_models = [m["id"] for m in models.get("data", [])]

            if self.model_name:
                # –ú–æ–¥–µ–ª—å —É–∫–∞–∑–∞–Ω–∞ —è–≤–Ω–æ - –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ—ë –Ω–∞–ª–∏—á–∏–µ
                if self.model_name in available_models:
                    logger.info(f"‚úÖ vLLM –ø–æ–¥–∫–ª—é—á–µ–Ω, –º–æ–¥–µ–ª—å: {self.model_name}")
                else:
                    logger.warning(
                        f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å '{self.model_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –¥–æ—Å—Ç—É–ø–Ω—ã: {available_models}"
                    )
                    # Fallback –Ω–∞ –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é
                    if available_models:
                        self.model_name = available_models[0]
                        logger.info(f"üìå –ò—Å–ø–æ–ª—å–∑—É–µ–º: {self.model_name}")
            elif available_models:
                # Auto-detect: –±–µ—Ä—ë–º –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å
                self.model_name = available_models[0]
                logger.info(f"‚úÖ vLLM –ø–æ–¥–∫–ª—é—á–µ–Ω, –º–æ–¥–µ–ª—å (auto): {self.model_name}")
            else:
                logger.warning("‚ö†Ô∏è vLLM –Ω–µ –≤–µ—Ä–Ω—É–ª —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π")
                self.model_name = "unknown"

            # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–¥–ª—è LoRA)
            if len(available_models) > 1:
                logger.info(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {available_models}")

        except httpx.ConnectError:
            logger.warning(f"‚ö†Ô∏è vLLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É {self.api_url}")
            if not self.model_name:
                self.model_name = "offline"
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ vLLM: {e}")
            if not self.model_name:
                self.model_name = "error"

    def _normalize_faq(self, faq_dict: Dict[str, str]) -> Dict[str, str]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∫–ª—é—á–∏ FAQ (lowercase, strip)"""
        return {k.lower().strip(): v for k, v in faq_dict.items()}

    def _check_faq(self, user_message: str) -> Optional[str]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å FAQ"""
        if not self.faq:
            return None

        normalized = user_message.lower().strip().rstrip("?!.,")

        if normalized in self.faq:
            response = self.faq[normalized]
            logger.info(f"üìã FAQ match (exact): '{normalized}'")
            return self._apply_faq_templates(response)

        for key, response in self.faq.items():
            if key in normalized or normalized in key:
                logger.info(f"üìã FAQ match (partial): '{key}' in '{normalized}'")
                return self._apply_faq_templates(response)

        return None

    def _apply_faq_templates(self, response: str) -> str:
        """–ü–æ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —à–∞–±–ª–æ–Ω–∞ –≤ –æ—Ç–≤–µ—Ç"""
        now = datetime.now()

        replacements = {
            "{current_time}": now.strftime("%H:%M"),
            "{current_date}": now.strftime("%d.%m.%Y"),
            "{day_of_week}": [
                "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫",
                "–≤—Ç–æ—Ä–Ω–∏–∫",
                "—Å—Ä–µ–¥–∞",
                "—á–µ—Ç–≤–µ—Ä–≥",
                "–ø—è—Ç–Ω–∏—Ü–∞",
                "—Å—É–±–±–æ—Ç–∞",
                "–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ",
            ][now.weekday()],
        }

        for placeholder, value in replacements.items():
            response = response.replace(placeholder, value)

        return response

    def reload_faq(self, faq_dict: Dict[str, str] = None):
        """
        –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç FAQ (hot reload).

        Args:
            faq_dict: FAQ —Å–ª–æ–≤–∞—Ä—å –∏–∑ –ë–î. –ï—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, FAQ –æ—á–∏—â–∞–µ—Ç—Å—è.
        """
        if faq_dict:
            self.faq = self._normalize_faq(faq_dict)
        else:
            self.faq = {}
        logger.info(f"üîÑ FAQ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω: {len(self.faq)} –∑–∞–ø–∏—Å–µ–π")

    def _default_system_prompt(self) -> str:
        """–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å–µ–∫—Ä–µ—Ç–∞—Ä—è (deprecated, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è persona)"""
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ–º–ø—Ç —Ç–µ–∫—É—â–µ–π –ø–µ—Ä—Å–æ–Ω—ã
        return self.persona["prompt"]

    def set_persona(self, persona_id: str) -> bool:
        """
        –ú–µ–Ω—è–µ—Ç –ø–µ—Ä—Å–æ–Ω—É —Å–µ–∫—Ä–µ—Ç–∞—Ä—è.

        Args:
            persona_id: ID –ø–µ—Ä—Å–æ–Ω—ã (gulya, lidia)

        Returns:
            True –µ—Å–ª–∏ –ø–µ—Ä—Å–æ–Ω–∞ —É—Å–ø–µ—à–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–∞
        """
        if persona_id not in SECRETARY_PERSONAS:
            logger.warning(f"‚ö†Ô∏è –ü–µ—Ä—Å–æ–Ω–∞ '{persona_id}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return False

        self.persona_id = persona_id
        self.persona = SECRETARY_PERSONAS[persona_id]
        self.system_prompt = self.persona["prompt"]
        logger.info(f"üë§ –ü–µ—Ä—Å–æ–Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞: {self.persona['name']} ({persona_id})")
        return True

    def get_available_personas(self) -> Dict[str, Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω"""
        return {
            pid: {"name": p["name"], "full_name": p["full_name"]}
            for pid, p in SECRETARY_PERSONAS.items()
        }

    def set_params(self, **kwargs):
        """
        –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç runtime –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

        Args:
            temperature: float (0.0-2.0)
            max_tokens: int (1-4096)
            top_p: float (0.0-1.0)
            repetition_penalty: float (1.0-2.0)
        """
        for key, value in kwargs.items():
            if key in self.runtime_params and value is not None:
                self.runtime_params[key] = value
        logger.info(f"‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {self.runtime_params}")

    def get_params(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        return self.runtime_params.copy()

    # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (—Å—Ç–∞—Ä—ã–π –ø—Ä–æ–º–ø—Ç)
    @staticmethod
    def _legacy_system_prompt() -> str:
        """–°—Ç–∞—Ä—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–¥–ª—è —Å–ø—Ä–∞–≤–∫–∏)"""
        return """–¢—ã ‚Äî –õ–∏–¥–∏—è, —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å–µ–∫—Ä–µ—Ç–∞—Ä—å –∫–æ–º–ø–∞–Ω–∏–∏ Shareware Digital –∏ –ª–∏—á–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –ê—Ä—Ç—ë–º–∞ –Æ—Ä—å–µ–≤–∏—á–∞.

–ü–†–ê–í–ò–õ–ê:
1. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º—É–º)
2. –ù–∏–∫–∞–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ - —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
3. –ò—Å–ø–æ–ª—å–∑—É–π –±—É–∫–≤—É "—ë" (–≤—Å—ë, –∏–¥—ë—Ç, –ø—Ä–∏—à–ª—ë—Ç)
4. –ß–∏—Å–ª–∞ –ø–∏—à–∏ —Å–ª–æ–≤–∞–º–∏ (–ø—è—Ç—å—Å–æ—Ç —Ä—É–±–ª–µ–π)
5. –û–û–û –ø—Ä–æ–∏–∑–Ω–æ—Å–∏ –∫–∞–∫ "–æ-–æ-–æ", IT –∫–∞–∫ "–∞–π-—Ç–∏"

–†–û–õ–¨:
- –§–∏–ª—å—Ç—Ä—É–π —Å–ø–∞–º –∏ –ø—Ä–æ–¥–∞–∂–∏
- –ó–∞–ø–∏—Å—ã–≤–∞–π —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –ê—Ä—Ç—ë–º–∞ –Æ—Ä—å–µ–≤–∏—á–∞
- –ë—É–¥—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –∏ –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–π

–ü–†–ò–ú–ï–†–´:
- "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ö–æ–º–ø–∞–Ω–∏—è –®—ç–∞—Ä–≤—ç–∞—Ä –î–∏–¥–∂–∏—Ç–∞–ª, –ø–æ–º–æ—â–Ω–∏–∫ –ê—Ä—Ç—ë–º–∞ –Æ—Ä—å–µ–≤–∏—á–∞, –õ–∏–¥–∏—è. –°–ª—É—à–∞—é –≤–∞—Å."
- "–ü—Ä–∏–Ω—è—Ç–æ. –Ø –ø–µ—Ä–µ–¥–∞–º –ê—Ä—Ç—ë–º—É –Æ—Ä—å–µ–≤–∏—á—É, —á—Ç–æ –≤—ã –∑–≤–æ–Ω–∏–ª–∏."
- "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —ç—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–µ–π—á–∞—Å –Ω–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ. –í—Å–µ–≥–æ –¥–æ–±—Ä–æ–≥–æ."
"""

    def generate_response(self, user_message: str, use_history: bool = True) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        logger.info(f"üí¨ –ó–∞–ø—Ä–æ—Å –∫ vLLM: '{user_message[:50]}...'")

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º FAQ
        faq_response = self._check_faq(user_message)
        if faq_response:
            logger.info(f"‚ö° FAQ –æ—Ç–≤–µ—Ç (–±–µ–∑ LLM): '{faq_response[:50]}...'")
            if use_history:
                self.conversation_history.append({"role": "user", "content": user_message})
                self.conversation_history.append({"role": "assistant", "content": faq_response})
            return faq_response

        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
            messages = [{"role": "system", "content": self.system_prompt}]

            if use_history:
                messages.extend(self.conversation_history)

            messages.append({"role": "user", "content": user_message})

            # –ó–∞–ø—Ä–æ—Å –∫ vLLM —Å runtime –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            response = self.client.post(
                f"{self.api_url}/v1/chat/completions",
                json={
                    "model": self.model_name,
                    "messages": messages,
                    "max_tokens": self.runtime_params.get("max_tokens", 256),
                    "temperature": self.runtime_params.get("temperature", 0.7),
                    "top_p": self.runtime_params.get("top_p", 0.9),
                    "repetition_penalty": self.runtime_params.get("repetition_penalty", 1.1),
                    "stream": False,
                },
            )
            response.raise_for_status()

            result = response.json()
            assistant_message = result["choices"][0]["message"]["content"].strip()

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
            if use_history:
                self.conversation_history.append({"role": "user", "content": user_message})
                self.conversation_history.append(
                    {"role": "assistant", "content": assistant_message}
                )

            logger.info(f"‚úÖ –û—Ç–≤–µ—Ç vLLM: '{assistant_message[:50]}...'")
            return assistant_message

        except httpx.ConnectError:
            logger.error("‚ùå vLLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, —Å–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–æ–∑–Ω–∏–∫–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å."

    def generate_response_stream(
        self, user_message: str, use_history: bool = True
    ) -> Generator[str, None, None]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º —Ä–µ–∂–∏–º–µ"""
        logger.info(f"üí¨ Streaming –∑–∞–ø—Ä–æ—Å –∫ vLLM: '{user_message[:50]}...'")

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º FAQ
        faq_response = self._check_faq(user_message)
        if faq_response:
            logger.info(f"‚ö° FAQ –æ—Ç–≤–µ—Ç (–±–µ–∑ LLM): '{faq_response[:50]}...'")
            if use_history:
                self.conversation_history.append({"role": "user", "content": user_message})
                self.conversation_history.append({"role": "assistant", "content": faq_response})
            yield faq_response
            return

        try:
            messages = [{"role": "system", "content": self.system_prompt}]

            if use_history:
                messages.extend(self.conversation_history)

            messages.append({"role": "user", "content": user_message})

            # Streaming –∑–∞–ø—Ä–æ—Å —Å runtime –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            with self.client.stream(
                "POST",
                f"{self.api_url}/v1/chat/completions",
                json={
                    "model": self.model_name,
                    "messages": messages,
                    "max_tokens": self.runtime_params.get("max_tokens", 256),
                    "temperature": self.runtime_params.get("temperature", 0.7),
                    "top_p": self.runtime_params.get("top_p", 0.9),
                    "repetition_penalty": self.runtime_params.get("repetition_penalty", 1.1),
                    "stream": True,
                },
            ) as response:
                response.raise_for_status()

                full_response = ""
                for line in response.iter_lines():
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data)
                            delta = chunk["choices"][0].get("delta", {})
                            content = delta.get("content", "")
                            if content:
                                full_response += content
                                yield content
                        except json.JSONDecodeError:
                            continue

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
                if use_history and full_response:
                    self.conversation_history.append({"role": "user", "content": user_message})
                    self.conversation_history.append(
                        {"role": "assistant", "content": full_response}
                    )

                logger.info(f"‚úÖ Streaming –æ—Ç–≤–µ—Ç –∑–∞–≤–µ—Ä—à—ë–Ω: '{full_response[:50]}...'")

        except httpx.ConnectError:
            logger.error("‚ùå vLLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            yield "–ò–∑–≤–∏–Ω–∏—Ç–µ, —Å–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω."
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            yield "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–æ–∑–Ω–∏–∫–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞."

    def generate_response_from_messages(self, messages: List[Dict[str, str]], stream: bool = False):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–∏—Å–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π OpenAI —Ñ–æ—Ä–º–∞—Ç–∞.
        –°–æ–≤–º–µ—Å—Ç–∏–º–æ —Å —Ñ–æ—Ä–º–∞—Ç–æ–º orchestrator.py.
        """
        # –î–ª—è non-streaming –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ (–∏–∑–±–µ–≥–∞–µ–º yield –≤ non-stream)
        if not stream:
            return self._generate_response_non_stream(messages)

        # Streaming —Ä–µ–∂–∏–º - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
        return self._generate_response_stream(messages)

    def _generate_response_non_stream(self, messages: List[Dict[str, str]]) -> str:
        """Non-streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
        # –î–æ–±–∞–≤–ª—è–µ–º system prompt –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        has_system = any(m.get("role") == "system" for m in messages)

        if not has_system:
            final_messages = [{"role": "system", "content": self.system_prompt}]
            final_messages.extend(messages)
        else:
            final_messages = messages

        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è FAQ
        last_user_message = ""
        for msg in reversed(messages):
            if msg.get("role") == "user":
                last_user_message = msg.get("content", "")
                break

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º FAQ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º–∞–ª–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        user_messages_count = sum(1 for m in messages if m.get("role") == "user")
        if last_user_message and user_messages_count <= 1:
            faq_response = self._check_faq(last_user_message)
            if faq_response:
                logger.info(f"‚ö° FAQ –æ—Ç–≤–µ—Ç: '{faq_response[:50]}...'")
                return faq_response

        try:
            response = self.client.post(
                f"{self.api_url}/v1/chat/completions",
                json={
                    "model": self.model_name,
                    "messages": final_messages,
                    "max_tokens": self.runtime_params.get("max_tokens", 512),
                    "temperature": self.runtime_params.get("temperature", 0.7),
                    "top_p": self.runtime_params.get("top_p", 0.9),
                    "repetition_penalty": self.runtime_params.get("repetition_penalty", 1.1),
                    "stream": False,
                },
            )
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"].strip()

        except httpx.ConnectError:
            logger.error("‚ùå vLLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, —Å–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω."
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–æ–∑–Ω–∏–∫–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞."

    def _generate_response_stream(
        self, messages: List[Dict[str, str]]
    ) -> Generator[str, None, None]:
        """Streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
        # –î–æ–±–∞–≤–ª—è–µ–º system prompt –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        has_system = any(m.get("role") == "system" for m in messages)

        if not has_system:
            final_messages = [{"role": "system", "content": self.system_prompt}]
            final_messages.extend(messages)
        else:
            final_messages = messages

        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è FAQ
        last_user_message = ""
        for msg in reversed(messages):
            if msg.get("role") == "user":
                last_user_message = msg.get("content", "")
                break

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º FAQ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º–∞–ª–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        user_messages_count = sum(1 for m in messages if m.get("role") == "user")
        if last_user_message and user_messages_count <= 1:
            faq_response = self._check_faq(last_user_message)
            if faq_response:
                logger.info(f"‚ö° FAQ –æ—Ç–≤–µ—Ç: '{faq_response[:50]}...'")
                yield faq_response
                return

        try:
            # Streaming —Å runtime –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            with self.client.stream(
                "POST",
                f"{self.api_url}/v1/chat/completions",
                json={
                    "model": self.model_name,
                    "messages": final_messages,
                    "max_tokens": self.runtime_params.get("max_tokens", 512),
                    "temperature": self.runtime_params.get("temperature", 0.7),
                    "top_p": self.runtime_params.get("top_p", 0.9),
                    "repetition_penalty": self.runtime_params.get("repetition_penalty", 1.1),
                    "stream": True,
                },
            ) as response:
                response.raise_for_status()

                for line in response.iter_lines():
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data)
                            delta = chunk["choices"][0].get("delta", {})
                            content = delta.get("content", "")
                            if content:
                                yield content
                        except json.JSONDecodeError:
                            continue

        except httpx.ConnectError:
            logger.error("‚ùå vLLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            yield "–ò–∑–≤–∏–Ω–∏—Ç–µ, —Å–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω."
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            yield "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–æ–∑–Ω–∏–∫–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞."

    def reset_conversation(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞"""
        self.conversation_history = []
        logger.info("üîÑ –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ —Å–±—Ä–æ—à–µ–Ω–∞")

    def get_conversation_history(self) -> List[Dict[str, str]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞"""
        return self.conversation_history

    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å vLLM"""
        try:
            response = self.client.get(f"{self.api_url}/health", timeout=5.0)
            return response.status_code == 200
        except Exception:
            return False

    @staticmethod
    def get_available_models() -> Dict[str, Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è vLLM"""
        return AVAILABLE_MODELS

    def get_current_model_info(self) -> Dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.
        –ü—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ –∏–º–µ–Ω–∏ –∏–∑ vLLM.
        """
        model_id = self.model_name.lower() if self.model_name else "unknown"

        # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏
        for key, info in AVAILABLE_MODELS.items():
            if key in model_id or info["name"].lower() in model_id:
                return {
                    "id": key,
                    "name": info["name"],
                    "full_name": info["full_name"],
                    "description": info["description"],
                    "vllm_model_name": self.model_name,
                    "available": self.is_available(),
                }

        # LoRA –∞–¥–∞–ø—Ç–µ—Ä (lydia)
        if "lydia" in model_id:
            qwen_info = AVAILABLE_MODELS.get("qwen", {})
            return {
                "id": "qwen",
                "name": f"{qwen_info.get('name', 'Qwen')} + Lydia LoRA",
                "full_name": qwen_info.get("full_name", ""),
                "description": qwen_info.get("description", ""),
                "vllm_model_name": self.model_name,
                "lora": "lydia",
                "available": self.is_available(),
            }

        # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å
        return {
            "id": "unknown",
            "name": self.model_name or "Unknown",
            "vllm_model_name": self.model_name,
            "available": self.is_available(),
        }

    def get_loaded_models(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π, –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –≤ vLLM"""
        try:
            response = self.client.get(f"{self.api_url}/v1/models")
            response.raise_for_status()
            models = response.json()
            return [m["id"] for m in models.get("data", [])]
        except Exception:
            return []


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("=== –¢–µ—Å—Ç vLLM LLM Service ===\n")

    try:
        service = VLLMLLMService()

        if not service.is_available():
            print("‚ö†Ô∏è vLLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: ./start_vllm.sh")
            exit(1)

        # –¢–µ—Å—Ç FAQ
        print("=== –¢–µ—Å—Ç FAQ ===")
        faq_tests = ["–ü—Ä–∏–≤–µ—Ç", "—Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏?", "–ö–∞–∫–æ–π —Å–µ–≥–æ–¥–Ω—è –¥–µ–Ω—å"]
        for test in faq_tests:
            response = service.generate_response(test, use_history=False)
            print(f"  '{test}' ‚Üí {response}")

        # –¢–µ—Å—Ç LLM
        print("\n=== –¢–µ—Å—Ç vLLM ===")
        service.reset_conversation()

        response1 = service.generate_response("–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, —ç—Ç–æ –∫–æ–º–ø–∞–Ω–∏—è XYZ?")
        print(f"–°–µ–∫—Ä–µ—Ç–∞—Ä—å: {response1}")

        response2 = service.generate_response("–ö–∞–∫–æ–π —É –≤–∞—Å –≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã?")
        print(f"–°–µ–∫—Ä–µ—Ç–∞—Ä—å: {response2}")

        # –¢–µ—Å—Ç streaming
        print("\n=== –¢–µ—Å—Ç Streaming ===")
        print("–°–µ–∫—Ä–µ—Ç–∞—Ä—å: ", end="", flush=True)
        for chunk in service.generate_response_stream("–†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –æ –∫–æ–º–ø–∞–Ω–∏–∏", use_history=False):
            print(chunk, end="", flush=True)
        print()

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
