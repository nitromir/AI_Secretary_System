#!/usr/bin/env python3
"""
Fine-tune Manager - —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –¥–ª—è AI Secretary System.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞, –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è.
"""
import subprocess
import threading
import asyncio
import os
import json
import logging
import re
import shutil
from pathlib import Path
from dataclasses import dataclass, asdict, field
from typing import Optional, List, Dict, AsyncGenerator
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class TrainingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è LoRA"""
    # Model
    base_model: str = "Qwen/Qwen2.5-7B-Instruct"

    # LoRA params
    lora_rank: int = 8
    lora_alpha: int = 16
    lora_dropout: float = 0.05

    # Training params
    batch_size: int = 1
    gradient_accumulation_steps: int = 64
    learning_rate: float = 2e-4
    num_epochs: int = 1
    warmup_ratio: float = 0.03
    weight_decay: float = 0.01
    max_seq_length: int = 768

    # Output
    output_dir: str = "qwen2.5-7b-lydia-lora-new"

    # Advanced
    gradient_checkpointing: bool = True
    fp16: bool = True
    logging_steps: int = 1
    save_steps: int = 100


@dataclass
class AdapterInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ LoRA –∞–¥–∞–ø—Ç–µ—Ä–µ"""
    name: str
    path: str
    size_mb: float
    modified: str
    active: bool = False
    config: Optional[dict] = None


@dataclass
class TrainingStatus:
    """–°—Ç–∞—Ç—É—Å —Ç–µ–∫—É—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    is_running: bool = False
    current_step: int = 0
    total_steps: int = 0
    current_epoch: int = 0
    total_epochs: int = 0
    loss: float = 0.0
    learning_rate: float = 0.0
    elapsed_seconds: float = 0.0
    eta_seconds: float = 0.0
    error: Optional[str] = None


@dataclass
class DatasetStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    total_sessions: int = 0
    total_messages: int = 0
    total_tokens: int = 0
    avg_tokens_per_message: float = 0.0
    file_path: Optional[str] = None
    file_size_mb: float = 0.0
    modified: Optional[str] = None


class FinetuneManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–æ–æ–±—É—á–µ–Ω–∏—è LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤.

    –§—É–Ω–∫—Ü–∏–∏:
    - –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (Telegram export)
    - –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    - –ó–∞–ø—É—Å–∫/–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è
    - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (SSE)
    - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏ (–∞–∫—Ç–∏–≤–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ)
    """

    # –ü—É—Ç–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–ª–æ–∫–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏)
    EXTERNAL_DATA_DIR = Path(os.path.expanduser("~/qwen-finetune"))  # –í–Ω–µ—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ
    VENV_PATH = EXTERNAL_DATA_DIR / "train_venv"  # venv –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

    # –°–∫—Ä–∏–ø—Ç—ã (–≤ finetune/)
    PREPARE_SCRIPT = "prepare_dataset.py"
    TRAIN_SCRIPT = "train.py"
    MERGE_SCRIPT = "merge_lora.py"
    QUANTIZE_SCRIPT = "quantize_awq.py"

    def __init__(self, base_dir: Optional[Path] = None):
        self.base_dir = base_dir or Path(__file__).parent

        # –õ–æ–∫–∞–ª—å–Ω—ã–µ –ø—É—Ç–∏ (–≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏)
        self.finetune_dir = self.base_dir / "finetune"
        self.datasets_dir = self.finetune_dir / "datasets"
        self.adapters_dir = self.finetune_dir / "adapters"

        # –í–Ω–µ—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        self.external_data_dir = self.EXTERNAL_DATA_DIR

        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
        self.training_process: Optional[subprocess.Popen] = None
        self.training_config: Optional[TrainingConfig] = None
        self.training_status = TrainingStatus()
        self.training_log: List[str] = []
        self.training_start_time: Optional[datetime] = None
        self._training_lock = threading.Lock()

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        self.datasets_dir.mkdir(parents=True, exist_ok=True)
        self.adapters_dir.mkdir(parents=True, exist_ok=True)

        # –¢–µ–∫—É—â–∏–π –∞–∫—Ç–∏–≤–Ω—ã–π –∞–¥–∞–ø—Ç–µ—Ä
        self.active_adapter: Optional[str] = None
        self._load_active_adapter()

        logger.info(f"üéì FinetuneManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"   üìÅ Finetune dir: {self.finetune_dir}")
        logger.info(f"   üìä Datasets: {self.datasets_dir}")
        logger.info(f"   üîß Adapters: {self.adapters_dir}")

    def _load_active_adapter(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–∫—Ç–∏–≤–Ω–æ–º –∞–¥–∞–ø—Ç–µ—Ä–µ"""
        active_file = self.adapters_dir / ".active"
        if active_file.exists():
            self.active_adapter = active_file.read_text().strip()

    def _save_active_adapter(self, adapter_name: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã–π –∞–¥–∞–ø—Ç–µ—Ä"""
        active_file = self.adapters_dir / ".active"
        active_file.write_text(adapter_name)
        self.active_adapter = adapter_name

    def _run_script(self, script_name: str, args: List[str] = None, capture_output: bool = True) -> dict:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç Python —Å–∫—Ä–∏–ø—Ç –≤ venv finetune"""
        script_path = self.finetune_dir / script_name
        if not script_path.exists():
            return {"status": "error", "message": f"–°–∫—Ä–∏–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {script_name}"}

        python_path = self.VENV_PATH / "bin" / "python"
        if not python_path.exists():
            # Fallback –Ω–∞ —Å–∏—Å—Ç–µ–º–Ω—ã–π python
            python_path = "python3"
            logger.warning(f"‚ö†Ô∏è venv –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.VENV_PATH}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π python")

        cmd = [str(python_path), str(script_path)]
        if args:
            cmd.extend(args)

        try:
            result = subprocess.run(
                cmd,
                cwd=str(self.finetune_dir),
                capture_output=capture_output,
                text=True,
                timeout=600  # 10 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç
            )

            if result.returncode == 0:
                return {
                    "status": "ok",
                    "stdout": result.stdout,
                    "stderr": result.stderr
                }
            else:
                return {
                    "status": "error",
                    "message": result.stderr or result.stdout,
                    "returncode": result.returncode
                }
        except subprocess.TimeoutExpired:
            return {"status": "error", "message": "–¢–∞–π–º–∞—É—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞"}
        except Exception as e:
            return {"status": "error", "message": str(e)}

    # ============== Dataset Operations ==============

    async def upload_dataset(self, content: bytes, filename: str) -> dict:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç (Telegram export JSON).
        """
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            if filename.endswith('.json'):
                dest_path = self.datasets_dir / "result.json"
            else:
                dest_path = self.datasets_dir / filename

            dest_path.write_bytes(content)
            file_size = len(content) / (1024 * 1024)

            logger.info(f"üì• –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {dest_path} ({file_size:.2f} MB)")

            return {
                "status": "ok",
                "message": f"–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {dest_path.name}",
                "path": str(dest_path),
                "size_mb": round(file_size, 2)
            }
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            return {"status": "error", "message": str(e)}

    async def process_dataset(self) -> dict:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç Telegram export –∏ —Å–æ–∑–¥–∞–µ—Ç JSONL –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
        –ó–∞–ø—É—Å–∫–∞–µ—Ç prepare_dataset.py
        """
        result = self._run_script(self.PREPARE_SCRIPT)

        if result["status"] == "ok":
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –∏—â–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–π jsonl —Ñ–∞–π–ª
            output_files = list(self.datasets_dir.glob("*_dataset_*.jsonl"))
            if output_files:
                output_file = max(output_files, key=lambda f: f.stat().st_mtime)
                lines = len(output_file.read_text().strip().split('\n'))
                return {
                    "status": "ok",
                    "message": f"–î–∞—Ç–∞—Å–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {lines} –ø—Ä–∏–º–µ—Ä–æ–≤",
                    "output_file": str(output_file),
                    "examples_count": lines
                }

        return result

    def get_dataset_stats(self, dataset_file: Optional[str] = None) -> DatasetStats:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞.
        –ï—Å–ª–∏ dataset_file –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–π .jsonl
        """
        stats = DatasetStats()

        # –ù–∞—Ö–æ–¥–∏–º —Ñ–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞
        if dataset_file:
            train_file = Path(dataset_file)
        else:
            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–π .jsonl —Ñ–∞–π–ª
            jsonl_files = list(self.datasets_dir.glob("*.jsonl"))
            if not jsonl_files:
                return stats
            train_file = max(jsonl_files, key=lambda f: f.stat().st_mtime)

        if not train_file.exists():
            return stats

        try:
            stat = train_file.stat()
            stats.file_path = str(train_file)
            stats.file_size_mb = round(stat.st_size / (1024 * 1024), 2)
            stats.modified = datetime.fromtimestamp(stat.st_mtime).isoformat()

            # –ü–∞—Ä—Å–∏–º JSONL
            with open(train_file, 'r', encoding='utf-8') as f:
                sessions = [json.loads(line) for line in f if line.strip()]

            stats.total_sessions = len(sessions)

            total_messages = 0
            total_chars = 0

            for session in sessions:
                messages = session.get("conversations", session.get("messages", []))
                total_messages += len(messages)
                for msg in messages:
                    content = msg.get("value", msg.get("content", ""))
                    total_chars += len(content)

            stats.total_messages = total_messages
            # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ (1 —Ç–æ–∫–µ–Ω ~ 3 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)
            stats.total_tokens = total_chars // 3
            stats.avg_tokens_per_message = round(stats.total_tokens / max(1, total_messages), 1)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")

        return stats

    def list_datasets(self) -> List[dict]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤.
        """
        datasets = []

        for f in self.datasets_dir.iterdir():
            if f.suffix == '.jsonl' and f.is_file():
                stat = f.stat()
                datasets.append({
                    "name": f.name,
                    "path": str(f),
                    "size_mb": round(stat.st_size / (1024 * 1024), 2),
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat()
                })
            elif f.suffix == '.json' and f.name == 'result.json':
                stat = f.stat()
                datasets.append({
                    "name": f.name,
                    "path": str(f),
                    "size_mb": round(stat.st_size / (1024 * 1024), 2),
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    "type": "telegram_export"
                })

        return sorted(datasets, key=lambda x: x["modified"], reverse=True)

    async def augment_dataset(self) -> dict:
        """
        –ê—É–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç (—É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ).
        –ü–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ.
        """
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é
        return {
            "status": "ok",
            "message": "–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã.",
            "stats": asdict(self.get_dataset_stats())
        }

    # ============== Training Configuration ==============

    def get_config(self) -> TrainingConfig:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è"""
        if self.training_config:
            return self.training_config

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ —Ñ–∞–π–ª–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
        config_file = self.finetune_dir / "training_config.json"
        if config_file.exists():
            try:
                data = json.loads(config_file.read_text())
                self.training_config = TrainingConfig(**data)
            except Exception:
                self.training_config = TrainingConfig()
        else:
            self.training_config = TrainingConfig()

        return self.training_config

    def set_config(self, config: TrainingConfig) -> dict:
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è"""
        self.training_config = config

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
        config_file = self.finetune_dir / "training_config.json"
        try:
            config_file.write_text(json.dumps(asdict(config), indent=2))
            logger.info(f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config_file}")
            return {"status": "ok", "config": asdict(config)}
        except Exception as e:
            return {"status": "error", "message": str(e)}

    def get_config_presets(self) -> Dict[str, TrainingConfig]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        return {
            "quick": TrainingConfig(
                lora_rank=4,
                lora_alpha=8,
                batch_size=1,
                gradient_accumulation_steps=32,
                num_epochs=1,
                max_seq_length=512,
                output_dir="adapter-quick",
            ),
            "standard": TrainingConfig(
                lora_rank=8,
                lora_alpha=16,
                batch_size=1,
                gradient_accumulation_steps=64,
                num_epochs=1,
                max_seq_length=768,
                output_dir="adapter-standard",
            ),
            "thorough": TrainingConfig(
                lora_rank=16,
                lora_alpha=32,
                batch_size=1,
                gradient_accumulation_steps=128,
                num_epochs=2,
                max_seq_length=1024,
                output_dir="adapter-thorough",
            ),
        }

    # ============== Training Operations ==============

    async def start_training(self, config: Optional[TrainingConfig] = None) -> dict:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç train.py –∏–∑ finetune/
        """
        if self.training_process and self.training_process.poll() is None:
            return {"status": "error", "message": "–û–±—É—á–µ–Ω–∏–µ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–æ"}

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—É—é –∏–ª–∏ —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if config:
            self.training_config = config
        elif not self.training_config:
            self.training_config = TrainingConfig()

        config = self.training_config

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        jsonl_files = list(self.datasets_dir.glob("*.jsonl"))
        if not jsonl_files:
            return {"status": "error", "message": "–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ."}

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç
        train_file = max(jsonl_files, key=lambda f: f.stat().st_mtime)

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∞–¥–∞–ø—Ç–µ—Ä–∞
        output_dir = self.adapters_dir / config.output_dir
        output_dir.mkdir(parents=True, exist_ok=True)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É –æ–±—É—á–µ–Ω–∏—è
        python_path = self.VENV_PATH / "bin" / "python"
        if not python_path.exists():
            return {"status": "error", "message": f"venv –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.VENV_PATH}"}

        train_script = self.finetune_dir / self.TRAIN_SCRIPT
        if not train_script.exists():
            return {"status": "error", "message": f"–°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω: {train_script}"}

        # train.py –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ö–∞—Ä–¥–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∑–∞–ø—É—Å–∫–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é
        # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        cmd = [str(python_path), str(train_script)]

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è GPU
        env = os.environ.copy()
        env["CUDA_VISIBLE_DEVICES"] = "1"
        env["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"

        try:
            # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –ª–æ–≥
            with self._training_lock:
                self.training_log = []
                self.training_status = TrainingStatus(is_running=True, total_epochs=config.num_epochs)
                self.training_start_time = datetime.now()

            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å
            self.training_process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                cwd=str(self.finetune_dir),
                env=env,
                text=True,
                bufsize=1,
            )

            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫ –¥–ª—è —á—Ç–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞
            threading.Thread(target=self._read_training_output, daemon=True).start()

            logger.info(f"üéì –û–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ: {' '.join(cmd[:5])}...")

            return {
                "status": "ok",
                "message": "–û–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ",
                "config": asdict(config),
                "pid": self.training_process.pid
            }

        except Exception as e:
            with self._training_lock:
                self.training_status.is_running = False
                self.training_status.error = str(e)
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return {"status": "error", "message": str(e)}

    def _read_training_output(self):
        """–ß–∏—Ç–∞–µ—Ç –≤—ã–≤–æ–¥ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –≤ —Ñ–æ–Ω–µ"""
        if not self.training_process:
            return

        # –†–µ–≥—É–ª—è—Ä–∫–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        step_pattern = re.compile(r"Step (\d+)/(\d+)")
        loss_pattern = re.compile(r"loss[=:\s]+([0-9.]+)", re.IGNORECASE)
        epoch_pattern = re.compile(r"Epoch (\d+)/(\d+)")
        lr_pattern = re.compile(r"lr[=:\s]+([0-9.e-]+)", re.IGNORECASE)

        for line in iter(self.training_process.stdout.readline, ''):
            if not line:
                break

            line = line.strip()

            with self._training_lock:
                self.training_log.append(line)

                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ª–æ–≥–∞
                if len(self.training_log) > 10000:
                    self.training_log = self.training_log[-5000:]

                # –ü–∞—Ä—Å–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                step_match = step_pattern.search(line)
                if step_match:
                    self.training_status.current_step = int(step_match.group(1))
                    self.training_status.total_steps = int(step_match.group(2))

                loss_match = loss_pattern.search(line)
                if loss_match:
                    self.training_status.loss = float(loss_match.group(1))

                epoch_match = epoch_pattern.search(line)
                if epoch_match:
                    self.training_status.current_epoch = int(epoch_match.group(1))
                    self.training_status.total_epochs = int(epoch_match.group(2))

                lr_match = lr_pattern.search(line)
                if lr_match:
                    self.training_status.learning_rate = float(lr_match.group(1))

                # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è
                if self.training_start_time:
                    elapsed = (datetime.now() - self.training_start_time).total_seconds()
                    self.training_status.elapsed_seconds = elapsed

                    # ETA
                    if self.training_status.current_step > 0 and self.training_status.total_steps > 0:
                        steps_remaining = self.training_status.total_steps - self.training_status.current_step
                        time_per_step = elapsed / self.training_status.current_step
                        self.training_status.eta_seconds = steps_remaining * time_per_step

        # –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–∏–ª—Å—è
        with self._training_lock:
            self.training_status.is_running = False
            returncode = self.training_process.wait()
            if returncode != 0:
                self.training_status.error = f"–ü—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –∫–æ–¥–æ–º {returncode}"
            else:
                logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")

    async def stop_training(self) -> dict:
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ"""
        if not self.training_process or self.training_process.poll() is not None:
            return {"status": "ok", "message": "–û–±—É—á–µ–Ω–∏–µ –Ω–µ –∑–∞–ø—É—â–µ–Ω–æ"}

        try:
            self.training_process.terminate()
            try:
                self.training_process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                self.training_process.kill()

            with self._training_lock:
                self.training_status.is_running = False

            logger.info("üõë –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            return {"status": "ok", "message": "–û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ"}

        except Exception as e:
            return {"status": "error", "message": str(e)}

    def get_training_status(self) -> TrainingStatus:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è"""
        with self._training_lock:
            return TrainingStatus(
                is_running=self.training_status.is_running,
                current_step=self.training_status.current_step,
                total_steps=self.training_status.total_steps,
                current_epoch=self.training_status.current_epoch,
                total_epochs=self.training_status.total_epochs,
                loss=self.training_status.loss,
                learning_rate=self.training_status.learning_rate,
                elapsed_seconds=self.training_status.elapsed_seconds,
                eta_seconds=self.training_status.eta_seconds,
                error=self.training_status.error,
            )

    def get_training_log(self, lines: int = 100, offset: int = 0) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª–æ–≥ –æ–±—É—á–µ–Ω–∏—è"""
        with self._training_lock:
            total = len(self.training_log)
            if offset > 0:
                end_idx = max(0, total - offset)
                start_idx = max(0, end_idx - lines)
            else:
                start_idx = max(0, total - lines)
                end_idx = total

            return {
                "lines": self.training_log[start_idx:end_idx],
                "total_lines": total,
                "start_line": start_idx,
                "end_line": end_idx,
            }

    async def stream_training_log(self, interval: float = 0.5) -> AsyncGenerator[str, None]:
        """SSE streaming –ª–æ–≥–∞ –æ–±—É—á–µ–Ω–∏—è"""
        last_line_idx = 0

        while True:
            with self._training_lock:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
                if len(self.training_log) > last_line_idx:
                    new_lines = self.training_log[last_line_idx:]
                    last_line_idx = len(self.training_log)

                    for line in new_lines:
                        yield json.dumps({
                            "type": "log",
                            "line": line,
                            "timestamp": datetime.now().isoformat()
                        })

                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
                yield json.dumps({
                    "type": "status",
                    "status": asdict(self.training_status)
                })

                is_running = self.training_status.is_running

            if not is_running:
                yield json.dumps({"type": "done"})
                break

            await asyncio.sleep(interval)

    # ============== Adapter Operations ==============

    def list_adapters(self) -> List[AdapterInfo]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤"""
        adapters = []

        if not self.adapters_dir.exists():
            return adapters

        for adapter_dir in self.adapters_dir.iterdir():
            if not adapter_dir.is_dir() or adapter_dir.name.startswith('.'):
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –∞–¥–∞–ø—Ç–µ—Ä–∞
            adapter_files = list(adapter_dir.glob("adapter_*.safetensors")) + list(adapter_dir.glob("adapter_*.bin"))
            if not adapter_files:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–ø–∞–ø–∫—É final
                final_dir = adapter_dir / "final"
                if final_dir.exists():
                    adapter_files = list(final_dir.glob("adapter_*.safetensors")) + list(final_dir.glob("adapter_*.bin"))

            if not adapter_files:
                continue

            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä
            total_size = sum(f.stat().st_size for f in adapter_dir.rglob("*") if f.is_file())
            size_mb = total_size / (1024 * 1024)

            # –î–∞—Ç–∞ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
            modified = datetime.fromtimestamp(adapter_dir.stat().st_mtime).isoformat()

            # –ö–æ–Ω—Ñ–∏–≥ –µ—Å–ª–∏ –µ—Å—Ç—å
            config = None
            config_file = adapter_dir / "adapter_config.json"
            if not config_file.exists():
                config_file = adapter_dir / "final" / "adapter_config.json"
            if config_file.exists():
                try:
                    config = json.loads(config_file.read_text())
                except Exception:
                    pass

            adapters.append(AdapterInfo(
                name=adapter_dir.name,
                path=str(adapter_dir),
                size_mb=round(size_mb, 2),
                modified=modified,
                active=(adapter_dir.name == self.active_adapter),
                config=config
            ))

        return sorted(adapters, key=lambda x: x.modified, reverse=True)

    async def activate_adapter(self, adapter_name: str) -> dict:
        """
        –ê–∫—Ç–∏–≤–∏—Ä—É–µ—Ç LoRA –∞–¥–∞–ø—Ç–µ—Ä (hot-swap –≤ vLLM).
        """
        adapter_dir = self.adapters_dir / adapter_name
        if not adapter_dir.exists():
            return {"status": "error", "message": f"–ê–¥–∞–ø—Ç–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω: {adapter_name}"}

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤
        final_dir = adapter_dir / "final"
        if final_dir.exists():
            adapter_path = final_dir
        else:
            adapter_path = adapter_dir

        adapter_files = list(adapter_path.glob("adapter_*.safetensors")) + list(adapter_path.glob("adapter_*.bin"))
        if not adapter_files:
            return {"status": "error", "message": f"–§–∞–π–ª—ã –∞–¥–∞–ø—Ç–µ—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {adapter_path}"}

        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å hot-swap —á–µ—Ä–µ–∑ vLLM API
        # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –∞–∫—Ç–∏–≤–Ω—ã–π
        self._save_active_adapter(adapter_name)

        logger.info(f"‚úÖ –ê–¥–∞–ø—Ç–µ—Ä –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω: {adapter_name}")

        return {
            "status": "ok",
            "message": f"–ê–¥–∞–ø—Ç–µ—Ä {adapter_name} –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ vLLM –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.",
            "adapter": adapter_name,
            "path": str(adapter_path),
            "note": "–¢—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ vLLM –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞"
        }

    async def delete_adapter(self, adapter_name: str) -> dict:
        """–£–¥–∞–ª—è–µ—Ç LoRA –∞–¥–∞–ø—Ç–µ—Ä"""
        adapter_dir = self.adapters_dir / adapter_name
        if not adapter_dir.exists():
            return {"status": "error", "message": f"–ê–¥–∞–ø—Ç–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω: {adapter_name}"}

        if adapter_name == self.active_adapter:
            return {"status": "error", "message": "–ù–µ–ª—å–∑—è —É–¥–∞–ª–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π –∞–¥–∞–ø—Ç–µ—Ä"}

        try:
            shutil.rmtree(adapter_dir)
            logger.info(f"üóëÔ∏è –ê–¥–∞–ø—Ç–µ—Ä —É–¥–∞–ª—ë–Ω: {adapter_name}")
            return {"status": "ok", "message": f"–ê–¥–∞–ø—Ç–µ—Ä {adapter_name} —É–¥–∞–ª—ë–Ω"}
        except Exception as e:
            return {"status": "error", "message": str(e)}


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
_finetune_manager: Optional[FinetuneManager] = None


def get_finetune_manager() -> FinetuneManager:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π FinetuneManager"""
    global _finetune_manager
    if _finetune_manager is None:
        _finetune_manager = FinetuneManager()
    return _finetune_manager


if __name__ == "__main__":
    import asyncio

    async def test():
        manager = FinetuneManager()

        print("=== Dataset Stats ===")
        stats = manager.get_dataset_stats()
        print(f"  Sessions: {stats.total_sessions}")
        print(f"  Messages: {stats.total_messages}")
        print(f"  Tokens: {stats.total_tokens}")

        print("\n=== Training Config ===")
        config = manager.get_config()
        print(f"  LoRA rank: {config.lora_rank}")
        print(f"  Batch size: {config.batch_size}")
        print(f"  Learning rate: {config.learning_rate}")

        print("\n=== Adapters ===")
        for adapter in manager.list_adapters():
            active = " (ACTIVE)" if adapter.active else ""
            print(f"  - {adapter.name}: {adapter.size_mb} MB{active}")

    asyncio.run(test())
