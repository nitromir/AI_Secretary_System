#!/usr/bin/env python3
"""
–°–µ—Ä–≤–∏—Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ –Ω–∞ –±–∞–∑–µ Whisper –∏ Vosk

Whisper - –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, batch processing
Vosk - realtime streaming, –Ω–∏–∑–∫–∏–µ —Ä–µ—Å—É—Ä—Å—ã, –æ—Ñ–ª–∞–π–Ω
"""
import torch
import logging
from pathlib import Path
from typing import Optional, Union, Generator, Callable
import numpy as np
import json
import wave
import tempfile
from abc import ABC, abstractmethod

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================================================
# Base STT Interface
# ============================================================================

class BaseSTTService(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è STT —Å–µ—Ä–≤–∏—Å–æ–≤"""

    @abstractmethod
    def transcribe(self, audio_path: Union[str, Path], language: str = "ru") -> dict:
        """–†–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å –∏–∑ —Ñ–∞–π–ª–∞"""
        pass

    @abstractmethod
    def transcribe_audio_data(self, audio_data: np.ndarray, sample_rate: int = 16000, language: str = "ru") -> dict:
        """–†–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å –∏–∑ numpy array"""
        pass


# ============================================================================
# Vosk STT Service (Realtime, Offline, Low Resource)
# ============================================================================

class VoskSTTService(BaseSTTService):
    """
    STT —Å–µ—Ä–≤–∏—Å –Ω–∞ –±–∞–∑–µ Vosk

    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
    - Realtime streaming —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
    - –ü–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ñ–ª–∞–π–Ω
    - –ù–∏–∑–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–µ—Å—É—Ä—Å–∞–º (~50-100MB RAM)
    - –•–æ—Ä–æ—à –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–∏–∏

    –ú–æ–¥–µ–ª–∏:
    - vosk-model-ru-0.42 (~1.5GB, –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)
    - vosk-model-small-ru-0.22 (~45MB, —Å—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ)
    - vosk-model-en-us-0.22 (~1.8GB, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
    """

    MODELS_DIR = Path("models/vosk")

    # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å URLs
    KNOWN_MODELS = {
        "ru": {
            "large": "vosk-model-ru-0.42",
            "small": "vosk-model-small-ru-0.22",
        },
        "en": {
            "large": "vosk-model-en-us-0.22",
            "small": "vosk-model-small-en-us-0.15",
        }
    }

    def __init__(
        self,
        model_path: Optional[Union[str, Path]] = None,
        language: str = "ru",
        model_size: str = "small",
        sample_rate: int = 16000
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Vosk STT

        Args:
            model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None, –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
            language: –Ø–∑—ã–∫ (ru, en)
            model_size: –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ (small, large)
            sample_rate: –ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ (16000 –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–∏–∏)
        """
        try:
            from vosk import Model, KaldiRecognizer, SetLogLevel
            SetLogLevel(-1)  # –û—Ç–∫–ª—é—á–∏—Ç—å –ª–æ–≥–∏ Vosk
        except ImportError:
            raise ImportError("Vosk –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install vosk")

        self.sample_rate = sample_rate
        self.language = language

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
        if model_path:
            self.model_path = Path(model_path)
        else:
            self.model_path = self._find_model(language, model_size)

        if not self.model_path or not self.model_path.exists():
            raise FileNotFoundError(
                f"–ú–æ–¥–µ–ª—å Vosk –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å –≤ {self.MODELS_DIR}/\n"
                f"–ù–∞–ø—Ä–∏–º–µ—Ä: vosk-model-small-ru-0.22 —Å https://alphacephei.com/vosk/models"
            )

        logger.info(f"üéß –ó–∞–≥—Ä—É–∑–∫–∞ Vosk –º–æ–¥–µ–ª–∏: {self.model_path}")
        self.model = Model(str(self.model_path))
        self.recognizer = KaldiRecognizer(self.model, sample_rate)
        self.recognizer.SetWords(True)  # –í–∫–ª—é—á–∏—Ç—å timestamps –¥–ª—è —Å–ª–æ–≤

        logger.info(f"‚úÖ Vosk STT –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ({language}, {sample_rate}Hz)")

    def _find_model(self, language: str, size: str) -> Optional[Path]:
        """–ù–∞–π—Ç–∏ –º–æ–¥–µ–ª—å –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –º–æ–¥–µ–ª–µ–π"""
        self.MODELS_DIR.mkdir(parents=True, exist_ok=True)

        # –ò—â–µ–º –∏–∑–≤–µ—Å—Ç–Ω—É—é –º–æ–¥–µ–ª—å
        if language in self.KNOWN_MODELS and size in self.KNOWN_MODELS[language]:
            model_name = self.KNOWN_MODELS[language][size]
            model_path = self.MODELS_DIR / model_name
            if model_path.exists():
                return model_path

        # –ò—â–µ–º –ª—é–±—É—é –º–æ–¥–µ–ª—å –¥–ª—è —è–∑—ã–∫–∞
        for path in self.MODELS_DIR.iterdir():
            if path.is_dir() and language in path.name.lower():
                return path

        return None

    def transcribe(self, audio_path: Union[str, Path], language: str = "ru") -> dict:
        """
        –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å –∏–∑ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞

        Args:
            audio_path: –ü—É—Ç—å –∫ WAV —Ñ–∞–π–ª—É (16kHz, mono)
            language: –Ø–∑—ã–∫ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –º–æ–¥–µ–ª—å—é)

        Returns:
            dict: {"text": str, "words": list, "confidence": float}
        """
        audio_path = Path(audio_path)

        if not audio_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}")

        logger.info(f"üé§ Vosk —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ: {audio_path}")

        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º recognizer
        self.recognizer.Reset()

        with wave.open(str(audio_path), "rb") as wf:
            if wf.getnchannels() != 1:
                raise ValueError("–¢—Ä–µ–±—É–µ—Ç—Å—è mono –∞—É–¥–∏–æ")
            if wf.getsampwidth() != 2:
                raise ValueError("–¢—Ä–µ–±—É–µ—Ç—Å—è 16-bit –∞—É–¥–∏–æ")
            if wf.getframerate() != self.sample_rate:
                logger.warning(f"Sample rate {wf.getframerate()} != {self.sample_rate}, –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ")

            results = []
            while True:
                data = wf.readframes(4000)
                if len(data) == 0:
                    break
                if self.recognizer.AcceptWaveform(data):
                    result = json.loads(self.recognizer.Result())
                    if result.get("text"):
                        results.append(result)

            # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            final = json.loads(self.recognizer.FinalResult())
            if final.get("text"):
                results.append(final)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        full_text = " ".join(r.get("text", "") for r in results).strip()
        all_words = []
        for r in results:
            all_words.extend(r.get("result", []))

        result = {
            "text": full_text,
            "language": self.language,
            "words": all_words,
            "segments": [{"text": r.get("text", ""), "words": r.get("result", [])} for r in results]
        }

        logger.info(f"‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: '{full_text[:100]}...' ({len(all_words)} —Å–ª–æ–≤)")
        return result

    def transcribe_audio_data(
        self,
        audio_data: np.ndarray,
        sample_rate: int = 16000,
        language: str = "ru"
    ) -> dict:
        """–†–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å –∏–∑ numpy array"""
        import soundfile as sf

        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ int16 –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if audio_data.dtype != np.int16:
                audio_data = (audio_data * 32767).astype(np.int16)
            sf.write(tmp.name, audio_data, sample_rate, subtype='PCM_16')
            result = self.transcribe(tmp.name, language)

        Path(tmp.name).unlink()
        return result

    def stream_recognize(
        self,
        audio_chunks: Generator[bytes, None, None],
        on_partial: Optional[Callable[[str], None]] = None,
        on_final: Optional[Callable[[dict], None]] = None
    ) -> Generator[dict, None, None]:
        """
        Streaming —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ (–¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–∏–∏)

        Args:
            audio_chunks: –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —á–∞–Ω–∫–æ–≤ –∞—É–¥–∏–æ (PCM 16-bit)
            on_partial: Callback –¥–ª—è —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            on_final: Callback –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Yields:
            dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        """
        self.recognizer.Reset()

        for chunk in audio_chunks:
            if self.recognizer.AcceptWaveform(chunk):
                result = json.loads(self.recognizer.Result())
                if result.get("text"):
                    if on_final:
                        on_final(result)
                    yield {"type": "final", **result}
            else:
                partial = json.loads(self.recognizer.PartialResult())
                if partial.get("partial"):
                    if on_partial:
                        on_partial(partial["partial"])
                    yield {"type": "partial", "text": partial["partial"]}

        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        final = json.loads(self.recognizer.FinalResult())
        if final.get("text"):
            if on_final:
                on_final(final)
            yield {"type": "final", **final}

    def recognize_microphone(
        self,
        duration: float = 5.0,
        on_partial: Optional[Callable[[str], None]] = None
    ) -> dict:
        """
        –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞

        Args:
            duration: –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            on_partial: Callback –¥–ª—è —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            dict —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
        """
        try:
            import sounddevice as sd
        except ImportError:
            raise ImportError("sounddevice –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sounddevice")

        logger.info(f"üéôÔ∏è –ó–∞–ø–∏—Å—å —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ ({duration}s)...")

        self.recognizer.Reset()
        results = []

        def audio_callback(indata, frames, time_info, status):
            if status:
                logger.warning(f"Audio status: {status}")
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ bytes
            audio_bytes = (indata[:, 0] * 32767).astype(np.int16).tobytes()

            if self.recognizer.AcceptWaveform(audio_bytes):
                result = json.loads(self.recognizer.Result())
                if result.get("text"):
                    results.append(result)
            else:
                partial = json.loads(self.recognizer.PartialResult())
                if partial.get("partial") and on_partial:
                    on_partial(partial["partial"])

        with sd.InputStream(
            samplerate=self.sample_rate,
            channels=1,
            dtype='float32',
            blocksize=4000,
            callback=audio_callback
        ):
            sd.sleep(int(duration * 1000))

        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        final = json.loads(self.recognizer.FinalResult())
        if final.get("text"):
            results.append(final)

        full_text = " ".join(r.get("text", "") for r in results).strip()

        return {
            "text": full_text,
            "language": self.language,
            "segments": results
        }


# ============================================================================
# Whisper STT Service (High Quality, Batch Processing)
# ============================================================================


class WhisperSTTService(BaseSTTService):
    """
    STT —Å–µ—Ä–≤–∏—Å –Ω–∞ –±–∞–∑–µ Whisper

    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
    - –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —è–∑—ã–∫–æ–≤
    - GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ

    –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:
    - –¢—Ä–µ–±—É–µ—Ç GPU –¥–ª—è –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã
    - –ù–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è realtime (batch processing)
    """

    def __init__(
        self,
        model_size: str = "medium",
        use_faster_whisper: bool = True,
        device: str = "auto"
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏

        Args:
            model_size: –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ (tiny, base, small, medium, large)
            use_faster_whisper: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å faster-whisper (–±—ã—Å—Ç—Ä–µ–µ)
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (auto, cuda, cpu)
        """
        self.model_size = model_size
        self.use_faster_whisper = use_faster_whisper

        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device

        logger.info(f"üéß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Whisper STT –Ω–∞ {self.device}")
        logger.info(f"üìä –ú–æ–¥–µ–ª—å: {model_size}")

        try:
            if use_faster_whisper:
                from faster_whisper import WhisperModel
                # Faster Whisper - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
                self.model = WhisperModel(
                    model_size,
                    device=self.device,
                    compute_type="float16" if self.device == "cuda" else "int8"
                )
                logger.info("‚úÖ Faster Whisper –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            else:
                import whisper
                # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π Whisper
                self.model = whisper.load_model(model_size, device=self.device)
                logger.info("‚úÖ OpenAI Whisper –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise

    def transcribe(
        self,
        audio_path: Union[str, Path],
        language: str = "ru"
    ) -> dict:
        """
        –†–∞—Å–ø–æ–∑–Ω–∞–µ—Ç —Ä–µ—á—å –∏–∑ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞

        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
            language: –Ø–∑—ã–∫ —Ä–µ—á–∏

        Returns:
            dict —Å –ø–æ–ª—è–º–∏: text, language, segments
        """
        logger.info(f"üé§ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ: {audio_path}")

        try:
            if self.use_faster_whisper:
                segments, info = self.model.transcribe(
                    str(audio_path),
                    language=language,
                    vad_filter=True,  # Voice Activity Detection
                    vad_parameters=dict(min_silence_duration_ms=500)
                )

                # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
                text_segments = []
                for segment in segments:
                    text_segments.append({
                        "start": segment.start,
                        "end": segment.end,
                        "text": segment.text.strip()
                    })

                full_text = " ".join([s["text"] for s in text_segments])

                result = {
                    "text": full_text,
                    "language": info.language,
                    "segments": text_segments
                }

            else:
                result_whisper = self.model.transcribe(
                    str(audio_path),
                    language=language,
                    fp16=(self.device == "cuda")
                )

                result = {
                    "text": result_whisper["text"].strip(),
                    "language": result_whisper["language"],
                    "segments": result_whisper.get("segments", [])
                }

            logger.info(f"‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: '{result['text'][:100]}...'")
            return result

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {e}")
            raise

    def transcribe_audio_data(
        self,
        audio_data: np.ndarray,
        sample_rate: int = 16000,
        language: str = "ru"
    ) -> dict:
        """
        –†–∞—Å–ø–æ–∑–Ω–∞–µ—Ç —Ä–µ—á—å –∏–∑ numpy array

        Args:
            audio_data: –ê—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ
            sample_rate: –ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
            language: –Ø–∑—ã–∫

        Returns:
            dict —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
        """
        import tempfile
        import soundfile as sf

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            sf.write(tmp.name, audio_data, sample_rate)
            result = self.transcribe(tmp.name, language)

        Path(tmp.name).unlink()  # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        return result


# –ê–ª–∏–∞—Å –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
STTService = WhisperSTTService


# ============================================================================
# Unified STT Manager (–∞–≤—Ç–æ–≤—ã–±–æ—Ä –¥–≤–∏–∂–∫–∞)
# ============================================================================

class UnifiedSTTService:
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π STT —Å–µ—Ä–≤–∏—Å —Å –∞–≤—Ç–æ–≤—ã–±–æ—Ä–æ–º –¥–≤–∏–∂–∫–∞

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Vosk –¥–ª—è realtime (—Ç–µ–ª–µ—Ñ–æ–Ω–∏—è) –∏ Whisper –¥–ª—è batch (–≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)
    """

    def __init__(
        self,
        vosk_model_path: Optional[Union[str, Path]] = None,
        whisper_model_size: str = "base",
        prefer_vosk: bool = True,
        language: str = "ru"
    ):
        """
        Args:
            vosk_model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ Vosk
            whisper_model_size: –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ Whisper
            prefer_vosk: –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞—Ç—å Vosk –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            language: –Ø–∑—ã–∫
        """
        self.language = language
        self.vosk_service: Optional[VoskSTTService] = None
        self.whisper_service: Optional[WhisperSTTService] = None
        self.prefer_vosk = prefer_vosk

        # –ü—Ä–æ–±—É–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Vosk
        try:
            self.vosk_service = VoskSTTService(
                model_path=vosk_model_path,
                language=language
            )
            logger.info("‚úÖ Vosk STT –¥–æ—Å—Ç—É–ø–µ–Ω")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Vosk STT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

        # –ü—Ä–æ–±—É–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Whisper (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
        self._whisper_model_size = whisper_model_size
        self._whisper_initialized = False

    def _init_whisper(self):
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Whisper"""
        if not self._whisper_initialized:
            try:
                self.whisper_service = WhisperSTTService(
                    model_size=self._whisper_model_size,
                    use_faster_whisper=True
                )
                logger.info("‚úÖ Whisper STT –¥–æ—Å—Ç—É–ø–µ–Ω")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Whisper STT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            self._whisper_initialized = True

    def transcribe(
        self,
        audio_path: Union[str, Path],
        language: str = "ru",
        use_whisper: bool = False
    ) -> dict:
        """
        –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å –∏–∑ —Ñ–∞–π–ª–∞

        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ
            language: –Ø–∑—ã–∫
            use_whisper: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Whisper

        Returns:
            dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
        """
        if use_whisper or (not self.vosk_service and not self.prefer_vosk):
            self._init_whisper()
            if self.whisper_service:
                return self.whisper_service.transcribe(audio_path, language)

        if self.vosk_service:
            return self.vosk_service.transcribe(audio_path, language)

        raise RuntimeError("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö STT —Å–µ—Ä–≤–∏—Å–æ–≤")

    def transcribe_realtime(
        self,
        audio_chunks: Generator[bytes, None, None],
        on_partial: Optional[Callable[[str], None]] = None
    ) -> Generator[dict, None, None]:
        """Realtime —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ (—Ç–æ–ª—å–∫–æ Vosk)"""
        if not self.vosk_service:
            raise RuntimeError("Vosk –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è realtime —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è")

        yield from self.vosk_service.stream_recognize(audio_chunks, on_partial=on_partial)

    def get_available_engines(self) -> list:
        """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–≤–∏–∂–∫–æ–≤"""
        engines = []
        if self.vosk_service:
            engines.append("vosk")
        self._init_whisper()
        if self.whisper_service:
            engines.append("whisper")
        return engines


if __name__ == "__main__":
    import sys

    print("=" * 60)
    print("STT Service Test")
    print("=" * 60)

    # –¢–µ—Å—Ç Vosk
    print("\nüìç –¢–µ—Å—Ç Vosk STT:")
    try:
        vosk = VoskSTTService(language="ru", model_size="small")
        print(f"   –ú–æ–¥–µ–ª—å: {vosk.model_path}")
        print("   ‚úÖ Vosk –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")

        # –¢–µ—Å—Ç —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å sounddevice)
        try:
            import sounddevice as sd
            print("\nüéôÔ∏è –ì–æ–≤–æ—Ä–∏—Ç–µ 3 —Å–µ–∫—É–Ω–¥—ã...")
            result = vosk.recognize_microphone(duration=3.0, on_partial=lambda t: print(f"   ... {t}"))
            print(f"   üìù –†–µ–∑—É–ª—å—Ç–∞—Ç: {result['text']}")
        except ImportError:
            print("   ‚ö†Ô∏è sounddevice –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞")

    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")

    # –¢–µ—Å—Ç Whisper
    print("\nüìç –¢–µ—Å—Ç Whisper STT:")
    try:
        whisper_stt = WhisperSTTService(model_size="base", use_faster_whisper=True)
        print("   ‚úÖ Whisper –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")

    print("\n" + "=" * 60)
