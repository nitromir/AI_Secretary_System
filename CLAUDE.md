# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

AI Secretary System - virtual secretary with voice cloning (XTTS v2, OpenVoice), pre-trained voices (Piper), local LLM (vLLM + Qwen/Llama/DeepSeek), cloud LLM fallback (Gemini with VLESS proxy support, Kimi, OpenAI, Claude, DeepSeek, OpenRouter), and Claude Code CLI bridge. Features GSM telephony support (SIM7600E-H), a Vue 3 PWA admin panel with 16 tabs, i18n (ru/en), themes, ~193 API endpoints across 18 routers, website chat widgets (multi-instance), Telegram bot integration (multi-instance) with sales bot features and YooMoney/YooKassa/Stars payments, and fine-tuning with project dataset generation.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Orchestrator (port 8002)                              â”‚
â”‚  orchestrator.py + app/routers/ (18 modular routers, ~193 endpoints)     â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Vue 3 Admin Panel (16 tabs, PWA)                      â”‚   â”‚
â”‚  â”‚                      admin/dist/                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“            â†“               â†“               â†“               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ vLLM   â”‚  â”‚ Cloud  â”‚     â”‚ XTTS v2  â”‚    â”‚ Piper    â”‚    â”‚ Vosk/    â”‚
â”‚ Local  â”‚  â”‚ LLM    â”‚     â”‚ OpenVoiceâ”‚    â”‚ (CPU)    â”‚    â”‚ Whisper  â”‚
â”‚ LLM    â”‚  â”‚ Factoryâ”‚     â”‚ TTS      â”‚    â”‚ TTS      â”‚    â”‚ STT      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
          â”‚ xray-core   â”‚  (optional, for Gemini VLESS proxy)
          â”‚ VLESS Proxy â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**GPU Mode (RTX 3060 12GB):** vLLM ~6GB (50% GPU) + XTTS v2 ~5GB

**Request flow:** User message â†’ FAQ check (instant) OR LLM â†’ TTS â†’ Audio returned

## Commands

### Quick Start (Docker - Recommended)

```bash
cp .env.docker.example .env && docker compose up -d     # GPU mode
# OR
docker compose -f docker-compose.yml -f docker-compose.cpu.yml up -d  # CPU mode

docker compose logs -f orchestrator             # View logs
docker compose build --no-cache orchestrator && docker compose up -d  # Rebuild
```

### Local Development

```bash
# Start system
./start_gpu.sh              # GPU: XTTS + Qwen2.5-7B + LoRA
./start_cpu.sh              # CPU: Piper + Gemini API

# Health check
curl http://localhost:8002/health
```

### Admin Panel Development

**Entry point:** http://localhost:8002/admin/ (login: admin / admin)

```bash
cd admin && npm install                # First-time setup
cd admin && npm run build              # Production build
cd admin && npm run dev                # Dev mode (:5173)
DEV_MODE=1 ./start_gpu.sh              # Backend proxies to Vite
```

### Code Quality

```bash
# First-time venv setup (if needed)
python3.11 -m venv .venv
source .venv/bin/activate
pip install ruff mypy pre-commit

# Python
ruff check .                           # Lint
ruff check . --fix                     # Auto-fix
ruff format .                          # Format
mypy orchestrator.py                   # Type check (optional)

# Vue/TypeScript
cd admin && npm run lint

# All checks
pre-commit run --all-files
```

### Testing

```bash
# Backend tests
source .venv/bin/activate
pytest tests/                          # All tests
pytest tests/unit/test_db.py -v        # Single file
pytest -k "test_chat" -v               # By name pattern
pytest -m "not slow" -v                # Exclude slow tests
pytest -m "not integration" -v         # Exclude integration tests
pytest -m "not gpu" -v                 # Exclude GPU-required tests
pytest --cov --cov-report=html         # With coverage report

# Frontend tests
cd admin && npm test

# Integration test (requires running system)
./test_system.sh

# Database tests
python scripts/test_db.py
```

**Test markers** (defined in `pyproject.toml`):
- `slow` â€” long-running tests
- `integration` â€” requires external services
- `gpu` â€” requires CUDA GPU

### CI/CD

GitHub Actions runs on push to `main`/`develop` and on PRs:
- `lint-backend` â€” ruff check + format check + mypy
- `lint-frontend` â€” npm ci + eslint + build (includes type check)
- `security` â€” Trivy vulnerability scanner

All checks must pass before merging (branch protection enabled).

### External Access (for Widget/Telegram)

```bash
ngrok http 8002                        # Dev tunnel
cloudflared tunnel --url http://localhost:8002  # Production tunnel
```

### Fine-tuning (separate venv)

```bash
cd finetune
python -m venv train_venv && source train_venv/bin/activate
pip install -r requirements.txt
python prepare_dataset.py && python train.py
```

## Key Components

### Backend Structure

```
orchestrator.py              # FastAPI entry point, global state, legacy endpoints
app/
â”œâ”€â”€ dependencies.py          # ServiceContainer for DI
â”œâ”€â”€ routers/                 # 15 modular routers (~160 endpoints)
â”‚   â”œâ”€â”€ auth.py              # 3 endpoints  - JWT login/logout/refresh
â”‚   â”œâ”€â”€ audit.py             # 4 endpoints  - Audit log viewing/export
â”‚   â”œâ”€â”€ services.py          # 6 endpoints  - vLLM start/stop/restart
â”‚   â”œâ”€â”€ monitor.py           # 7 endpoints  - GPU stats, health, SSE metrics
â”‚   â”œâ”€â”€ faq.py               # 7 endpoints  - FAQ CRUD, reload, test
â”‚   â”œâ”€â”€ stt.py               # 4 endpoints  - STT status, transcribe
â”‚   â”œâ”€â”€ llm.py               # 30 endpoints - Backend, persona, cloud providers, VLESS proxy, bridge
â”‚   â”œâ”€â”€ tts.py               # 15 endpoints - Presets, params, test, cache, streaming
â”‚   â”œâ”€â”€ chat.py              # 12 endpoints - Sessions (CRUD, bulk delete, grouping), messages, streaming
â”‚   â”œâ”€â”€ usage.py             # 10 endpoints - Usage tracking, limits, stats, cleanup
â”‚   â”œâ”€â”€ telegram.py          # 29 endpoints - Bot instances CRUD, control, payments, YooMoney
â”‚   â”œâ”€â”€ widget.py            # 7 endpoints  - Widget instances CRUD
â”‚   â”œâ”€â”€ gsm.py               # 12 endpoints - GSM telephony, SIM7600E-H module
â”‚   â”œâ”€â”€ bot_sales.py         # 20 endpoints - Sales bot (quiz, segments, funnels, testimonials)
â”‚   â”œâ”€â”€ legal.py             # 15 endpoints - GDPR compliance, consents, privacy policy
â”‚   â”œâ”€â”€ backup.py            # 8 endpoints  - Backup/restore system
â”‚   â”œâ”€â”€ github_webhook.py    # 4 endpoints  - GitHub webhook (stars, releases)
â”‚   â””â”€â”€ yoomoney_webhook.py  # 2 endpoints  - YooMoney payment webhook
â””â”€â”€ services/
    â”œâ”€â”€ audio_pipeline.py    # Telephony audio processing (GSM frames, G.711)
    â”œâ”€â”€ sales_funnel.py      # Sales funnel logic (segmentation, pricing, follow-ups)
    â”œâ”€â”€ yoomoney_service.py  # YooMoney OAuth & payment processing
    â””â”€â”€ backup_service.py    # Backup/restore system (ZIP archives, checksums)
```

**Core Services:**
| File | Purpose |
|------|---------|
| `cloud_llm_service.py` | Cloud LLM factory (Gemini, Kimi, OpenAI, Claude, DeepSeek, OpenRouter, Claude Bridge) |
| `bridge_manager.py` | CLI-OpenAI Bridge process manager (auto-start/stop subprocess) |
| `xray_proxy_manager.py` | VLESS proxy manager for Gemini (xray-core process, URL parsing) |
| `vllm_llm_service.py` | vLLM API + `SECRETARY_PERSONAS` dict |
| `voice_clone_service.py` | XTTS v2 with custom presets + streaming synthesis |
| `piper_tts_service.py` | Piper TTS (CPU) with Dmitri/Irina voices, auto-discovers models dir |
| `stt_service.py` | Vosk (realtime) + Whisper (batch) STT |
| `multi_bot_manager.py` | Subprocess manager for multiple Telegram bots (auto-start on app launch) |
| `finetune_manager.py` | LoRA fine-tuning manager (dataset processing, training, adapters, project dataset generation) |
| `app/rate_limiter.py` | Rate limiting with slowapi (configurable per endpoint type) |
| `app/security_headers.py` | Security headers middleware (X-Frame-Options, CSP, etc.) |
| `app/services/audio_pipeline.py` | GSM telephony audio processing (8kHz, PCM16, G.711) |
| `app/services/sales_funnel.py` | Sales funnel logic (segmentation, pricing calculator, follow-ups) |

### Admin Panel (Vue 3)

```
admin/src/
â”œâ”€â”€ views/                   # 18 views (grouped into 5 accordion sections)
â”œâ”€â”€ components/AccordionNav.vue  # Collapsible navigation with 5 groups
â”œâ”€â”€ api/                     # API clients + SSE helpers
â”œâ”€â”€ stores/                  # Pinia (auth, theme, toast, audit, services, llm)
â”œâ”€â”€ composables/             # useSSE, useRealtimeMetrics, useExportImport
â””â”€â”€ plugins/i18n.ts          # vue-i18n translations (ru/en)
```

**Navigation Groups (Accordion):**
- **ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³**: Dashboard, Monitoring, Services, Audit
- **AI-Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¸**: LLM, TTS, Models, Fine-tune
- **ĞšĞ°Ğ½Ğ°Ğ»Ñ‹**: Chat, Telegram, Widget, Telephony (GSM)
- **Ğ‘Ğ¸Ğ·Ğ½ĞµÑ**: FAQ, Sales, CRM (amoCRM placeholder)
- **Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°**: Settings, About

### Database (SQLite + Redis)

**Location:** `data/secretary.db`

**Key tables:** `chat_sessions` (with `source`, `source_id` for tracking origin), `chat_messages`, `faq_entries`, `tts_presets`, `llm_presets`, `system_config`, `telegram_sessions`, `audit_log`, `cloud_llm_providers`, `bot_instances` (with `auto_start`, payment fields), `widget_instances`, `payment_log`, `usage_log`, `usage_limits`, `user_consents`

**Redis (optional):** Used for caching with graceful fallback if unavailable.

```bash
python scripts/migrate_json_to_db.py      # First-time migration
python scripts/migrate_to_instances.py    # Multi-instance migration
python scripts/migrate_add_payment_fields.py  # Payment fields migration
python scripts/migrate_sales_bot.py       # Sales bot tables migration
python scripts/migrate_legal_compliance.py # GDPR consent tables migration
```

**Repository pattern:**
```
db/
â”œâ”€â”€ database.py           # SQLite async connection
â”œâ”€â”€ models.py             # SQLAlchemy ORM models + PROVIDER_TYPES dict
â”œâ”€â”€ redis_client.py       # Redis caching with fallback
â”œâ”€â”€ integration.py        # Backward-compatible managers
â””â”€â”€ repositories/         # Data access layer
    â”œâ”€â”€ base.py           # BaseRepository with CRUD
    â”œâ”€â”€ chat.py           # ChatRepository
    â”œâ”€â”€ faq.py            # FAQRepository
    â”œâ”€â”€ preset.py         # PresetRepository
    â”œâ”€â”€ config.py         # ConfigRepository
    â”œâ”€â”€ telegram.py       # TelegramRepository
    â”œâ”€â”€ bot_instance.py   # BotInstanceRepository (Telegram bots)
    â”œâ”€â”€ payment.py        # PaymentRepository (payment logging)
    â”œâ”€â”€ widget_instance.py # WidgetInstanceRepository
    â”œâ”€â”€ cloud_provider.py # CloudProviderRepository
    â”œâ”€â”€ consent.py        # ConsentRepository (GDPR consent management)
    â”œâ”€â”€ usage.py          # UsageRepository, UsageLimitsRepository
    â””â”€â”€ audit.py          # AuditRepository
```

## Environment Variables

```bash
LLM_BACKEND=vllm                    # "vllm", "gemini", or "cloud:{provider_id}"
VLLM_API_URL=http://localhost:11434 # Base URL without /v1 suffix (auto-normalized)
VLLM_MODEL_NAME=lydia               # LoRA adapter name
VLLM_GPU_ID=1                       # GPU ID for vLLM Docker container (default: 1)
SECRETARY_PERSONA=gulya             # "gulya" or "lidia"
GEMINI_API_KEY=...                  # Only for gemini backend
ORCHESTRATOR_PORT=8002
CUDA_VISIBLE_DEVICES=1
ADMIN_JWT_SECRET=...                # Auto-generated if empty
REDIS_URL=redis://localhost:6379/0  # Optional, for caching

# Security (production)
CORS_ORIGINS=*                      # Comma-separated origins, "*" for dev
RATE_LIMIT_ENABLED=true             # Enable rate limiting
RATE_LIMIT_DEFAULT=60/minute        # Default rate limit
RATE_LIMIT_AUTH=10/minute           # Auth endpoints rate limit
RATE_LIMIT_CHAT=30/minute           # Chat endpoints rate limit
RATE_LIMIT_TTS=20/minute            # TTS endpoints rate limit
SECURITY_HEADERS_ENABLED=true       # Enable security headers
X_FRAME_OPTIONS=DENY                # DENY or SAMEORIGIN
```

## Code Patterns

**Adding a new API endpoint:**
1. Create or edit router in `app/routers/`
2. Use `ServiceContainer` from `app/dependencies.py` for DI
3. Router is auto-registered via `app/routers/__init__.py`

**Adding a new cloud LLM provider type:**
1. Add entry to `PROVIDER_TYPES` dict in `db/models.py` (includes name, base_url, default_model)
2. If OpenAI-compatible, `OpenAICompatibleProvider` in `cloud_llm_service.py` handles it automatically
3. For custom SDK (like Gemini), create new provider class inheriting `BaseLLMProvider`
4. Register in `CloudLLMService.PROVIDER_CLASSES`
5. UI dropdown auto-populates from `GET /admin/llm/providers` endpoint

**Adding a new XTTS voice:**
1. Create folder with WAV samples: `./NewVoice/`
2. Add service instance in `orchestrator.py`
3. Add voice ID to admin endpoints

**Adding a new secretary persona:**
1. Add entry to `SECRETARY_PERSONAS` dict in `vllm_llm_service.py`

**Adding i18n translations:**
1. Edit `admin/src/plugins/i18n.ts`
2. Add keys to both `ru` and `en` message objects

**Adding a new theme:**
1. Add CSS variables in `admin/src/assets/main.css`
2. Update `Theme` type in `admin/src/stores/theme.ts`
3. Add translations in `admin/src/plugins/i18n.ts`

## API Quick Reference

**OpenAI-compatible (for OpenWebUI):**
- `POST /v1/chat/completions` â€” Chat with streaming
- `POST /v1/audio/speech` â€” TTS with current voice
- `GET /v1/models` â€” Available models

**Streaming TTS (for telephony):**
- `POST /admin/tts/stream` â€” HTTP chunked streaming (target <500ms TTFA)
- `WS /admin/tts/ws/stream` â€” WebSocket real-time TTS for GSM telephony

```bash
# HTTP streaming example
curl -X POST http://localhost:8002/admin/tts/stream \
  -H "Content-Type: application/json" \
  -d '{"text":"ĞŸÑ€Ğ¸Ğ²ĞµÑ‚!", "voice":"gulya", "target_sample_rate":8000}' \
  --output audio.pcm

# Benchmark streaming latency
python scripts/benchmark_streaming_tts.py --iterations 5
```

**Admin API (JWT required):** See `app/routers/` for complete endpoint definitions.

Key patterns:
- `GET/POST /admin/{resource}` â€” List/create
- `GET/PUT/DELETE /admin/{resource}/{id}` â€” CRUD
- `POST /admin/{resource}/{id}/action` â€” Actions (start, stop, test)
- `GET /admin/{resource}/stream` â€” SSE endpoints

## Known Issues

1. **Vosk model required** â€” Download to `models/vosk/` for STT:
   ```bash
   mkdir -p models/vosk && cd models/vosk
   wget https://alphacephei.com/vosk/models/vosk-model-ru-0.42.zip && unzip vosk-model-ru-0.42.zip
   ```
2. **XTTS requires CC >= 7.0** â€” RTX 3060+; use OpenVoice for older GPUs (CC >= 6.1)
3. **GPU memory sharing** â€” vLLM 50% (~6GB) + XTTS ~5GB on 12GB GPU
4. **OpenWebUI Docker** â€” Use `172.17.0.1` not `localhost` for API URL
5. **Ruff ignores Cyrillic** â€” RUF001/002/003 disabled to allow Russian strings in code
6. **Docker + vLLM** â€” vLLM Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ² Ğ°Ğ´Ğ¼Ğ¸Ğ½ĞºĞµ. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ñ€Ğ°Ğ· Ğ½ÑƒĞ¶Ğ½Ğ¾ ÑĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ·: `docker pull vllm/vllm-openai:latest` (~9GB). **Note:** `VLLM_API_URL` is auto-normalized â€” trailing `/v1` is stripped (code adds it internally)
7. **xray-core for VLESS** â€” Included in Docker image. For local dev, download to `./bin/xray`:
   ```bash
   mkdir -p bin && cd bin
   wget https://github.com/XTLS/Xray-core/releases/download/v1.8.7/Xray-linux-64.zip
   unzip Xray-linux-64.zip && chmod +x xray
   ```

## Configuration Files

| File | Purpose |
|------|---------|
| `pyproject.toml` | ruff, mypy, pytest, coverage config + test markers |
| `.pre-commit-config.yaml` | Pre-commit hooks |
| `admin/.eslintrc.cjs` | ESLint config |
| `admin/.prettierrc` | Prettier config |
| `.env.docker.example` | Docker environment template |
| `.env.example` | Local development environment template |

## Roadmap

See [BACKLOG.md](./BACKLOG.md) for detailed task tracking and [docs/IMPROVEMENT_PLAN.md](./docs/IMPROVEMENT_PLAN.md) for production readiness plan.

**Current focus:** Foundation (security, testing) â†’ Monetization â†’ GSM Telephony

## Cloud LLM Providers

Supported providers (configured via Admin Panel â†’ LLM â†’ Cloud Providers):

| Provider | Free Models | Paid Models |
|----------|-------------|-------------|
| **OpenRouter** | `nemotron-3-nano:free`, `trinity-large:free`, `solar-pro-3:free` | `gemini-2.0-flash`, `gpt-4o-mini` |
| **Google Gemini** | â€” | `gemini-2.0-flash`, `gemini-2.5-pro` |
| **OpenAI** | â€” | `gpt-4o`, `gpt-4o-mini` |
| **Anthropic** | â€” | `claude-opus-4.5`, `claude-sonnet-4` |
| **DeepSeek** | â€” | `deepseek-chat`, `deepseek-coder` |
| **Kimi** | â€” | `kimi-k2`, `moonshot-v1-128k` |
| **Claude Bridge** | (uses local `claude` CLI) | `sonnet`, `opus`, `haiku` |

**Usage in Telegram bots:**
- Set `llm_backend` in bot config: `"vllm"` or `"cloud:{provider_id}"` (dynamic dropdown in UI)
- Action buttons can override LLM per-mode (e.g., creative mode uses different model)
- LLM dropdown dynamically loads all enabled cloud providers from database

**Per-session LLM override in Chat:**
- Chat view has an LLM selector dropdown in the header
- Select a provider to override the default LLM for that session
- "Default" uses the system-wide LLM backend setting
- Useful for testing different providers without changing global settings

## VLESS Proxy for Gemini

For regions where Google API is restricted, Gemini providers support optional VLESS proxy routing via xray-core with **automatic failover** support.

**Setup:**
1. Install xray-core (included in Docker image, or download to `./bin/xray`)
2. Create/edit Gemini provider in Admin Panel â†’ LLM â†’ Cloud Providers
3. In the modal, enter VLESS URLs in the "VLESS Proxy" section (one per line for failover)
4. Click "Test All Proxies" to verify connections
5. Save â€” all Gemini API requests will route through the proxy

**Multiple Proxies with Fallback:**
- Add multiple VLESS URLs (one per line) for automatic failover
- When current proxy fails, system switches to next available
- UI shows proxy count badge (e.g., "3 Proxy") on provider cards
- "Test All Proxies" tests each URL and shows per-proxy results

**VLESS URL format:**
```
vless://uuid@host:port?security=reality&pbk=PUBLIC_KEY&sid=SHORT_ID&type=tcp&flow=xtls-rprx-vision#Name
```

**Supported protocols:**
- Security: `none`, `tls`, `reality`
- Transport: `tcp`, `ws` (WebSocket), `grpc`
- Flow: `xtls-rprx-vision` (for XTLS)

**API endpoints:**
- `GET /admin/llm/proxy/status` â€” xray availability, proxy list and current proxy
- `POST /admin/llm/proxy/test` â€” Test single VLESS URL
- `POST /admin/llm/proxy/test-multiple` â€” Test multiple VLESS URLs
- `POST /admin/llm/proxy/reset` â€” Reset all proxies to enabled state
- `POST /admin/llm/proxy/switch-next` â€” Manually switch to next proxy
- `GET /admin/llm/proxy/validate` â€” Validate VLESS URL format

**How it works:**
```
GeminiProvider â†’ XrayProxyManagerWithFallback â†’ xray-core (SOCKS5/HTTP) â†’ VLESS Server â†’ Google API
                         â†“ (on failure)
                 Auto-switch to next proxy
```

**Storage:** VLESS URLs stored in provider's `config` JSON field:
```json
{
  "temperature": 0.7,
  "vless_urls": [
    "vless://uuid@host1:port?...#proxy1",
    "vless://uuid@host2:port?...#proxy2"
  ]
}
```

**Error handling:**
- xray not found â†’ Warning logged, falls back to direct connection
- Invalid VLESS URL â†’ Error shown in UI at save time
- Proxy fails â†’ Auto-switch to next proxy (if multiple configured)
- All proxies fail â†’ Fallback to direct connection
- VLESS server unreachable â†’ SDK timeout, error returned to user

## CLI-OpenAI Bridge (Claude Code)

The Claude Bridge provider wraps the local `claude` CLI (Claude Code) into an OpenAI-compatible API via a bridge subprocess. This allows using Claude Code as an LLM backend without an API key.

**How it works:**
```
Admin Panel â†’ Select "Claude Bridge" provider â†’ Click "Use"
                                                      â†“
                                              BridgeProcessManager.start()
                                                      â†“
                                          services/bridge/ (FastAPI on port 8787)
                                                      â†“
                                              claude CLI (subprocess)
                                                      â†“
                                          OpenAICompatibleProvider â†’ /v1/chat/completions
```

**Setup:**
1. Ensure `claude` CLI is installed and authenticated
2. In Admin â†’ LLM â†’ Cloud Providers â†’ Add Provider
3. Select type "Claude Bridge (Local CLI)"
4. Configure permission level (chat/readonly/edit/full) and port
5. Click "Use" â€” bridge auto-starts

**Permission levels:**
- `chat` â€” Chat only, no file access (safe, default)
- `readonly` â€” Can read files
- `edit` â€” Can edit files
- `full` â€” Full access (dangerous)

**API endpoints:**
- `GET /admin/llm/bridge/status` â€” Bridge process status (running, pid, port, uptime)
- `POST /admin/llm/bridge/start` â€” Manually start bridge
- `POST /admin/llm/bridge/stop` â€” Manually stop bridge

**Auto-management:**
- Bridge auto-starts when switching to a `claude_bridge` provider
- Bridge auto-stops when switching to another provider or backend
- Bridge config stored in provider's `config` JSON: `{"bridge_port": 8787, "permission_level": "chat"}`

**Key files:**
- `bridge_manager.py` â€” Process manager (start/stop/status)
- `services/bridge/` â€” Full bridge source (FastAPI server)
- `services/bridge/.env` â€” Bridge configuration

## GSM Telephony (SIM7600E-H)

Support for GSM telephony via SIM7600E-H 4G LTE module for voice calls and SMS.

**Hardware:**
- Module: SIM7600E-H (4G LTE, voice, SMS)
- Connection: USB to server
- Antennas: MAIN (required), AUX (optional for better signal)

**USB Ports (Linux):**
```
/dev/ttyUSB0 - Diag (diagnostics)
/dev/ttyUSB1 - NMEA (GPS data)
/dev/ttyUSB2 - AT commands â† main control port
/dev/ttyUSB3 - Modem (PPP)
/dev/ttyUSB4 - Audio (USB PCM) â† voice stream
```

**Audio format:** 8kHz, 16-bit PCM, mono (compatible with TelephonyAudioPipeline)

**Key AT commands:**
```bash
AT           # Check connection
AT+CPIN?     # SIM status
AT+CSQ       # Signal strength (0-31, 99=unknown)
AT+CREG?     # Network registration
AT+CLIP=1    # Enable Caller ID
ATA          # Answer incoming call
ATH          # Hang up
AT+CMGF=1    # SMS text mode
AT+CMGS="+7..." # Send SMS
```

**API endpoints (`/admin/gsm/`):**
- `GET /status` â€” Module status (signal, SIM, network)
- `GET/PUT /config` â€” Configuration (auto-answer, timeouts, messages)
- `GET /calls` â€” Call history
- `POST /calls/answer|hangup|dial` â€” Call control
- `GET/POST /sms` â€” SMS history and send
- `POST /at` â€” Execute AT command (debug)
- `GET /ports` â€” List serial ports

**Admin UI:** Tab "Ğ¢ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ¸Ñ" with status, calls, SMS, settings, and AT console.

## Telegram Bot Auto-Start

Telegram bots persist their running state and automatically restart after app/container restart.

**How it works:**
1. When bot is started via UI â†’ `auto_start=true` saved in DB
2. When bot is stopped via UI â†’ `auto_start=false` saved in DB
3. On app startup â†’ all bots with `auto_start=true` automatically start

**Startup logs:**
```
ğŸ“± Auto-started Telegram bot: MyBot
ğŸ“± Auto-started 2/2 Telegram bots
```

**Migration for existing databases:**
```sql
ALTER TABLE bot_instances ADD COLUMN auto_start BOOLEAN DEFAULT 0;
```

## Telegram Bot Payments

Telegram bots support accepting payments via YooKassa (RUB), YooMoney (OAuth), and Telegram Stars (XTR).

**Supported payment methods:**
- **YooKassa** â€” Russian payment provider, requires provider token from BotFather
- **YooMoney** â€” Direct wallet payments via OAuth (no BotFather token needed)
- **Telegram Stars (XTR)** â€” Telegram's native digital currency, no provider token needed

**How it works:**
1. Configure payment in Admin Panel â†’ Telegram â†’ Edit bot â†’ Payments section
2. Enable payments, add products (title, description, price in RUB/Stars)
3. Bot shows "ĞĞ¿Ğ»Ğ°Ñ‚Ğ°" button in keyboard and responds to `/pay` command
4. User selects product â†’ Telegram sends invoice â†’ payment processed
5. Payment logged to `payment_log` table, visible in admin panel

**Payment flow:**
```
/pay or "ĞĞ¿Ğ»Ğ°Ñ‚Ğ°" button â†’ send_invoice() â†’ PreCheckoutQuery (auto-approved) â†’ SuccessfulPayment â†’ log to DB
```

**YooMoney OAuth flow:**
1. Configure YooMoney client_id/secret in bot settings
2. Click "Authorize YooMoney" â†’ OAuth popup
3. User grants access â†’ callback stores access token
4. Bot can now accept YooMoney payments

**API endpoints:**
- `POST /admin/telegram/instances/{id}/payments` â€” Log payment (internal, from bot)
- `GET /admin/telegram/instances/{id}/payments` â€” Payment history (admin UI)
- `GET /admin/telegram/instances/{id}/payments/stats` â€” Payment statistics
- `GET /admin/telegram/instances/{id}/yoomoney/auth-url` â€” Get YooMoney OAuth URL
- `GET /admin/telegram/instances/{id}/yoomoney/callback` â€” OAuth callback handler
- `POST /admin/telegram/instances/{id}/yoomoney/disconnect` â€” Disconnect YooMoney
- `POST /yoomoney/webhook` â€” YooMoney payment notification webhook

**Migration for existing databases:**
```bash
python scripts/migrate_add_payment_fields.py
```

## Sales Bot Features

Telegram bots support advanced sales automation via `app/routers/bot_sales.py` (20 endpoints).

**Features:**
- **Quiz funnels** â€” lead qualification via interactive questions
- **Segment targeting** â€” different messages for different user segments
- **Pricing calculator** â€” dynamic pricing based on user responses
- **Testimonials** â€” social proof integration
- **Follow-up sequences** â€” automated drip campaigns

**Key files:**
- `app/routers/bot_sales.py` â€” API endpoints
- `app/services/sales_funnel.py` â€” Funnel logic, segmentation, pricing

**Migration:**
```bash
python scripts/migrate_sales_bot.py
```

## Fine-tuning & Project Dataset Generation

The system supports LoRA fine-tuning for Qwen2.5-7B with built-in dataset generation from project sources.

**Admin panel:** Tab "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ" (Fine-tune) â†’ "Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°"

**Project dataset sources:**
- **Sales scenarios (Ğ¢Ğ—)** â€” pricing, objection handling, case studies, multi-turn sales flows
- **FAQ from DB** â€” all FAQ entries automatically converted to training pairs
- **Technical docs** â€” installation, configuration, API, models, integrations
- **Escalation templates** â€” examples of handoff to senior support

**API endpoint:**
```bash
# Generate project dataset
curl -X POST http://localhost:8002/admin/finetune/dataset/generate-project \
  -H "Content-Type: application/json" \
  -d '{"include_tz": true, "include_faq": true, "include_docs": true, "include_escalation": true}'
```

**Output:** `finetune/datasets/project_dataset.jsonl` (same format as Telegram export dataset)

**Training pipeline:**
1. Generate project dataset OR upload Telegram export â†’ process
2. Configure LoRA params (rank, alpha, epochs, learning rate)
3. Start training (runs on GPU in background)
4. Activate trained adapter â†’ restart vLLM

**Key files:**
| File | Purpose |
|------|---------|
| `finetune_manager.py` | Dataset processing, training control, adapter management |
| `finetune/train.py` | LoRA training script (4-bit QLoRA on RTX 3060) |
| `finetune/prepare_dataset.py` | Telegram export â†’ JSONL conversion |

## Local Model Discovery

The system automatically discovers downloaded HuggingFace models in `~/.cache/huggingface/hub/`.

**Supported model types:**
- Qwen, Llama, DeepSeek, Mistral, Phi, Gemma, Yi

**Detected quantization formats:**
- AWQ, GPTQ, GGUF, BNB-4bit, EXL2, FP16

**API response:**
```json
{
  "available_models": {
    "qwen2_5_7b_instruct_awq": {
      "full_name": "Qwen/Qwen2.5-7B-Instruct-AWQ",
      "downloaded": true,
      "quant_type": "AWQ",
      "lora_support": true
    }
  }
}
```

**Models tab** in admin panel shows all local models with download status and quantization type
