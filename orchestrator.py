#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä - –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤—Å–µ —Å–µ—Ä–≤–∏—Å—ã
STT (Whisper) -> LLM (Gemini) -> TTS (XTTS v2)
"""
from fastapi import FastAPI, UploadFile, File, HTTPException, Depends, Request
from fastapi.responses import FileResponse, StreamingResponse, Response, RedirectResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import logging
from pathlib import Path
import os
from typing import Optional, List, Dict
from pydantic import BaseModel
from datetime import datetime
import json
import time
import threading
import hashlib
import re
import numpy as np
from collections import OrderedDict
import asyncio
from concurrent.futures import ThreadPoolExecutor
import soundfile as sf

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
from voice_clone_service import VoiceCloneService, INTONATION_PRESETS
from stt_service import STTService, VoskSTTService, UnifiedSTTService
from llm_service import LLMService
from piper_tts_service import PiperTTSService
from service_manager import get_service_manager, ServiceManager
from finetune_manager import get_finetune_manager, FinetuneManager, TrainingConfig, DatasetStats
from system_monitor import get_system_monitor
from tts_finetune_manager import get_tts_finetune_manager
from model_manager import get_model_manager
from auth_manager import (
    LoginRequest, LoginResponse, User,
    authenticate_user, create_access_token, get_current_user,
    get_optional_user, require_admin, get_auth_status, AUTH_ENABLED
)

# Database integration
from db.integration import (
    init_database,
    shutdown_database,
    get_database_status,
    async_chat_manager,
    async_faq_manager,
    async_preset_manager,
    async_config_manager,
    async_telegram_manager,
    async_audit_logger,
)

# vLLM –∏–º–ø–æ—Ä—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π - –ª–æ–∫–∞–ª—å–Ω–∞—è Llama —á–µ—Ä–µ–∑ vLLM)
try:
    from vllm_llm_service import VLLMLLMService
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False
    VLLMLLMService = None

# OpenVoice –∏–º–ø–æ—Ä—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π - –¥–ª—è GPU P104-100)
try:
    from openvoice_service import OpenVoiceService
    OPENVOICE_AVAILABLE = True
except ImportError:
    OPENVOICE_AVAILABLE = False
    OpenVoiceService = None

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–æ–π LLM backend –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
LLM_BACKEND = os.getenv("LLM_BACKEND", "gemini").lower()  # "gemini" –∏–ª–∏ "vllm"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============== Streaming TTS Manager ==============
class StreamingTTSManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ TTS –≤–æ –≤—Ä–µ–º—è streaming LLM.

    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    1. –í–æ –≤—Ä–µ–º—è streaming chat/completions - –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –∏ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
       –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∑–∞–ø—É—Å–∫–∞–µ–º —Å–∏–Ω—Ç–µ–∑ –≤ —Ñ–æ–Ω–æ–≤–æ–º –ø–æ—Ç–æ–∫–µ
    2. –•—Ä–∞–Ω–∏–º —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã –≤ –∫—ç—à–µ –ø–æ —Ö—ç—à—É –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    3. –ü—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ /v1/audio/speech - —Å–∫–ª–µ–∏–≤–∞–µ–º –≥–æ—Ç–æ–≤—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
    """

    def __init__(self, max_cache_size: int = 50, cache_ttl: int = 300):
        self.max_cache_size = max_cache_size
        self.cache_ttl = cache_ttl  # —Å–µ–∫—É–Ω–¥

        # –ö—ç—à: response_hash -> {"segments": [...], "full_audio": np.array, "timestamp": float}
        self._cache: OrderedDict[str, Dict] = OrderedDict()
        self._cache_lock = threading.Lock()

        # –¢–µ–∫—É—â–∏–µ —Å–µ—Å—Å–∏–∏ —Å–∏–Ω—Ç–µ–∑–∞: session_id -> {"text": str, "segments": [...], "futures": [...]}
        self._active_sessions: Dict[str, Dict] = {}
        self._session_lock = threading.Lock()

        # Thread pool –¥–ª—è —Ñ–æ–Ω–æ–≤–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="tts_")

        # –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        self._sentence_pattern = re.compile(r'([^.!?]*[.!?]+)')

        logger.info("üéôÔ∏è StreamingTTSManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _get_text_hash(self, text: str) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö—ç—à —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        normalized = text.strip().lower()
        return hashlib.md5(normalized.encode()).hexdigest()[:16]

    def _clean_old_cache(self):
        """–£–¥–∞–ª—è–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –∫—ç—à–∞"""
        now = time.time()
        with self._cache_lock:
            keys_to_delete = []
            for key, value in self._cache.items():
                if now - value.get("timestamp", 0) > self.cache_ttl:
                    keys_to_delete.append(key)
            for key in keys_to_delete:
                del self._cache[key]
                logger.debug(f"üóëÔ∏è –£–¥–∞–ª—ë–Ω —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à: {key}")

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
            while len(self._cache) > self.max_cache_size:
                self._cache.popitem(last=False)

    def start_session(self, session_id: str) -> None:
        """–ù–∞—á–∏–Ω–∞–µ—Ç –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é streaming —Å–∏–Ω—Ç–µ–∑–∞"""
        with self._session_lock:
            self._active_sessions[session_id] = {
                "text_buffer": "",
                "full_text": "",
                "segments": [],  # [(text, audio_data, sample_rate), ...]
                "pending_futures": [],
                "start_time": time.time(),
            }
        logger.info(f"üé¨ –ù–∞—á–∞—Ç–∞ —Å–µ—Å—Å–∏—è TTS: {session_id}")

    def add_text_chunk(self, session_id: str, chunk: str, voice_service) -> None:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç chunk —Ç–µ–∫—Å—Ç–∞ –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç —Å–∏–Ω—Ç–µ–∑ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ streaming LLM response.
        """
        with self._session_lock:
            if session_id not in self._active_sessions:
                return

            session = self._active_sessions[session_id]
            session["text_buffer"] += chunk
            session["full_text"] += chunk

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            buffer = session["text_buffer"]
            sentences = self._sentence_pattern.findall(buffer)

            if sentences:
                # –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                for sentence in sentences:
                    sentence = sentence.strip()
                    if len(sentence) > 3:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
                        future = self._executor.submit(
                            self._synthesize_segment,
                            sentence,
                            voice_service,
                            session_id
                        )
                        session["pending_futures"].append((sentence, future))
                        logger.info(f"üîÑ –ó–∞–ø—É—â–µ–Ω —Å–∏–Ω—Ç–µ–∑: '{sentence[:40]}...'")

                # –£–¥–∞–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –±—É—Ñ–µ—Ä–∞
                last_sentence = sentences[-1]
                idx = buffer.rfind(last_sentence) + len(last_sentence)
                session["text_buffer"] = buffer[idx:]

    def _synthesize_segment(self, text: str, voice_service, session_id: str) -> tuple:
        """–°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç (–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ thread pool)"""
        try:
            wav, sr = voice_service.synthesize(
                text=text,
                preset="natural",
                preprocess_text=True,
                split_sentences=False  # –£–∂–µ —Ä–∞–∑–±–∏–ª–∏
            )
            logger.info(f"‚úÖ –°–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω —Å–µ–≥–º–µ–Ω—Ç: '{text[:30]}...'")
            return (text, wav, sr)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Å–µ–≥–º–µ–Ω—Ç–∞: {e}")
            return (text, None, None)

    def finish_session(self, session_id: str, voice_service) -> None:
        """
        –ó–∞–≤–µ—Ä—à–∞–µ—Ç —Å–µ—Å—Å–∏—é: —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –æ—Å—Ç–∞–≤—à–∏–π—Å—è —Ç–µ–∫—Å—Ç –∏ –∫—ç—à–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
        """
        with self._session_lock:
            if session_id not in self._active_sessions:
                return

            session = self._active_sessions[session_id]

            # –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º –æ—Å—Ç–∞—Ç–æ–∫ –±—É—Ñ–µ—Ä–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
            remaining = session["text_buffer"].strip()
            if remaining and len(remaining) > 3:
                future = self._executor.submit(
                    self._synthesize_segment,
                    remaining,
                    voice_service,
                    session_id
                )
                session["pending_futures"].append((remaining, future))
                logger.info(f"üîÑ –ó–∞–ø—É—â–µ–Ω —Å–∏–Ω—Ç–µ–∑ –æ—Å—Ç–∞—Ç–∫–∞: '{remaining[:40]}...'")

            # –ñ–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö futures
            for text, future in session["pending_futures"]:
                try:
                    result = future.result(timeout=60)
                    if result[1] is not None:
                        session["segments"].append(result)
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å–∏–Ω—Ç–µ–∑–∞: {e}")

            # –°–∫–ª–µ–∏–≤–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã
            full_text = session["full_text"]
            if session["segments"]:
                self._cache_full_audio(full_text, session["segments"])

            elapsed = time.time() - session["start_time"]
            logger.info(f"‚úÖ –°–µ—Å—Å–∏—è {session_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed:.2f}s, "
                       f"—Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(session['segments'])}")

            # –£–¥–∞–ª—è–µ–º —Å–µ—Å—Å–∏—é
            del self._active_sessions[session_id]

    def _cache_full_audio(self, full_text: str, segments: list) -> None:
        """–°–∫–ª–µ–∏–≤–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã –∏ –∫—ç—à–∏—Ä—É–µ—Ç –ø–æ–ª–Ω–æ–µ –∞—É–¥–∏–æ"""
        if not segments:
            return

        # –ü–æ–ª—É—á–∞–µ–º sample rate –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
        sample_rate = segments[0][2]

        # –°–∫–ª–µ–∏–≤–∞–µ–º –∞—É–¥–∏–æ —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –ø–∞—É–∑–∞–º–∏
        pause_samples = int(0.1 * sample_rate)  # 100ms –ø–∞—É–∑–∞
        pause = np.zeros(pause_samples, dtype=np.float32)

        audio_parts = []
        for text, wav, sr in segments:
            if wav is not None:
                if isinstance(wav, list):
                    wav = np.array(wav, dtype=np.float32)
                audio_parts.append(wav)
                audio_parts.append(pause)

        if audio_parts:
            full_audio = np.concatenate(audio_parts[:-1])  # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –ø–∞—É–∑—É

            text_hash = self._get_text_hash(full_text)
            with self._cache_lock:
                self._cache[text_hash] = {
                    "full_audio": full_audio,
                    "sample_rate": sample_rate,
                    "full_text": full_text,
                    "timestamp": time.time(),
                    "segments_count": len(segments),
                }
                logger.info(f"üíæ –ó–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–æ –∞—É–¥–∏–æ: {text_hash} ({len(full_audio)/sample_rate:.2f}s)")

            self._clean_old_cache()

    def get_cached_audio(self, text: str) -> Optional[tuple]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ –¥–ª—è —Ç–µ–∫—Å—Ç–∞.
        Returns: (audio_data, sample_rate) –∏–ª–∏ None
        """
        text_hash = self._get_text_hash(text)

        with self._cache_lock:
            if text_hash in self._cache:
                cached = self._cache[text_hash]
                logger.info(f"‚ö° Cache HIT: {text_hash}")
                return (cached["full_audio"], cached["sample_rate"])

        logger.info(f"‚ùå Cache MISS: {text_hash}")
        return None

    def get_stats(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–µ–Ω–µ–¥–∂–µ—Ä–∞"""
        with self._cache_lock:
            cache_size = len(self._cache)
        with self._session_lock:
            active_sessions = len(self._active_sessions)

        return {
            "cache_size": cache_size,
            "active_sessions": active_sessions,
            "max_cache_size": self.max_cache_size,
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä streaming TTS
streaming_tts_manager: Optional[StreamingTTSManager] = None

app = FastAPI(title="AI Secretary Orchestrator", version="1.0.0")

# CORS –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã
voice_service: Optional[VoiceCloneService] = None  # XTTS (–õ–∏–¥–∏—è) - GPU CC >= 7.0
gulya_voice_service: Optional[VoiceCloneService] = None  # XTTS (–ì—É–ª—è) - GPU CC >= 7.0
piper_service: Optional[PiperTTSService] = None    # Piper (Dmitri, Irina) - CPU
openvoice_service: Optional["OpenVoiceService"] = None  # OpenVoice v2 (–õ–∏–¥–∏—è) - GPU CC 6.1+
stt_service: Optional[STTService] = None
llm_service: Optional[LLMService] = None

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–µ–∫—É—â–µ–≥–æ –≥–æ–ª–æ—Å–∞
# engine: "xtts" (–õ–∏–¥–∏—è/–ì—É–ª—è –Ω–∞ GPU CC>=7.0), "piper" (Dmitri/Irina –Ω–∞ CPU), "openvoice" (–õ–∏–¥–∏—è –Ω–∞ GPU CC 6.1+)
# –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º –ì—É–ª—é (XTTS) –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞, –∏–Ω–∞—á–µ Piper
current_voice_config = {
    "engine": "xtts",
    "voice": "gulya",  # gulya / lidia / dmitri / irina / lidia_openvoice
}

# –ü–∞–ø–∫–∞ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
TEMP_DIR = Path("./temp")
TEMP_DIR.mkdir(exist_ok=True)

# –ü–∞–ø–∫–∞ –¥–ª—è –ª–æ–≥–æ–≤ –∑–≤–æ–Ω–∫–æ–≤
CALLS_LOG_DIR = Path("./calls_log")
CALLS_LOG_DIR.mkdir(exist_ok=True)


class ConversationRequest(BaseModel):
    text: str
    session_id: Optional[str] = None


class TTSRequest(BaseModel):
    text: str
    language: str = "ru"


class OpenAISpeechRequest(BaseModel):
    """OpenAI-compatible TTS request for OpenWebUI integration"""
    model: str = "lidia-voice"
    input: str
    voice: str = "lidia"
    response_format: str = "wav"
    speed: float = 1.0


class ChatMessage(BaseModel):
    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    """OpenAI-compatible chat completion request"""
    model: str = "gulya-secretary-qwen"  # Format: {persona}-secretary-{backend}
    messages: List[ChatMessage]
    stream: bool = False
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None


@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ"""
    global voice_service, gulya_voice_service, piper_service, openvoice_service, stt_service, llm_service, streaming_tts_manager

    logger.info("üöÄ –ó–∞–ø—É—Å–∫ AI Secretary Orchestrator")

    # Initialize database first
    await init_database()

    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Piper TTS (Dmitri, Irina) - CPU, –∑–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–≤—ã–º
        logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ Piper TTS Service (CPU)...")
        try:
            piper_service = PiperTTSService()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Piper TTS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            piper_service = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenVoice v2 (–õ–∏–¥–∏—è) - GPU CC 6.1+ (P104-100)
        if OPENVOICE_AVAILABLE:
            logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ OpenVoice TTS Service (GPU CC 6.1+)...")
            try:
                openvoice_service = OpenVoiceService()
                logger.info("‚úÖ OpenVoice v2 –∑–∞–≥—Ä—É–∂–µ–Ω (P104-100)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è OpenVoice –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                openvoice_service = None
        else:
            logger.info("‚è≠Ô∏è OpenVoice –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º)")
            openvoice_service = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è XTTS (–ì—É–ª—è) - GPU CC >= 7.0, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ Voice Clone Service (XTTS - –ì—É–ª—è)...")
        try:
            gulya_voice_service = VoiceCloneService(voice_samples_dir="./–ì—É–ª—è")
            logger.info(f"‚úÖ XTTS (–ì—É–ª—è) –∑–∞–≥—Ä—É–∂–µ–Ω: {len(gulya_voice_service.voice_samples)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è XTTS (–ì—É–ª—è) –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            gulya_voice_service = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è XTTS (–õ–∏–¥–∏—è) - GPU CC >= 7.0, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
        logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ Voice Clone Service (XTTS - –õ–∏–¥–∏—è)...")
        try:
            voice_service = VoiceCloneService(voice_samples_dir="./–õ–∏–¥–∏—è")
            logger.info(f"‚úÖ XTTS (–õ–∏–¥–∏—è) –∑–∞–≥—Ä—É–∂–µ–Ω: {len(voice_service.voice_samples)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è XTTS (–õ–∏–¥–∏—è) –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (—Ç—Ä–µ–±—É–µ—Ç—Å—è GPU CC >= 7.0): {e}")
            voice_service = None

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≥–æ–ª–æ—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        global current_voice_config
        if gulya_voice_service:
            current_voice_config = {"engine": "xtts", "voice": "gulya"}
            logger.info("üé§ –ì–æ–ª–æ—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –ì—É–ª—è (XTTS)")
        elif voice_service:
            current_voice_config = {"engine": "xtts", "voice": "lidia"}
            logger.info("üé§ –ì–æ–ª–æ—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –õ–∏–¥–∏—è (XTTS)")
        elif piper_service:
            current_voice_config = {"engine": "piper", "voice": "dmitri"}
            logger.info("üé§ –ì–æ–ª–æ—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –î–º–∏—Ç—Ä–∏–π (Piper)")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM Service (vLLM –∏–ª–∏ Gemini)
        if LLM_BACKEND == "vllm" and VLLM_AVAILABLE:
            logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ vLLM LLM Service (Llama-3.1-8B)...")
            try:
                llm_service = VLLMLLMService()
                if llm_service.is_available():
                    logger.info("‚úÖ vLLM –ø–æ–¥–∫–ª—é—á–µ–Ω")
                else:
                    logger.warning("‚ö†Ô∏è vLLM –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç, –ø—Ä–æ–±—É–µ–º Gemini...")
                    llm_service = LLMService()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è vLLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({e}), –∏—Å–ø–æ–ª—å–∑—É–µ–º Gemini")
                llm_service = LLMService()
        else:
            logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ Gemini LLM Service...")
            llm_service = LLMService()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Streaming TTS Manager
        logger.info("üì¶ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Streaming TTS Manager...")
        streaming_tts_manager = StreamingTTSManager(max_cache_size=50, cache_ttl=300)

        # STT –æ—Ç–∫–ª—é—á—ë–Ω –≤—Ä–µ–º–µ–Ω–Ω–æ - –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞—Ç–∞ –Ω–µ –Ω—É–∂–µ–Ω
        # –ú–æ–¥–µ–ª—å faster-whisper –∑–∞–≤–∏—Å–∞–µ—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
        logger.info("‚è≠Ô∏è STT –æ—Ç–∫–ª—é—á—ë–Ω (–¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞—Ç–∞ –Ω–µ –Ω—É–∂–µ–Ω)")
        stt_service = None

        logger.info("‚úÖ –û—Å–Ω–æ–≤–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        raise


@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    logger.info("üõë Shutting down AI Secretary Orchestrator")
    await shutdown_database()
    logger.info("‚úÖ Shutdown complete")


@app.get("/")
async def root():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏"""
    return {
        "status": "ok",
        "service": "AI Secretary Orchestrator",
        "endpoints": {
            "health": "/health",
            "process_call": "/process_call (POST)",
            "tts": "/tts (POST)",
            "stt": "/stt (POST)",
            "chat": "/chat (POST)",
        }
    }


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤"""
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø LLM —Å–µ—Ä–≤–∏—Å–∞
    llm_backend_type = "unknown"
    if llm_service:
        if hasattr(llm_service, 'api_url'):  # vLLM
            llm_backend_type = f"vllm ({llm_service.model_name})"
        elif hasattr(llm_service, 'model_name'):  # Gemini
            llm_backend_type = f"gemini ({llm_service.model_name})"

    services_status = {
        "voice_clone_xtts_gulya": gulya_voice_service is not None,
        "voice_clone_xtts_lidia": voice_service is not None,
        "voice_clone_openvoice": openvoice_service is not None,
        "piper_tts": piper_service is not None,
        "stt": stt_service is not None,
        "llm": llm_service is not None,
        "llm_backend": llm_backend_type,
        "streaming_tts": streaming_tts_manager is not None,
        "current_voice": current_voice_config,
    }

    # –î–ª—è health check –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª—é–±–æ–π TTS + llm
    any_tts = services_status["voice_clone_xtts_gulya"] or services_status["voice_clone_xtts_lidia"] or services_status["voice_clone_openvoice"] or services_status["piper_tts"]
    core_ok = any_tts and services_status["llm"]

    # Get database status
    db_status = await get_database_status()

    result = {
        "status": "healthy" if core_ok else "degraded",
        "services": services_status,
        "database": db_status.get("database", {}),
        "timestamp": datetime.now().isoformat()
    }

    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É streaming TTS –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
    if streaming_tts_manager is not None:
        result["streaming_tts_stats"] = streaming_tts_manager.get_stats()

    return result


def synthesize_with_current_voice(text: str, output_path: str, language: str = "ru"):
    """
    –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç —Ä–µ—á—å —Å —Ç–µ–∫—É—â–∏–º –≤—ã–±—Ä–∞–Ω–Ω—ã–º –≥–æ–ª–æ—Å–æ–º.
    –£—á–∏—Ç—ã–≤–∞–µ—Ç current_voice_config.

    Engines:
    - piper: CPU, –±—ã—Å—Ç—Ä—ã–π, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≥–æ–ª–æ—Å–∞ (dmitri, irina)
    - openvoice: GPU CC 6.1+, –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–∞ (lidia_openvoice)
    - xtts: GPU CC >= 7.0, –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (gulya, lidia)
    """
    engine = current_voice_config["engine"]
    voice = current_voice_config["voice"]

    if engine == "piper" and piper_service:
        logger.info(f"üéôÔ∏è Piper —Å–∏–Ω—Ç–µ–∑ ({voice}): '{text[:40]}...'")
        piper_service.synthesize_to_file(text, output_path, voice=voice)
    elif engine == "openvoice" and openvoice_service:
        logger.info(f"üéôÔ∏è OpenVoice —Å–∏–Ω—Ç–µ–∑ (–õ–∏–¥–∏—è): '{text[:40]}...'")
        openvoice_service.synthesize_to_file(text, output_path, language=language)
    elif engine == "xtts" and voice == "gulya" and gulya_voice_service:
        logger.info(f"üéôÔ∏è XTTS —Å–∏–Ω—Ç–µ–∑ (–ì—É–ª—è): '{text[:40]}...'")
        gulya_voice_service.synthesize_to_file(text, output_path, language=language)
    elif engine == "xtts" and voice == "lidia" and voice_service:
        logger.info(f"üéôÔ∏è XTTS —Å–∏–Ω—Ç–µ–∑ (–õ–∏–¥–∏—è): '{text[:40]}...'")
        voice_service.synthesize_to_file(text, output_path, language=language)
    elif gulya_voice_service:
        # Fallback to –ì—É–ª—è if available (default)
        logger.info(f"üéôÔ∏è XTTS —Å–∏–Ω—Ç–µ–∑ (–ì—É–ª—è fallback): '{text[:40]}...'")
        gulya_voice_service.synthesize_to_file(text, output_path, language=language)
    elif voice_service:
        # Fallback to –õ–∏–¥–∏—è if available
        logger.info(f"üéôÔ∏è XTTS —Å–∏–Ω—Ç–µ–∑ (–õ–∏–¥–∏—è fallback): '{text[:40]}...'")
        voice_service.synthesize_to_file(text, output_path, language=language)
    elif openvoice_service:
        # Fallback to OpenVoice if XTTS not available
        logger.info(f"üéôÔ∏è OpenVoice —Å–∏–Ω—Ç–µ–∑ (fallback): '{text[:40]}...'")
        openvoice_service.synthesize_to_file(text, output_path, language=language)
    elif piper_service:
        # Fallback to Piper
        logger.info(f"üéôÔ∏è Piper —Å–∏–Ω—Ç–µ–∑ (fallback): '{text[:40]}...'")
        piper_service.synthesize_to_file(text, output_path, voice="irina")
    else:
        raise RuntimeError("No TTS service available")


@app.post("/tts")
async def text_to_speech(request: TTSRequest):
    """
    –°–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏ —Å —Ç–µ–∫—É—â–∏–º –≤—ã–±—Ä–∞–Ω–Ω—ã–º –≥–æ–ª–æ—Å–æ–º
    """
    if not voice_service and not piper_service:
        raise HTTPException(status_code=503, detail="No TTS service initialized")

    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        output_file = TEMP_DIR / f"tts_{datetime.now().timestamp()}.wav"

        # –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º —Å —Ç–µ–∫—É—â–∏–º –≥–æ–ª–æ—Å–æ–º
        synthesize_with_current_voice(
            text=request.text,
            output_path=str(output_file),
            language=request.language
        )

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∞–π–ª
        return FileResponse(
            path=output_file,
            media_type="audio/wav",
            filename="response.wav"
        )

    except Exception as e:
        logger.error(f"‚ùå TTS Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/stt")
async def speech_to_text(audio: UploadFile = File(...)):
    """
    –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∏–∑ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞
    """
    if not stt_service:
        raise HTTPException(status_code=503, detail="STT service not initialized")

    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        temp_audio = TEMP_DIR / f"stt_{datetime.now().timestamp()}_{audio.filename}"

        with open(temp_audio, "wb") as f:
            content = await audio.read()
            f.write(content)

        # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º
        result = stt_service.transcribe(temp_audio, language="ru")

        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        temp_audio.unlink()

        return {
            "text": result["text"],
            "language": result["language"],
            "segments_count": len(result.get("segments", []))
        }

    except Exception as e:
        logger.error(f"‚ùå STT Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat")
async def chat(request: ConversationRequest):
    """
    –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç LLM (Gemini)
    """
    if not llm_service:
        raise HTTPException(status_code=503, detail="LLM service not initialized")

    try:
        response = llm_service.generate_response(request.text)

        return {
            "response": response,
            "session_id": request.session_id
        }

    except Exception as e:
        logger.error(f"‚ùå LLM Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/process_call")
async def process_call(audio: UploadFile = File(...)):
    """
    –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–≤–æ–Ω–∫–∞:
    1. STT - —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
    2. LLM - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    3. TTS - —Å–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞—É–¥–∏–æ —Å –æ—Ç–≤–µ—Ç–æ–º —Å–µ–∫—Ä–µ—Ç–∞—Ä—è
    """
    call_id = f"call_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    logger.info(f"üìû –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–≤–æ–Ω–∫–∞ {call_id}")

    try:
        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ö–æ–¥—è—â–∏–π –∞—É–¥–∏–æ
        input_audio = CALLS_LOG_DIR / f"{call_id}_input.wav"
        with open(input_audio, "wb") as f:
            content = await audio.read()
            f.write(content)

        # 2. –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Ä–µ—á—å (STT)
        logger.info(f"üéß STT –¥–ª—è {call_id}")
        stt_result = stt_service.transcribe(input_audio, language="ru")
        recognized_text = stt_result["text"]
        logger.info(f"üìù –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {recognized_text}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
        with open(CALLS_LOG_DIR / f"{call_id}_transcript.txt", "w") as f:
            f.write(f"USER: {recognized_text}\n")

        # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç (LLM)
        logger.info(f"ü§ñ LLM –¥–ª—è {call_id}")
        llm_response = llm_service.generate_response(recognized_text)
        logger.info(f"üí¨ –û—Ç–≤–µ—Ç: {llm_response}")

        # –î–æ–ø–æ–ª–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
        with open(CALLS_LOG_DIR / f"{call_id}_transcript.txt", "a") as f:
            f.write(f"ASSISTANT: {llm_response}\n")

        # 4. –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç (TTS)
        logger.info(f"üéôÔ∏è  TTS –¥–ª—è {call_id}")
        output_audio = CALLS_LOG_DIR / f"{call_id}_output.wav"
        voice_service.synthesize_to_file(
            text=llm_response,
            output_path=str(output_audio),
            language="ru"
        )

        logger.info(f"‚úÖ –ó–≤–æ–Ω–æ–∫ {call_id} –æ–±—Ä–∞–±–æ—Ç–∞–Ω")

        # 5. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∞—É–¥–∏–æ –æ—Ç–≤–µ—Ç
        return FileResponse(
            path=output_audio,
            media_type="audio/wav",
            filename=f"{call_id}_response.wav",
            headers={
                "X-Call-ID": call_id,
                "X-Recognized-Text": recognized_text,
                "X-Response-Text": llm_response
            }
        )

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–≤–æ–Ω–∫–∞ {call_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/reset_conversation")
async def reset_conversation():
    """–°–±—Ä–æ—Å –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞"""
    if llm_service:
        llm_service.reset_conversation()
        return {"status": "ok", "message": "Conversation history reset"}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== OpenAI-Compatible Endpoints for OpenWebUI ==============

@app.get("/v1/models")
@app.get("/v1/models/")
async def list_models():
    """OpenAI-compatible models list for OpenWebUI"""
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º backend –∏ —Å—É—Ñ—Ñ–∏–∫—Å –¥–ª—è –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏
    if llm_service and hasattr(llm_service, 'api_url'):
        # vLLM backend - –ø—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å
        model_name = getattr(llm_service, 'model_name', 'unknown')
        if model_name == "lydia" or "qwen" in model_name.lower():
            backend_suffix = "qwen"
            backend_desc = "Qwen2.5-7B + LoRA"
        elif "llama" in model_name.lower():
            backend_suffix = "llama"
            backend_desc = "Llama-3.1-8B"
        else:
            backend_suffix = "vllm"
            backend_desc = model_name
    else:
        backend_suffix = "gemini"
        backend_desc = "Gemini"

    return {
        "object": "list",
        "data": [
            {
                "id": f"gulya-secretary-{backend_suffix}",
                "object": "model",
                "created": 1700000000,
                "owned_by": "ai-secretary",
                "permission": [],
                "root": f"gulya-secretary-{backend_suffix}",
                "parent": None,
                "description": f"–ì—É–ª—è - —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å–µ–∫—Ä–µ—Ç–∞—Ä—å ({backend_desc})"
            },
            {
                "id": f"lidia-secretary-{backend_suffix}",
                "object": "model",
                "created": 1700000000,
                "owned_by": "ai-secretary",
                "permission": [],
                "root": f"lidia-secretary-{backend_suffix}",
                "parent": None,
                "description": f"–õ–∏–¥–∏—è - —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å–µ–∫—Ä–µ—Ç–∞—Ä—å ({backend_desc})"
            }
        ]
    }


@app.get("/v1/voices")
async def list_voices():
    """List available voices"""
    voices = []
    if gulya_voice_service:
        voices.append({"voice_id": "gulya", "name": "–ì—É–ª—è", "language": "ru"})
    if voice_service:
        voices.append({"voice_id": "lidia", "name": "–õ–∏–¥–∏—è", "language": "ru"})
    if piper_service:
        voices.append({"voice_id": "dmitri", "name": "–î–º–∏—Ç—Ä–∏–π", "language": "ru"})
        voices.append({"voice_id": "irina", "name": "–ò—Ä–∏–Ω–∞", "language": "ru"})
    return {"voices": voices}


@app.post("/v1/audio/speech")
async def openai_speech(request: OpenAISpeechRequest):
    """
    OpenAI-compatible TTS endpoint for OpenWebUI integration
    POST /v1/audio/speech

    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫—ç—à streaming TTS manager.
    –ï—Å–ª–∏ –∞—É–¥–∏–æ —É–∂–µ –±—ã–ª–æ –ø—Ä–µ–¥—Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–æ –≤–æ –≤—Ä–µ–º—è streaming LLM - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–≥–Ω–æ–≤–µ–Ω–Ω–æ.
    """
    if not voice_service and not piper_service:
        raise HTTPException(status_code=503, detail="No TTS service initialized")

    try:
        output_file = TEMP_DIR / f"speech_{datetime.now().timestamp()}.wav"
        start_time = time.time()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à streaming TTS (—Ç–æ–ª—å–∫–æ –¥–ª—è XTTS)
        cached_audio = None
        if current_voice_config["engine"] == "xtts" and streaming_tts_manager is not None:
            cached_audio = streaming_tts_manager.get_cached_audio(request.input)

        if cached_audio is not None:
            # Cache HIT - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥—Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ
            audio_data, sample_rate = cached_audio
            sf.write(str(output_file), audio_data, sample_rate)
            elapsed = time.time() - start_time
            logger.info(f"‚ö° TTS –∏–∑ –∫—ç—à–∞ –∑–∞ {elapsed:.3f}s (vs ~5-10s –æ–±—ã—á–Ω—ã–π —Å–∏–Ω—Ç–µ–∑)")
        else:
            # Cache MISS - —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º —Å —Ç–µ–∫—É—â–∏–º –≥–æ–ª–æ—Å–æ–º
            synthesize_with_current_voice(
                text=request.input,
                output_path=str(output_file),
                language="ru"
            )
            elapsed = time.time() - start_time
            logger.info(f"üéôÔ∏è TTS —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω –∑–∞ {elapsed:.2f}s")

        return FileResponse(
            path=output_file,
            media_type="audio/wav",
            filename="speech.wav"
        )

    except Exception as e:
        logger.error(f"‚ùå OpenAI TTS Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """
    OpenAI-compatible chat completions endpoint for OpenWebUI
    Supports both streaming and non-streaming responses.
    –ü—Ä–∏ streaming - –∑–∞–ø—É—Å–∫–∞–µ—Ç —Ñ–æ–Ω–æ–≤—ã–π —Å–∏–Ω—Ç–µ–∑ TTS –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º.
    """
    if not llm_service:
        raise HTTPException(status_code=503, detail="LLM service not initialized")

    logger.info(f"üí¨ Chat completions request: stream={request.stream}, messages={len(request.messages)}")

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Pydantic –º–æ–¥–µ–ª–∏ –≤ dict
    messages = [{"role": m.role, "content": m.content} for m in request.messages]

    if request.stream:
        # Streaming response (SSE) —Å —Ñ–æ–Ω–æ–≤—ã–º —Å–∏–Ω—Ç–µ–∑–æ–º TTS
        async def generate_stream():
            created = int(time.time())
            chunk_id = f"chatcmpl-{created}"
            session_id = f"tts-{created}"

            # –ù–∞—á–∏–Ω–∞–µ–º —Å–µ—Å—Å–∏—é streaming TTS –µ—Å–ª–∏ —Å–µ—Ä–≤–∏—Å—ã –¥–æ—Å—Ç—É–ø–Ω—ã
            use_streaming_tts = (
                streaming_tts_manager is not None and
                voice_service is not None
            )

            if use_streaming_tts:
                streaming_tts_manager.start_session(session_id)
                logger.info(f"üé¨ Streaming TTS –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Å–µ—Å—Å–∏–∏ {session_id}")

            try:
                for text_chunk in llm_service.generate_response_from_messages(messages, stream=True):
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º chunk –∫–ª–∏–µ–Ω—Ç—É
                    chunk_data = {
                        "id": chunk_id,
                        "object": "chat.completion.chunk",
                        "created": created,
                        "model": request.model,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": text_chunk},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"

                    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º chunk –≤ streaming TTS manager
                    if use_streaming_tts and text_chunk:
                        streaming_tts_manager.add_text_chunk(
                            session_id, text_chunk, voice_service
                        )

                # Final chunk
                final_chunk = {
                    "id": chunk_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                yield "data: [DONE]\n\n"

                # –ó–∞–≤–µ—Ä—à–∞–µ–º —Å–µ—Å—Å–∏—é TTS (—Å–∫–ª–µ–∏–≤–∞–µ—Ç –∏ –∫—ç—à–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ)
                if use_streaming_tts:
                    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å response
                    threading.Thread(
                        target=streaming_tts_manager.finish_session,
                        args=(session_id, voice_service),
                        daemon=True
                    ).start()

            except Exception as e:
                logger.error(f"‚ùå Streaming error: {e}")
                error_chunk = {
                    "error": {"message": str(e), "type": "server_error"}
                }
                yield f"data: {json.dumps(error_chunk)}\n\n"

        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            }
        )
    else:
        # Non-streaming response
        try:
            response_text = llm_service.generate_response_from_messages(messages, stream=False)

            # Consume generator if it returns one
            if hasattr(response_text, '__iter__') and not isinstance(response_text, str):
                response_text = "".join(response_text)

            return {
                "id": f"chatcmpl-{int(time.time())}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": request.model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response_text
                    },
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0
                }
            }
        except Exception as e:
            logger.error(f"‚ùå Chat completions error: {e}")
            raise HTTPException(status_code=500, detail=str(e))


# ============== Admin Web Interface ==============
# Vue 3 admin panel served from /admin (see Static Files section below)


# ============== Admin API Endpoints ==============

# ============== Auth Endpoints ==============

@app.post("/admin/auth/login", response_model=LoginResponse)
async def admin_login(request: LoginRequest):
    """
    Authenticate user and return JWT token.

    Default credentials: admin / admin
    Set ADMIN_USERNAME and ADMIN_PASSWORD_HASH env vars for production.
    """
    user = authenticate_user(request.username, request.password)
    if not user:
        raise HTTPException(
            status_code=401,
            detail="Invalid username or password"
        )

    token, expires_in = create_access_token(user.username, user.role)
    return LoginResponse(
        access_token=token,
        token_type="bearer",
        expires_in=expires_in
    )


@app.get("/admin/auth/me")
async def admin_get_current_user(user: User = Depends(get_current_user)):
    """Get current authenticated user info"""
    return {
        "username": user.username,
        "role": user.role
    }


@app.get("/admin/auth/status")
async def admin_auth_status():
    """Get authentication configuration status"""
    return get_auth_status()


# ============== Admin Models ==============

class AdminTTSPresetRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ—Å–µ—Ç–∞ TTS"""
    preset: str  # warm, calm, energetic, natural, neutral


class AdminLLMPromptRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
    prompt: str


class AdminLLMModelRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ LLM"""
    model: str  # gemini-2.5-flash, gemini-2.5-pro


class AdminTTSTestRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã–π —Å–∏–Ω—Ç–µ–∑"""
    text: str
    preset: str = "natural"


@app.get("/admin/status")
async def admin_status():
    """–ü–æ–ª–Ω—ã–π —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –∞–¥–º–∏–Ω–∫–∏"""
    import torch

    status = {
        "orchestrator": "running",
        "services": {
            "voice_clone": voice_service is not None,
            "llm": llm_service is not None,
            "stt": stt_service is not None,
            "streaming_tts": streaming_tts_manager is not None,
            "piper_tts": piper_service is not None,
        },
        "gpu": None,
        "streaming_tts_stats": None,
        "llm_config": None,
        "tts_config": None,
    }

    # GPU –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    if torch.cuda.is_available():
        gpu_info = []
        for i in range(torch.cuda.device_count()):
            try:
                name = torch.cuda.get_device_name(i)
                total = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                allocated = torch.cuda.memory_allocated(i) / (1024**3)
                gpu_info.append({
                    "id": i,
                    "name": name,
                    "total_gb": round(total, 2),
                    "used_gb": round(allocated, 2),
                })
            except Exception:
                pass
        status["gpu"] = gpu_info

    # Streaming TTS —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if streaming_tts_manager:
        status["streaming_tts_stats"] = streaming_tts_manager.get_stats()

    # LLM –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    if llm_service:
        if hasattr(llm_service, 'get_config'):
            status["llm_config"] = llm_service.get_config()
        else:
            # –î–ª—è vLLM –∏ –¥—Ä—É–≥–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –±–µ–∑ get_config
            status["llm_config"] = {
                "model_name": getattr(llm_service, 'model_name', 'unknown'),
                "api_url": getattr(llm_service, 'api_url', None),
                "backend": "vllm" if hasattr(llm_service, 'api_url') else "gemini",
            }

    # TTS –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    xtts_svc = gulya_voice_service or voice_service
    if xtts_svc:
        status["tts_config"] = {
            "device": xtts_svc.device,
            "default_preset": xtts_svc.default_preset,
            "samples_count": len(xtts_svc.voice_samples),
            "cache_dir": str(xtts_svc.cache_dir),
        }

    return status


@app.get("/admin/tts/presets")
async def admin_tts_presets():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö TTS –ø—Ä–µ—Å–µ—Ç–æ–≤"""
    from voice_clone_service import INTONATION_PRESETS

    presets = {}
    for name, preset in INTONATION_PRESETS.items():
        presets[name] = {
            "display_name": preset.name,
            "temperature": preset.temperature,
            "repetition_penalty": preset.repetition_penalty,
            "top_k": preset.top_k,
            "top_p": preset.top_p,
            "speed": preset.speed,
        }

    xtts_svc = gulya_voice_service or voice_service
    current = xtts_svc.default_preset if xtts_svc else "natural"

    return {
        "presets": presets,
        "current": current,
    }


@app.post("/admin/tts/preset")
async def admin_set_tts_preset(request: AdminTTSPresetRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–µ–∫—É—â–∏–π –ø—Ä–µ—Å–µ—Ç TTS"""
    from voice_clone_service import INTONATION_PRESETS

    if request.preset not in INTONATION_PRESETS:
        raise HTTPException(
            status_code=400,
            detail=f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–µ—Å–µ—Ç: {request.preset}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {list(INTONATION_PRESETS.keys())}"
        )

    xtts_svc = gulya_voice_service or voice_service
    if xtts_svc:
        xtts_svc.default_preset = request.preset
        preset = INTONATION_PRESETS[request.preset]
        return {
            "status": "ok",
            "preset": request.preset,
            "display_name": preset.name,
            "settings": {
                "temperature": preset.temperature,
                "speed": preset.speed,
            }
        }

    raise HTTPException(status_code=503, detail="No XTTS voice service available")


@app.post("/admin/tts/test")
async def admin_tts_test(request: AdminTTSTestRequest):
    """–¢–µ—Å—Ç–æ–≤—ã–π —Å–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏ - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞—É–¥–∏–æ-—Ñ–∞–π–ª –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –≤ –±—Ä–∞—É–∑–µ—Ä–µ"""
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–π –≥–æ–ª–æ—Å
    engine = current_voice_config.get("engine", "xtts")
    voice = current_voice_config.get("voice", "gulya")

    try:
        import time as t
        start = t.time()

        output_file = TEMP_DIR / f"admin_test_{datetime.now().timestamp()}.wav"

        # –í—ã–±–∏—Ä–∞–µ–º TTS —Å–µ—Ä–≤–∏—Å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ –¥–≤–∏–∂–∫–∞
        if engine == "piper" and piper_service:
            # Piper TTS (CPU)
            piper_service.synthesize_to_file(
                text=request.text,
                output_path=str(output_file),
                voice=voice  # dmitri –∏–ª–∏ irina
            )
            logger.info(f"üîä Piper TTS test: voice={voice}")
        elif engine == "openvoice" and openvoice_service:
            # OpenVoice v2
            openvoice_service.synthesize_to_file(
                text=request.text,
                output_path=str(output_file),
                language="ru"
            )
            logger.info(f"üîä OpenVoice TTS test")
        elif engine == "xtts":
            # XTTS v2
            tts_service = None
            if voice == "gulya" and gulya_voice_service:
                tts_service = gulya_voice_service
            elif voice == "lidia" and voice_service:
                tts_service = voice_service
            elif gulya_voice_service:
                tts_service = gulya_voice_service
            elif voice_service:
                tts_service = voice_service

            if not tts_service:
                raise HTTPException(status_code=503, detail="No XTTS voice service available")

            tts_service.synthesize_to_file(
                text=request.text,
                output_path=str(output_file),
                preset=request.preset,
                language="ru"
            )
            logger.info(f"üîä XTTS TTS test: voice={voice}, preset={request.preset}")
        else:
            # Fallback - –ø–æ–ø—Ä–æ–±—É–µ–º –ª—é–±–æ–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —Å–µ—Ä–≤–∏—Å
            if piper_service:
                piper_service.synthesize_to_file(
                    text=request.text,
                    output_path=str(output_file),
                    voice="dmitri"
                )
            elif gulya_voice_service:
                gulya_voice_service.synthesize_to_file(
                    text=request.text,
                    output_path=str(output_file),
                    preset=request.preset,
                    language="ru"
                )
            else:
                raise HTTPException(status_code=503, detail="No TTS service available")

        elapsed = t.time() - start

        # –ü–æ–ª—É—á–∞–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ
        import wave
        with wave.open(str(output_file), 'rb') as wf:
            frames = wf.getnframes()
            rate = wf.getframerate()
            duration = frames / float(rate)

        logger.info(f"üîä TTS test: {duration:.2f}s audio in {elapsed:.2f}s (RTF: {elapsed/duration:.2f})")

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∞—É–¥–∏–æ-—Ñ–∞–π–ª –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –≤ –±—Ä–∞—É–∑–µ—Ä–µ
        from fastapi.responses import FileResponse
        return FileResponse(
            path=str(output_file),
            media_type="audio/wav",
            filename="test_synthesis.wav",
            headers={
                "X-Duration-Sec": str(round(duration, 2)),
                "X-Synthesis-Time-Sec": str(round(elapsed, 2)),
                "X-RTF": str(round(elapsed / duration, 2) if duration > 0 else 0),
            }
        )

    except Exception as e:
        logger.error(f"‚ùå Admin TTS test error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/admin/tts/cache")
async def admin_tts_cache():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ streaming TTS"""
    if streaming_tts_manager:
        return streaming_tts_manager.get_stats()
    return {"cache_size": 0, "active_sessions": 0}


@app.delete("/admin/tts/cache")
async def admin_clear_tts_cache():
    """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à streaming TTS"""
    if streaming_tts_manager:
        with streaming_tts_manager._cache_lock:
            count = len(streaming_tts_manager._cache)
            streaming_tts_manager._cache.clear()
        return {"status": "ok", "cleared_items": count}
    return {"status": "ok", "cleared_items": 0}


# ============== STT API ==============

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ STT —Å–µ—Ä–≤–∏—Å—ã (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)
_vosk_service: Optional[VoskSTTService] = None
_unified_stt: Optional[UnifiedSTTService] = None


def get_vosk_service() -> Optional[VoskSTTService]:
    """–ü–æ–ª—É—á–∏—Ç—å Vosk STT —Å–µ—Ä–≤–∏—Å (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)"""
    global _vosk_service
    if _vosk_service is None:
        try:
            _vosk_service = VoskSTTService(language="ru", model_size="small")
            logger.info("‚úÖ Vosk STT –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Vosk STT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    return _vosk_service


def get_unified_stt() -> Optional[UnifiedSTTService]:
    """–ü–æ–ª—É—á–∏—Ç—å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π STT —Å–µ—Ä–≤–∏—Å"""
    global _unified_stt
    if _unified_stt is None:
        try:
            _unified_stt = UnifiedSTTService(language="ru", prefer_vosk=True)
            logger.info("‚úÖ Unified STT –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Unified STT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    return _unified_stt


class STTTranscribeRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏"""
    language: str = "ru"
    engine: str = "auto"  # auto, vosk, whisper


@app.get("/admin/stt/status")
async def admin_stt_status():
    """–°—Ç–∞—Ç—É—Å STT —Å–µ—Ä–≤–∏—Å–æ–≤"""
    vosk_available = False
    whisper_available = False
    vosk_model = None

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Vosk
    try:
        vosk = get_vosk_service()
        if vosk:
            vosk_available = True
            vosk_model = str(vosk.model_path.name) if vosk.model_path else None
    except:
        pass

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Whisper
    try:
        from faster_whisper import WhisperModel
        whisper_available = True
    except:
        pass

    return {
        "vosk": {
            "available": vosk_available,
            "model": vosk_model,
            "realtime": True,
            "offline": True
        },
        "whisper": {
            "available": whisper_available,
            "realtime": False,
            "offline": True
        },
        "preferred_engine": "vosk" if vosk_available else ("whisper" if whisper_available else None)
    }


@app.get("/admin/stt/models")
async def admin_stt_models():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π STT"""
    models_dir = Path("models/vosk")
    models = []

    if models_dir.exists():
        for path in models_dir.iterdir():
            if path.is_dir() and "model" in path.name.lower():
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏
                size_bytes = sum(f.stat().st_size for f in path.rglob("*") if f.is_file())
                models.append({
                    "name": path.name,
                    "path": str(path),
                    "size_mb": round(size_bytes / (1024 * 1024), 1),
                    "language": "ru" if "ru" in path.name else ("en" if "en" in path.name else "unknown")
                })

    return {
        "models_dir": str(models_dir),
        "models": models,
        "download_url": "https://alphacephei.com/vosk/models"
    }


@app.post("/admin/stt/transcribe")
async def admin_stt_transcribe(
    file: UploadFile = File(...),
    language: str = "ru",
    engine: str = "auto"
):
    """
    –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞

    Args:
        file: WAV/MP3 –∞—É–¥–∏–æ —Ñ–∞–π–ª
        language: –Ø–∑—ã–∫ (ru, en)
        engine: –î–≤–∏–∂–æ–∫ (auto, vosk, whisper)
    """
    import tempfile

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
        content = await file.read()
        tmp.write(content)
        tmp_path = tmp.name

    try:
        # –í—ã–±–∏—Ä–∞–µ–º –¥–≤–∏–∂–æ–∫
        if engine == "vosk" or (engine == "auto" and get_vosk_service()):
            vosk = get_vosk_service()
            if not vosk:
                raise HTTPException(status_code=503, detail="Vosk STT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            result = vosk.transcribe(tmp_path, language)
            result["engine"] = "vosk"

        elif engine == "whisper":
            unified = get_unified_stt()
            if not unified:
                raise HTTPException(status_code=503, detail="Whisper STT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            result = unified.transcribe(tmp_path, language, use_whisper=True)
            result["engine"] = "whisper"

        else:
            unified = get_unified_stt()
            if not unified:
                raise HTTPException(status_code=503, detail="STT —Å–µ—Ä–≤–∏—Å—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
            result = unified.transcribe(tmp_path, language)
            result["engine"] = "auto"

        return result

    finally:
        Path(tmp_path).unlink(missing_ok=True)


@app.post("/admin/stt/test")
async def admin_stt_test(text_to_speak: str = "–ü—Ä–∏–≤–µ—Ç, —ç—Ç–æ —Ç–µ—Å—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏"):
    """
    –¢–µ—Å—Ç STT: —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º —Ä–µ—á—å —á–µ—Ä–µ–∑ TTS, –∑–∞—Ç–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞—ë–º –æ–±—Ä–∞—Ç–Ω–æ

    –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ STT
    """
    import tempfile

    # –°–Ω–∞—á–∞–ª–∞ —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º —Ä–µ—á—å
    tts_service = gulya_voice_service or voice_service
    if not tts_service:
        raise HTTPException(status_code=503, detail="TTS —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

    vosk = get_vosk_service()
    if not vosk:
        raise HTTPException(status_code=503, detail="Vosk STT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
        tmp_path = tmp.name

    try:
        # –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º
        tts_service.synthesize(text_to_speak, tmp_path)

        # –†–∞—Å–ø–æ–∑–Ω–∞—ë–º
        result = vosk.transcribe(tmp_path)

        return {
            "original_text": text_to_speak,
            "recognized_text": result["text"],
            "match": text_to_speak.lower().strip() == result["text"].lower().strip(),
            "words_count": len(result.get("words", []))
        }

    finally:
        Path(tmp_path).unlink(missing_ok=True)


@app.get("/admin/llm/prompt")
async def admin_get_llm_prompt():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç LLM"""
    if llm_service:
        persona = getattr(llm_service, 'current_persona', None) or os.getenv("SECRETARY_PERSONA", "gulya")
        return {
            "prompt": llm_service.system_prompt,
            "model": llm_service.model_name,
            "persona": persona,
        }
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.post("/admin/llm/prompt")
async def admin_set_llm_prompt(request: AdminLLMPromptRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–æ–≤—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç LLM"""
    if llm_service:
        llm_service.set_system_prompt(request.prompt)
        return {
            "status": "ok",
            "prompt": request.prompt[:100] + "..." if len(request.prompt) > 100 else request.prompt,
        }
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.get("/admin/llm/model")
async def admin_get_llm_model():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å LLM"""
    if llm_service:
        return {"model": llm_service.model_name}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.post("/admin/llm/model")
async def admin_set_llm_model(request: AdminLLMModelRequest):
    """–ò–∑–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å LLM"""
    allowed_models = ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash"]

    if request.model not in allowed_models:
        raise HTTPException(
            status_code=400,
            detail=f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {request.model}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {allowed_models}"
        )

    if llm_service:
        try:
            llm_service.set_model(request.model)
            return {"status": "ok", "model": request.model}
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.delete("/admin/llm/history")
async def admin_clear_llm_history():
    """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ LLM"""
    if llm_service:
        count = len(llm_service.conversation_history)
        llm_service.reset_conversation()
        return {"status": "ok", "cleared_messages": count}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.get("/admin/llm/history")
async def admin_get_llm_history():
    """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ LLM"""
    if llm_service:
        return {
            "history": llm_service.conversation_history,
            "count": len(llm_service.conversation_history),
        }
    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== Voice Selection API ==============

class AdminVoiceRequest(BaseModel):
    voice: str  # gulya / lidia / dmitri / irina


@app.get("/admin/voices")
async def admin_get_voices():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤"""
    voices = []

    # XTTS –≥–æ–ª–æ—Å (–ì—É–ª—è) - —Ç—Ä–µ–±—É–µ—Ç GPU CC >= 7.0 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    if gulya_voice_service:
        voices.append({
            "id": "gulya",
            "name": "–ì—É–ª—è (XTTS)",
            "engine": "xtts",
            "description": "–ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥–æ–ª–æ—Å –ì—É–ª–∏ (XTTS v2, GPU CC >= 7.0)",
            "available": True,
            "samples_count": len(gulya_voice_service.voice_samples),
            "default": True,
        })

    # XTTS –≥–æ–ª–æ—Å (–õ–∏–¥–∏—è) - —Ç—Ä–µ–±—É–µ—Ç GPU CC >= 7.0
    if voice_service:
        voices.append({
            "id": "lidia",
            "name": "–õ–∏–¥–∏—è (XTTS)",
            "engine": "xtts",
            "description": "–ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥–æ–ª–æ—Å –õ–∏–¥–∏–∏ (XTTS v2, GPU CC >= 7.0)",
            "available": True,
            "samples_count": len(voice_service.voice_samples),
        })

    # OpenVoice –≥–æ–ª–æ—Å (–õ–∏–¥–∏—è) - —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ GPU CC 6.1+
    if openvoice_service:
        voices.append({
            "id": "lidia_openvoice",
            "name": "–õ–∏–¥–∏—è (OpenVoice)",
            "engine": "openvoice",
            "description": "–ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥–æ–ª–æ—Å (OpenVoice v2, GPU CC 6.1+)",
            "available": True,
            "samples_count": len(openvoice_service.voice_samples) if openvoice_service.voice_samples else 0,
        })

    # Piper –≥–æ–ª–æ—Å–∞ (CPU)
    if piper_service:
        piper_voices = piper_service.get_available_voices()
        for voice_id, info in piper_voices.items():
            voices.append({
                "id": voice_id,
                "name": info["name"],
                "engine": "piper",
                "description": info["description"],
                "available": info["available"],
            })

    return {
        "voices": voices,
        "current": current_voice_config,
    }


@app.get("/admin/voice")
async def admin_get_current_voice():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π –≤—ã–±—Ä–∞–Ω–Ω—ã–π –≥–æ–ª–æ—Å"""
    return current_voice_config


@app.post("/admin/voice")
async def admin_set_voice(request: AdminVoiceRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π –≥–æ–ª–æ—Å"""
    global current_voice_config

    voice_id = request.voice.lower()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≥–æ–ª–æ—Å–∞
    if voice_id == "gulya":
        if not gulya_voice_service:
            raise HTTPException(status_code=503, detail="XTTS service (–ì—É–ª—è) not available (requires GPU CC >= 7.0)")
        current_voice_config = {"engine": "xtts", "voice": "gulya"}
        logger.info(f"üé§ –ì–æ–ª–æ—Å –∏–∑–º–µ–Ω—ë–Ω –Ω–∞: –ì—É–ª—è (XTTS)")

    elif voice_id == "lidia":
        if not voice_service:
            raise HTTPException(status_code=503, detail="XTTS service (–õ–∏–¥–∏—è) not available (requires GPU CC >= 7.0)")
        current_voice_config = {"engine": "xtts", "voice": "lidia"}
        logger.info(f"üé§ –ì–æ–ª–æ—Å –∏–∑–º–µ–Ω—ë–Ω –Ω–∞: –õ–∏–¥–∏—è (XTTS)")

    elif voice_id == "lidia_openvoice":
        if not openvoice_service:
            raise HTTPException(status_code=503, detail="OpenVoice service not available")
        current_voice_config = {"engine": "openvoice", "voice": "lidia_openvoice"}
        logger.info(f"üé§ –ì–æ–ª–æ—Å –∏–∑–º–µ–Ω—ë–Ω –Ω–∞: –õ–∏–¥–∏—è (OpenVoice)")

    elif voice_id in ["dmitri", "irina"]:
        if not piper_service:
            raise HTTPException(status_code=503, detail="Piper TTS service not available")
        piper_voices = piper_service.get_available_voices()
        if voice_id not in piper_voices or not piper_voices[voice_id]["available"]:
            raise HTTPException(status_code=400, detail=f"Voice model not found: {voice_id}")
        current_voice_config = {"engine": "piper", "voice": voice_id}
        logger.info(f"üé§ –ì–æ–ª–æ—Å –∏–∑–º–µ–Ω—ë–Ω –Ω–∞: {piper_voices[voice_id]['name']} (Piper)")

    else:
        raise HTTPException(
            status_code=400,
            detail=f"Unknown voice: {voice_id}. Available: gulya, lidia, lidia_openvoice, dmitri, irina"
        )

    return {"status": "ok", **current_voice_config}


@app.post("/admin/voice/test")
async def admin_test_voice(request: AdminVoiceRequest):
    """–¢–µ—Å—Ç–æ–≤—ã–π —Å–∏–Ω—Ç–µ–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –≥–æ–ª–æ—Å–æ–º"""
    voice_id = request.voice.lower()
    test_text = "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥–æ–ª–æ—Å–∞."

    output_path = TEMP_DIR / f"voice_test_{voice_id}_{int(time.time())}.wav"

    try:
        if voice_id == "gulya":
            if not gulya_voice_service:
                raise HTTPException(status_code=503, detail="XTTS (–ì—É–ª—è) not available (requires GPU CC >= 7.0)")
            gulya_voice_service.synthesize_to_file(test_text, str(output_path), preset="natural")

        elif voice_id == "lidia":
            if not voice_service:
                raise HTTPException(status_code=503, detail="XTTS (–õ–∏–¥–∏—è) not available (requires GPU CC >= 7.0)")
            voice_service.synthesize_to_file(test_text, str(output_path), preset="natural")

        elif voice_id == "lidia_openvoice":
            if not openvoice_service:
                raise HTTPException(status_code=503, detail="OpenVoice not available")
            openvoice_service.synthesize_to_file(test_text, str(output_path), language="ru")

        elif voice_id in ["dmitri", "irina"]:
            if not piper_service:
                raise HTTPException(status_code=503, detail="Piper not available")
            piper_service.synthesize_to_file(test_text, str(output_path), voice=voice_id)

        else:
            raise HTTPException(status_code=400, detail=f"Unknown voice: {voice_id}. Available: gulya, lidia, lidia_openvoice, dmitri, irina")

        return FileResponse(
            output_path,
            media_type="audio/wav",
            filename=f"test_{voice_id}.wav"
        )

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def get_current_tts_service():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π TTS —Å–µ—Ä–≤–∏—Å –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    engine = current_voice_config["engine"]
    voice = current_voice_config["voice"]

    if engine == "xtts" and voice == "gulya":
        return gulya_voice_service, {"preset": "natural"}
    elif engine == "xtts" and voice == "lidia":
        return voice_service, {"preset": "natural"}
    elif engine == "piper":
        return piper_service, {"voice": voice}
    else:
        # Default to gulya if available
        return gulya_voice_service or voice_service, {"preset": "natural"}


# ============== Extended Admin API Endpoints ==============

# Pydantic models for new endpoints
class AdminBackendRequest(BaseModel):
    backend: str  # "vllm" or "gemini"
    stop_unused: bool = False  # –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Å–µ—Ä–≤–∏—Å (vLLM) –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è GPU


class AdminPersonaRequest(BaseModel):
    persona: str  # "gulya" or "lidia"


class AdminLLMParamsRequest(BaseModel):
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    repetition_penalty: Optional[float] = None


class AdminXTTSParamsRequest(BaseModel):
    temperature: Optional[float] = None
    repetition_penalty: Optional[float] = None
    top_k: Optional[int] = None
    top_p: Optional[float] = None
    speed: Optional[float] = None
    gpt_cond_len: Optional[int] = None
    gpt_cond_chunk_len: Optional[int] = None


class AdminPiperParamsRequest(BaseModel):
    speed: float = 1.0


class AdminCustomPresetRequest(BaseModel):
    name: str
    params: dict


class AdminFAQRequest(BaseModel):
    trigger: str
    response: str


class AdminFAQTestRequest(BaseModel):
    text: str


class AdminWidgetConfigRequest(BaseModel):
    enabled: bool = True
    title: str = "AI –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç"
    greeting: str = "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ö–æ–º–ø–∞–Ω–∏—è –®–∞–µ—Ä–≤—ç–π –î–∏-–ò–¥–∂–∏—Ç–∞–ª, —á–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?"
    placeholder: str = "–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ..."
    primary_color: str = "#6366f1"
    position: str = "right"  # "left" or "right"
    allowed_domains: List[str] = []
    tunnel_url: str = ""


class AdminTelegramConfigRequest(BaseModel):
    enabled: bool = False
    bot_token: str = ""
    api_url: str = "http://localhost:8002"
    allowed_users: List[int] = []
    admin_users: List[int] = []
    welcome_message: str = "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ–º–ø–∞–Ω–∏–∏ –®–∞–µ—Ä–≤—ç–π. –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?"
    unauthorized_message: str = "–ò–∑–≤–∏–Ω–∏—Ç–µ, —É –≤–∞—Å –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ —ç—Ç–æ–º—É –±–æ—Ç—É."
    error_message: str = "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
    typing_enabled: bool = True


class AdminFinetuneConfigRequest(BaseModel):
    lora_rank: Optional[int] = None
    lora_alpha: Optional[int] = None
    batch_size: Optional[int] = None
    gradient_accumulation_steps: Optional[int] = None
    learning_rate: Optional[float] = None
    num_epochs: Optional[int] = None
    max_seq_length: Optional[int] = None
    output_dir: Optional[str] = None


class AdminAdapterRequest(BaseModel):
    adapter: str

class AdminAuditQueryRequest(BaseModel):
    action: Optional[str] = None
    resource: Optional[str] = None
    user_id: Optional[str] = None
    from_date: Optional[str] = None  # ISO format
    to_date: Optional[str] = None    # ISO format
    limit: int = 100
    offset: int = 0


# ============== Chat Models & Manager ==============

class ChatMessageModel(BaseModel):
    id: str
    role: str
    content: str
    timestamp: str
    edited: bool = False


class ChatSessionModel(BaseModel):
    id: str
    title: str
    messages: List[ChatMessageModel]
    system_prompt: Optional[str] = None
    created: str
    updated: str


class CreateSessionRequest(BaseModel):
    title: Optional[str] = None
    system_prompt: Optional[str] = None


class UpdateSessionRequest(BaseModel):
    title: Optional[str] = None
    system_prompt: Optional[str] = None


class SendMessageRequest(BaseModel):
    content: str


class EditMessageRequest(BaseModel):
    content: str


class ChatManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —á–∞—Ç-—Å–µ—Å—Å–∏–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ —Ñ–∞–π–ª"""

    def __init__(self, storage_path: Path = None):
        self.storage_path = storage_path or Path(__file__).parent / "chat_sessions.json"
        self.sessions: Dict[str, dict] = {}
        self._load()

    def _load(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–µ—Å—Å–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        if self.storage_path.exists():
            try:
                with open(self.storage_path, 'r', encoding='utf-8') as f:
                    self.sessions = json.load(f)
                logger.info(f"üí¨ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.sessions)} —á–∞—Ç-—Å–µ—Å—Å–∏–π")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞—Ç–æ–≤: {e}")
                self.sessions = {}
        else:
            self.sessions = {}

    def _save(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–µ—Å—Å–∏–∏ –≤ —Ñ–∞–π–ª"""
        try:
            with open(self.storage_path, 'w', encoding='utf-8') as f:
                json.dump(self.sessions, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–∞—Ç–æ–≤: {e}")

    def _generate_id(self) -> str:
        return f"chat_{int(time.time() * 1000)}"

    def _generate_message_id(self) -> str:
        return f"msg_{int(time.time() * 1000)}_{hashlib.md5(str(time.time()).encode()).hexdigest()[:6]}"

    def list_sessions(self) -> List[dict]:
        """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–µ—Å—Å–∏–π (–∫—Ä–∞—Ç–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)"""
        summaries = []
        for sid, session in sorted(
            self.sessions.items(),
            key=lambda x: x[1].get('updated', ''),
            reverse=True
        ):
            messages = session.get('messages', [])
            last_msg = messages[-1]['content'][:100] if messages else None
            summaries.append({
                'id': sid,
                'title': session.get('title', '–ù–æ–≤—ã–π —á–∞—Ç'),
                'message_count': len(messages),
                'last_message': last_msg,
                'created': session.get('created'),
                'updated': session.get('updated'),
            })
        return summaries

    def get_session(self, session_id: str) -> Optional[dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é —Å–µ—Å—Å–∏—é"""
        return self.sessions.get(session_id)

    def create_session(self, title: str = None, system_prompt: str = None) -> dict:
        """–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é"""
        session_id = self._generate_id()
        now = datetime.now().isoformat()
        session = {
            'id': session_id,
            'title': title or '–ù–æ–≤—ã–π —á–∞—Ç',
            'messages': [],
            'system_prompt': system_prompt,
            'created': now,
            'updated': now,
        }
        self.sessions[session_id] = session
        self._save()
        return session

    def update_session(self, session_id: str, title: str = None, system_prompt: str = None) -> Optional[dict]:
        """–û–±–Ω–æ–≤–∏—Ç—å —Å–µ—Å—Å–∏—é"""
        if session_id not in self.sessions:
            return None
        session = self.sessions[session_id]
        if title is not None:
            session['title'] = title
        if system_prompt is not None:
            session['system_prompt'] = system_prompt
        session['updated'] = datetime.now().isoformat()
        self._save()
        return session

    def delete_session(self, session_id: str) -> bool:
        """–£–¥–∞–ª–∏—Ç—å —Å–µ—Å—Å–∏—é"""
        if session_id in self.sessions:
            del self.sessions[session_id]
            self._save()
            return True
        return False

    def add_message(self, session_id: str, role: str, content: str) -> Optional[dict]:
        """–î–æ–±–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Å–µ—Å—Å–∏—é"""
        if session_id not in self.sessions:
            return None
        msg = {
            'id': self._generate_message_id(),
            'role': role,
            'content': content,
            'timestamp': datetime.now().isoformat(),
            'edited': False,
        }
        self.sessions[session_id]['messages'].append(msg)
        self.sessions[session_id]['updated'] = datetime.now().isoformat()

        # –ê–≤—Ç–æ-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
        if len(self.sessions[session_id]['messages']) == 1 and self.sessions[session_id]['title'] == '–ù–æ–≤—ã–π —á–∞—Ç':
            self.sessions[session_id]['title'] = content[:50] + ('...' if len(content) > 50 else '')

        self._save()
        return msg

    def edit_message(self, session_id: str, message_id: str, content: str) -> Optional[dict]:
        """–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ"""
        if session_id not in self.sessions:
            return None
        for msg in self.sessions[session_id]['messages']:
            if msg['id'] == message_id:
                msg['content'] = content
                msg['edited'] = True
                msg['timestamp'] = datetime.now().isoformat()
                self.sessions[session_id]['updated'] = datetime.now().isoformat()
                self._save()
                return msg
        return None

    def delete_message(self, session_id: str, message_id: str) -> bool:
        """–£–¥–∞–ª–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –≤—Å–µ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ"""
        if session_id not in self.sessions:
            return False
        messages = self.sessions[session_id]['messages']
        for i, msg in enumerate(messages):
            if msg['id'] == message_id:
                self.sessions[session_id]['messages'] = messages[:i]
                self.sessions[session_id]['updated'] = datetime.now().isoformat()
                self._save()
                return True
        return False

    def get_messages_for_llm(self, session_id: str, system_prompt: str = None) -> List[dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è LLM"""
        if session_id not in self.sessions:
            return []
        session = self.sessions[session_id]
        messages = []

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt = session.get('system_prompt') or system_prompt
        if prompt:
            messages.append({'role': 'system', 'content': prompt})

        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        for msg in session['messages']:
            messages.append({'role': msg['role'], 'content': msg['content']})

        return messages


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä —á–∞—Ç–æ–≤
chat_manager: Optional[ChatManager] = None


def get_chat_manager() -> ChatManager:
    global chat_manager
    if chat_manager is None:
        chat_manager = ChatManager()
    return chat_manager


# ============== Services Endpoints ==============

@app.get("/admin/services/status")
async def admin_services_status():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤"""
    manager = get_service_manager()
    status = manager.get_all_status()

    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –∏–∑ orchestrator
    status["services"]["xtts_gulya"]["is_running"] = gulya_voice_service is not None
    status["services"]["xtts_lidia"]["is_running"] = voice_service is not None
    status["services"]["piper"]["is_running"] = piper_service is not None
    status["services"]["openvoice"]["is_running"] = openvoice_service is not None
    status["services"]["orchestrator"]["is_running"] = True

    return status


@app.post("/admin/services/{service}/start")
async def admin_start_service(service: str):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–∏—Å"""
    manager = get_service_manager()
    return await manager.start_service(service)


@app.post("/admin/services/{service}/stop")
async def admin_stop_service(service: str):
    """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–µ—Ä–≤–∏—Å"""
    manager = get_service_manager()
    return await manager.stop_service(service)


@app.post("/admin/services/{service}/restart")
async def admin_restart_service(service: str):
    """–ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–∏—Å"""
    manager = get_service_manager()
    return await manager.restart_service(service)


@app.post("/admin/services/start-all")
async def admin_start_all_services():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ –≤–Ω–µ—à–Ω–∏–µ —Å–µ—Ä–≤–∏—Å—ã"""
    manager = get_service_manager()
    results = {}
    for service in ["vllm"]:  # –¢–æ–ª—å–∫–æ –≤–Ω–µ—à–Ω–∏–µ —Å–µ—Ä–≤–∏—Å—ã
        results[service] = await manager.start_service(service)
    return {"status": "ok", "results": results}


@app.post("/admin/services/stop-all")
async def admin_stop_all_services():
    """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Å–µ –≤–Ω–µ—à–Ω–∏–µ —Å–µ—Ä–≤–∏—Å—ã"""
    manager = get_service_manager()
    results = {}
    for service in ["vllm"]:
        results[service] = await manager.stop_service(service)
    return {"status": "ok", "results": results}


# ============== Logs Endpoints ==============

@app.get("/admin/logs")
async def admin_list_logs():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ª–æ–≥–æ–≤"""
    manager = get_service_manager()
    return {"logs": manager.get_available_logs()}


@app.get("/admin/logs/{logfile}")
async def admin_read_log(
    logfile: str,
    lines: int = 100,
    offset: int = 0,
    search: Optional[str] = None
):
    """–ü—Ä–æ—á–∏—Ç–∞—Ç—å –ª–æ–≥ —Ñ–∞–π–ª"""
    manager = get_service_manager()
    return manager.read_log(logfile, lines=lines, offset=offset, search=search)


@app.get("/admin/logs/stream/{logfile}")
async def admin_stream_log(logfile: str):
    """SSE streaming –ª–æ–≥–æ–≤"""
    manager = get_service_manager()

    async def generate():
        async for data in manager.stream_log(logfile):
            yield f"data: {data}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
    )


# ============== LLM Enhanced Endpoints ==============

@app.get("/admin/llm/backend")
async def admin_get_llm_backend():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π LLM backend"""
    if llm_service:
        backend = "vllm" if hasattr(llm_service, 'api_url') else "gemini"
        return {
            "backend": backend,
            "model": getattr(llm_service, 'model_name', 'unknown'),
            "api_url": getattr(llm_service, 'api_url', None),
        }
    return {"backend": "none", "error": "LLM service not initialized"}


@app.post("/admin/llm/backend")
async def admin_set_llm_backend(request: AdminBackendRequest):
    """–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å LLM backend —Å –≥–æ—Ä—è—á–µ–π –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–æ–π —Å–µ—Ä–≤–∏—Å–∞"""
    global LLM_BACKEND, llm_service

    if request.backend not in ["vllm", "gemini"]:
        raise HTTPException(status_code=400, detail="Invalid backend. Use 'vllm' or 'gemini'")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–∏–π –±—ç–∫–µ–Ω–¥
    current_backend = "vllm" if (llm_service and hasattr(llm_service, 'api_url')) else "gemini"
    if request.backend == current_backend:
        return {
            "status": "ok",
            "backend": request.backend,
            "message": f"–£–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {request.backend}"
        }

    manager = get_service_manager()
    stop_vllm = request.stop_unused if hasattr(request, 'stop_unused') else False

    try:
        if request.backend == "vllm":
            # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ vLLM
            logger.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ vLLM...")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–ø—É—â–µ–Ω –ª–∏ vLLM
            vllm_status = manager.get_service_status("vllm")

            if not vllm_status.get("is_running"):
                # –ó–∞–ø—É—Å–∫–∞–µ–º vLLM
                logger.info("üöÄ –ó–∞–ø—É—Å–∫ vLLM...")
                start_result = await manager.start_service("vllm")
                if start_result.get("status") != "ok":
                    raise HTTPException(
                        status_code=503,
                        detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å vLLM: {start_result.get('message', 'Unknown error')}"
                    )

                # –ñ–¥—ë–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ vLLM (–¥–æ 120 —Å–µ–∫—É–Ω–¥)
                logger.info("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ vLLM...")
                import httpx
                for i in range(60):  # 60 * 2 = 120 —Å–µ–∫—É–Ω–¥
                    await asyncio.sleep(2)
                    try:
                        async with httpx.AsyncClient() as client:
                            resp = await client.get("http://localhost:11434/health", timeout=5.0)
                            if resp.status_code == 200:
                                logger.info(f"‚úÖ vLLM –≥–æ—Ç–æ–≤ (–ø–æ–ø—ã—Ç–∫–∞ {i+1})")
                                break
                    except:
                        pass
                else:
                    raise HTTPException(status_code=503, detail="vLLM –Ω–µ —Å—Ç–∞–ª –¥–æ—Å—Ç—É–ø–µ–Ω –∑–∞ 120 —Å–µ–∫—É–Ω–¥")

            # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π vLLM —Å–µ—Ä–≤–∏—Å
            if VLLMLLMService is None:
                raise HTTPException(status_code=503, detail="VLLMLLMService –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")

            new_service = VLLMLLMService()
            if not new_service.is_available():
                raise HTTPException(status_code=503, detail="vLLM –∑–∞–ø—É—â–µ–Ω, –Ω–æ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ API")

            llm_service = new_service
            LLM_BACKEND = "vllm"
            os.environ["LLM_BACKEND"] = "vllm"

            logger.info("‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ vLLM")
            return {
                "status": "ok",
                "backend": "vllm",
                "model": getattr(llm_service, 'model_name', 'unknown'),
                "message": "–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ vLLM"
            }

        else:
            # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ Gemini
            logger.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ Gemini...")

            new_service = LLMService()
            llm_service = new_service
            LLM_BACKEND = "gemini"
            os.environ["LLM_BACKEND"] = "gemini"

            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º vLLM –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è GPU
            if stop_vllm:
                vllm_status = manager.get_service_status("vllm")
                if vllm_status.get("is_running"):
                    logger.info("üõë –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º vLLM –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è GPU...")
                    await manager.stop_service("vllm")

            logger.info("‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ Gemini")
            return {
                "status": "ok",
                "backend": "gemini",
                "model": getattr(llm_service, 'model_name', 'unknown'),
                "message": "–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ Gemini" + (" (vLLM –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)" if stop_vllm else "")
            }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –±—ç–∫–µ–Ω–¥–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/admin/llm/personas")
async def admin_get_personas():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω"""
    if llm_service and hasattr(llm_service, 'get_available_personas'):
        return {"personas": llm_service.get_available_personas()}

    # Fallback –¥–ª—è Gemini LLM Service
    from vllm_llm_service import SECRETARY_PERSONAS
    return {
        "personas": {
            pid: {"name": p["name"], "full_name": p.get("full_name", p["name"])}
            for pid, p in SECRETARY_PERSONAS.items()
        }
    }


@app.get("/admin/llm/persona")
async def admin_get_current_persona():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –ø–µ—Ä—Å–æ–Ω—É"""
    if llm_service:
        persona_id = getattr(llm_service, 'persona_id', 'gulya')
        persona = getattr(llm_service, 'persona', {})
        return {
            "id": persona_id,
            "name": persona.get("name", "Unknown"),
        }
    return {"id": "none", "error": "LLM service not initialized"}


@app.post("/admin/llm/persona")
async def admin_set_persona(request: AdminPersonaRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–µ—Ä—Å–æ–Ω—É"""
    if llm_service and hasattr(llm_service, 'set_persona'):
        success = llm_service.set_persona(request.persona)
        if success:
            return {"status": "ok", "persona": request.persona}
        raise HTTPException(status_code=400, detail=f"Persona not found: {request.persona}")
    raise HTTPException(status_code=503, detail="LLM service does not support personas")


@app.get("/admin/llm/params")
async def admin_get_llm_params():
    """–ü–æ–ª—É—á–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM"""
    if llm_service and hasattr(llm_service, 'runtime_params'):
        return {"params": llm_service.runtime_params}

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    return {
        "params": {
            "temperature": 0.7,
            "max_tokens": 512,
            "top_p": 0.9,
            "repetition_penalty": 1.1
        }
    }


@app.post("/admin/llm/params")
async def admin_set_llm_params(request: AdminLLMParamsRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM"""
    if llm_service and hasattr(llm_service, 'set_params'):
        params = {k: v for k, v in request.dict().items() if v is not None}
        llm_service.set_params(**params)
        return {"status": "ok", "params": llm_service.runtime_params}

    # –î–ª—è vLLM —Å–µ—Ä–≤–∏—Å–∞ –±–µ–∑ set_params - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∞—Ç—Ä–∏–±—É—Ç–µ
    if llm_service:
        if not hasattr(llm_service, 'runtime_params'):
            llm_service.runtime_params = {}
        params = {k: v for k, v in request.dict().items() if v is not None}
        llm_service.runtime_params.update(params)
        return {"status": "ok", "params": llm_service.runtime_params}

    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.get("/admin/llm/prompt/{persona}")
async def admin_get_persona_prompt(persona: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω—ã"""
    try:
        from vllm_llm_service import SECRETARY_PERSONAS
        if persona in SECRETARY_PERSONAS:
            return {
                "persona": persona,
                "prompt": SECRETARY_PERSONAS[persona]["prompt"]
            }
        raise HTTPException(status_code=404, detail=f"Persona not found: {persona}")
    except ImportError:
        raise HTTPException(status_code=503, detail="vLLM service not available")


@app.post("/admin/llm/prompt/{persona}")
async def admin_set_persona_prompt(persona: str, request: AdminLLMPromptRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω—ã"""
    try:
        from vllm_llm_service import SECRETARY_PERSONAS
        if persona not in SECRETARY_PERSONAS:
            raise HTTPException(status_code=404, detail=f"Persona not found: {persona}")

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–º–ø—Ç
        SECRETARY_PERSONAS[persona]["prompt"] = request.prompt

        # –ï—Å–ª–∏ —ç—Ç–æ —Ç–µ–∫—É—â–∞—è –ø–µ—Ä—Å–æ–Ω–∞ - –æ–±–Ω–æ–≤–ª—è–µ–º –≤ —Å–µ—Ä–≤–∏—Å–µ
        if llm_service and hasattr(llm_service, 'persona_id') and llm_service.persona_id == persona:
            llm_service.system_prompt = request.prompt

        return {"status": "ok", "persona": persona}
    except ImportError:
        raise HTTPException(status_code=503, detail="vLLM service not available")


@app.post("/admin/llm/prompt/{persona}/reset")
async def admin_reset_persona_prompt(persona: str):
    """–°–±—Ä–æ—Å–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –ø–µ—Ä—Å–æ–Ω—ã –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
    # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
    raise HTTPException(status_code=501, detail="Not implemented yet")


# ============== TTS Enhanced Endpoints ==============

@app.get("/admin/tts/xtts/params")
async def admin_get_xtts_params():
    """–ü–æ–ª—É—á–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã XTTS"""
    service = gulya_voice_service or voice_service
    if not service:
        raise HTTPException(status_code=503, detail="XTTS service not available")

    preset = service.get_preset(service.default_preset)
    return {
        "default_preset": service.default_preset,
        "current_params": {
            "temperature": preset.temperature,
            "repetition_penalty": preset.repetition_penalty,
            "top_k": preset.top_k,
            "top_p": preset.top_p,
            "speed": preset.speed,
            "gpt_cond_len": preset.gpt_cond_len,
            "gpt_cond_chunk_len": preset.gpt_cond_chunk_len,
        }
    }


@app.post("/admin/tts/xtts/params")
async def admin_set_xtts_params(request: AdminXTTSParamsRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã XTTS (–¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞)"""
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    global xtts_param_overrides
    if 'xtts_param_overrides' not in globals():
        xtts_param_overrides = {}

    params = {k: v for k, v in request.dict().items() if v is not None}
    xtts_param_overrides.update(params)

    return {"status": "ok", "params": xtts_param_overrides}


@app.get("/admin/tts/piper/params")
async def admin_get_piper_params():
    """–ü–æ–ª—É—á–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Piper TTS"""
    if not piper_service:
        raise HTTPException(status_code=503, detail="Piper service not available")

    return {
        "speed": getattr(piper_service, 'speed', 1.0),
        "voices": piper_service.get_available_voices()
    }


@app.post("/admin/tts/piper/params")
async def admin_set_piper_params(request: AdminPiperParamsRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Piper TTS"""
    if not piper_service:
        raise HTTPException(status_code=503, detail="Piper service not available")

    piper_service.speed = request.speed
    return {"status": "ok", "speed": request.speed}


@app.get("/admin/tts/presets/custom")
async def admin_get_custom_presets():
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–µ—Å–µ—Ç—ã TTS"""
    presets = await async_preset_manager.get_custom()
    return {"presets": presets}


@app.post("/admin/tts/presets/custom")
async def admin_create_custom_preset(request: AdminCustomPresetRequest):
    """–°–æ–∑–¥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–µ—Å–µ—Ç TTS"""
    await async_preset_manager.create(request.name, request.params)
    return {"status": "ok", "preset": request.name}


@app.put("/admin/tts/presets/custom/{name}")
async def admin_update_custom_preset(name: str, request: AdminCustomPresetRequest):
    """–û–±–Ω–æ–≤–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–µ—Å–µ—Ç TTS"""
    result = await async_preset_manager.update(name, request.params)
    if not result:
        raise HTTPException(status_code=404, detail=f"Preset not found: {name}")

    return {"status": "ok", "preset": name}


@app.delete("/admin/tts/presets/custom/{name}")
async def admin_delete_custom_preset(name: str):
    """–£–¥–∞–ª–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–µ—Å–µ—Ç TTS"""
    if not await async_preset_manager.delete(name):
        raise HTTPException(status_code=404, detail=f"Preset not found: {name}")

    return {"status": "ok", "deleted": name}


# ============== FAQ Endpoints ==============

@app.get("/admin/faq")
async def admin_get_faq():
    """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ FAQ –∑–∞–ø–∏—Å–∏"""
    faq = await async_faq_manager.get_all()
    return {"faq": faq}


@app.post("/admin/faq")
async def admin_add_faq(request: AdminFAQRequest):
    """–î–æ–±–∞–≤–∏—Ç—å FAQ –∑–∞–ø–∏—Å—å"""
    await async_faq_manager.add(request.trigger, request.response)

    # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º FAQ –≤ LLM —Å–µ—Ä–≤–∏—Å–µ
    if llm_service and hasattr(llm_service, 'reload_faq'):
        llm_service.reload_faq()

    return {"status": "ok", "trigger": request.trigger}


@app.put("/admin/faq/{trigger}")
async def admin_update_faq(trigger: str, request: AdminFAQRequest):
    """–û–±–Ω–æ–≤–∏—Ç—å FAQ –∑–∞–ø–∏—Å—å"""
    result = await async_faq_manager.update(trigger, request.trigger, request.response)
    if not result:
        raise HTTPException(status_code=404, detail="FAQ entry not found")

    if llm_service and hasattr(llm_service, 'reload_faq'):
        llm_service.reload_faq()

    return {"status": "ok", "trigger": request.trigger}


@app.delete("/admin/faq/{trigger}")
async def admin_delete_faq(trigger: str):
    """–£–¥–∞–ª–∏—Ç—å FAQ –∑–∞–ø–∏—Å—å"""
    if not await async_faq_manager.delete(trigger):
        raise HTTPException(status_code=404, detail=f"Trigger not found: {trigger}")

    if llm_service and hasattr(llm_service, 'reload_faq'):
        llm_service.reload_faq()

    return {"status": "ok", "deleted": trigger}


@app.post("/admin/faq/reload")
async def admin_reload_faq():
    """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å FAQ –∏–∑ —Ñ–∞–π–ª–∞"""
    if llm_service and hasattr(llm_service, 'reload_faq'):
        llm_service.reload_faq()
        return {"status": "ok", "count": len(llm_service.faq)}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.post("/admin/faq/save")
async def admin_save_faq():
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å FAQ (—É–∂–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è)"""
    return {"status": "ok", "message": "FAQ is saved automatically on each change"}


@app.post("/admin/faq/test")
async def admin_test_faq(request: AdminFAQTestRequest):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å FAQ –ø–æ–∏—Å–∫"""
    if llm_service and hasattr(llm_service, '_check_faq'):
        result = llm_service._check_faq(request.text)
        if result:
            return {"match": True, "response": result}
        return {"match": False, "response": None}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== Fine-tuning Endpoints ==============

@app.post("/admin/finetune/dataset/upload")
async def admin_upload_dataset(file: UploadFile = File(...)):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç (Telegram export JSON)"""
    manager = get_finetune_manager()
    content = await file.read()
    return await manager.upload_dataset(content, file.filename)


class DatasetProcessRequest(BaseModel):
    owner_name: Optional[str] = None
    transcribe_voice: Optional[bool] = None
    min_dialog_messages: Optional[int] = None
    max_message_length: Optional[int] = None
    max_dialog_length: Optional[int] = None
    include_groups: Optional[bool] = None
    output_name: Optional[str] = None


@app.post("/admin/finetune/dataset/process")
async def admin_process_dataset(request: Optional[DatasetProcessRequest] = None):
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç"""
    manager = get_finetune_manager()
    config = request.model_dump(exclude_none=True) if request else None
    return await manager.process_dataset(config)


@app.get("/admin/finetune/dataset/config")
async def admin_get_dataset_config():
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    manager = get_finetune_manager()
    return {"config": manager.get_dataset_config()}


@app.post("/admin/finetune/dataset/config")
async def admin_set_dataset_config(request: DatasetProcessRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    manager = get_finetune_manager()
    return manager.set_dataset_config(**request.model_dump(exclude_none=True))


@app.get("/admin/finetune/dataset/processing-status")
async def admin_get_processing_status():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    manager = get_finetune_manager()
    return {"status": manager.get_processing_status()}


@app.get("/admin/finetune/dataset/stats")
async def admin_get_dataset_stats():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    manager = get_finetune_manager()
    stats = manager.get_dataset_stats()
    return {
        "stats": {
            "total_sessions": stats.total_sessions,
            "total_messages": stats.total_messages,
            "total_tokens": stats.total_tokens,
            "avg_tokens_per_message": stats.avg_tokens_per_message,
            "file_path": stats.file_path,
            "file_size_mb": stats.file_size_mb,
            "modified": stats.modified,
        }
    }


@app.get("/admin/finetune/dataset/list")
async def admin_list_datasets():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    manager = get_finetune_manager()
    return {"datasets": manager.list_datasets()}


@app.post("/admin/finetune/dataset/augment")
async def admin_augment_dataset():
    """–ê—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç"""
    manager = get_finetune_manager()
    return await manager.augment_dataset()


@app.get("/admin/finetune/config")
async def admin_get_finetune_config():
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è"""
    manager = get_finetune_manager()
    config = manager.get_config()
    return {
        "config": {
            "base_model": config.base_model,
            "lora_rank": config.lora_rank,
            "lora_alpha": config.lora_alpha,
            "lora_dropout": config.lora_dropout,
            "batch_size": config.batch_size,
            "gradient_accumulation_steps": config.gradient_accumulation_steps,
            "learning_rate": config.learning_rate,
            "num_epochs": config.num_epochs,
            "warmup_ratio": config.warmup_ratio,
            "max_seq_length": config.max_seq_length,
            "output_dir": config.output_dir,
        },
        "presets": {
            name: {
                "lora_rank": p.lora_rank,
                "batch_size": p.batch_size,
                "num_epochs": p.num_epochs,
            }
            for name, p in manager.get_config_presets().items()
        }
    }


@app.post("/admin/finetune/config")
async def admin_set_finetune_config(request: AdminFinetuneConfigRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è"""
    manager = get_finetune_manager()
    config = manager.get_config()

    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    if request.lora_rank is not None:
        config.lora_rank = request.lora_rank
    if request.lora_alpha is not None:
        config.lora_alpha = request.lora_alpha
    if request.batch_size is not None:
        config.batch_size = request.batch_size
    if request.gradient_accumulation_steps is not None:
        config.gradient_accumulation_steps = request.gradient_accumulation_steps
    if request.learning_rate is not None:
        config.learning_rate = request.learning_rate
    if request.num_epochs is not None:
        config.num_epochs = request.num_epochs
    if request.max_seq_length is not None:
        config.max_seq_length = request.max_seq_length
    if request.output_dir is not None:
        config.output_dir = request.output_dir

    return manager.set_config(config)


@app.post("/admin/finetune/train/start")
async def admin_start_training():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ"""
    manager = get_finetune_manager()
    return await manager.start_training()


@app.post("/admin/finetune/train/stop")
async def admin_stop_training():
    """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ"""
    manager = get_finetune_manager()
    return await manager.stop_training()


@app.get("/admin/finetune/train/status")
async def admin_get_training_status():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è"""
    manager = get_finetune_manager()
    status = manager.get_training_status()
    return {
        "status": {
            "is_running": status.is_running,
            "current_step": status.current_step,
            "total_steps": status.total_steps,
            "current_epoch": status.current_epoch,
            "total_epochs": status.total_epochs,
            "loss": status.loss,
            "learning_rate": status.learning_rate,
            "elapsed_seconds": status.elapsed_seconds,
            "eta_seconds": status.eta_seconds,
            "error": status.error,
        }
    }


@app.get("/admin/finetune/train/log")
async def admin_stream_training_log():
    """SSE streaming –ª–æ–≥–∞ –æ–±—É—á–µ–Ω–∏—è"""
    manager = get_finetune_manager()

    async def generate():
        async for data in manager.stream_training_log():
            yield f"data: {data}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
    )


@app.get("/admin/finetune/adapters")
async def admin_list_adapters():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤"""
    manager = get_finetune_manager()
    adapters = manager.list_adapters()
    return {
        "adapters": [
            {
                "name": a.name,
                "path": a.path,
                "size_mb": a.size_mb,
                "modified": a.modified,
                "active": a.active,
                "config": a.config,
            }
            for a in adapters
        ],
        "active": manager.active_adapter
    }


@app.post("/admin/finetune/adapters/activate")
async def admin_activate_adapter(request: AdminAdapterRequest):
    """–ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å LoRA –∞–¥–∞–ø—Ç–µ—Ä"""
    manager = get_finetune_manager()
    return await manager.activate_adapter(request.adapter)


@app.delete("/admin/finetune/adapters/{name}")
async def admin_delete_adapter(name: str):
    """–£–¥–∞–ª–∏—Ç—å LoRA –∞–¥–∞–ø—Ç–µ—Ä"""
    manager = get_finetune_manager()
    return await manager.delete_adapter(name)


# ============== TTS Finetune Endpoints ==============

@app.get("/admin/tts-finetune/config")
async def admin_get_tts_finetune_config():
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é TTS fine-tuning"""
    manager = get_tts_finetune_manager()
    return {"config": manager.get_config()}


@app.post("/admin/tts-finetune/config")
async def admin_set_tts_finetune_config(config: dict):
    """–û–±–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é TTS fine-tuning"""
    manager = get_tts_finetune_manager()
    return {"status": "ok", "config": manager.set_config(config)}


@app.get("/admin/tts-finetune/samples")
async def admin_get_tts_samples():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–∑—Ü–æ–≤ –≥–æ–ª–æ—Å–∞"""
    manager = get_tts_finetune_manager()
    return {"samples": manager.get_samples()}


@app.post("/admin/tts-finetune/samples/upload")
async def admin_upload_tts_sample(file: UploadFile = File(...)):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–±—Ä–∞–∑–µ—Ü –≥–æ–ª–æ—Å–∞"""
    manager = get_tts_finetune_manager()
    content = await file.read()
    sample = manager.add_sample(file.filename, content)
    return {"status": "ok", "sample": {
        "filename": sample.filename,
        "path": sample.path,
        "duration_sec": sample.duration_sec,
        "size_kb": sample.size_kb
    }}


@app.delete("/admin/tts-finetune/samples/{filename}")
async def admin_delete_tts_sample(filename: str):
    """–£–¥–∞–ª–∏—Ç—å –æ–±—Ä–∞–∑–µ—Ü –≥–æ–ª–æ—Å–∞"""
    manager = get_tts_finetune_manager()
    if manager.delete_sample(filename):
        return {"status": "ok", "message": f"Sample {filename} deleted"}
    raise HTTPException(status_code=404, detail="Sample not found")


@app.put("/admin/tts-finetune/samples/{filename}/transcript")
async def admin_update_tts_transcript(filename: str, request: dict):
    """–û–±–Ω–æ–≤–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –æ–±—Ä–∞–∑—Ü–∞"""
    manager = get_tts_finetune_manager()
    transcript = request.get("transcript", "")
    sample = manager.update_transcript(filename, transcript)
    if not sample:
        raise HTTPException(status_code=404, detail="Sample not found")
    return {"status": "ok", "sample": {
        "filename": sample.filename,
        "transcript": sample.transcript,
        "transcript_edited": sample.transcript_edited
    }}


@app.post("/admin/tts-finetune/transcribe")
async def admin_transcribe_tts_samples():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é –æ–±—Ä–∞–∑—Ü–æ–≤ —á–µ—Ä–µ–∑ Whisper"""
    manager = get_tts_finetune_manager()
    if manager.transcribe_samples():
        return {"status": "ok", "message": "Transcription started"}
    return {"status": "error", "message": "Already running or no samples to transcribe"}


@app.post("/admin/tts-finetune/prepare")
async def admin_prepare_tts_dataset():
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç (–∏–∑–≤–ª–µ—á—å audio_codes)"""
    manager = get_tts_finetune_manager()
    if manager.prepare_dataset():
        return {"status": "ok", "message": "Dataset preparation started"}
    return {"status": "error", "message": "Already running or no samples with transcripts"}


@app.get("/admin/tts-finetune/processing-status")
async def admin_get_tts_processing_status():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    manager = get_tts_finetune_manager()
    return {"status": manager.get_processing_status()}


@app.post("/admin/tts-finetune/train/start")
async def admin_start_tts_training():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ TTS"""
    manager = get_tts_finetune_manager()
    if manager.start_training():
        return {"status": "ok", "message": "Training started"}
    return {"status": "error", "message": "Already running or dataset not prepared"}


@app.post("/admin/tts-finetune/train/stop")
async def admin_stop_tts_training():
    """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ TTS"""
    manager = get_tts_finetune_manager()
    if manager.stop_training():
        return {"status": "ok", "message": "Training stopped"}
    return {"status": "error", "message": "Training not running"}


@app.get("/admin/tts-finetune/train/status")
async def admin_get_tts_training_status():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è TTS"""
    manager = get_tts_finetune_manager()
    return {"status": manager.get_training_status()}


@app.get("/admin/tts-finetune/train/log")
async def admin_get_tts_training_log():
    """–ü–æ–ª—É—á–∏—Ç—å –ª–æ–≥ –æ–±—É—á–µ–Ω–∏—è TTS"""
    manager = get_tts_finetune_manager()
    return {"log": manager.get_training_log()}


@app.get("/admin/tts-finetune/models")
async def admin_get_tts_trained_models():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –æ–±—É—á–µ–Ω–Ω—ã—Ö TTS –º–æ–¥–µ–ª–µ–π"""
    manager = get_tts_finetune_manager()
    return {"models": manager.get_trained_models()}


# ============== Monitoring Endpoints ==============

@app.get("/admin/monitor/gpu")
async def admin_get_gpu_stats():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É GPU"""
    import torch

    if not torch.cuda.is_available():
        return {"available": False, "gpus": []}

    gpus = []
    for i in range(torch.cuda.device_count()):
        try:
            name = torch.cuda.get_device_name(i)
            props = torch.cuda.get_device_properties(i)
            total_memory = props.total_memory / (1024**3)
            allocated = torch.cuda.memory_allocated(i) / (1024**3)
            reserved = torch.cuda.memory_reserved(i) / (1024**3)

            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —É—Ç–∏–ª–∏–∑–∞—Ü–∏—é —á–µ—Ä–µ–∑ nvidia-smi
            utilization = None
            temperature = None
            try:
                import subprocess
                result = subprocess.run(
                    ['nvidia-smi', f'--id={i}', '--query-gpu=utilization.gpu,temperature.gpu',
                     '--format=csv,noheader,nounits'],
                    capture_output=True, text=True, timeout=5
                )
                if result.returncode == 0:
                    parts = result.stdout.strip().split(',')
                    if len(parts) >= 2:
                        utilization = int(parts[0].strip())
                        temperature = int(parts[1].strip())
            except Exception:
                pass

            gpus.append({
                "id": i,
                "name": name,
                "total_memory_gb": round(total_memory, 2),
                "allocated_gb": round(allocated, 2),
                "reserved_gb": round(reserved, 2),
                "free_gb": round(total_memory - reserved, 2),
                "utilization_percent": utilization,
                "temperature_c": temperature,
                "compute_capability": f"{props.major}.{props.minor}",
            })
        except Exception as e:
            logger.warning(f"Error getting GPU {i} stats: {e}")

    return {"available": True, "gpus": gpus}


@app.get("/admin/monitor/gpu/stream")
async def admin_stream_gpu_stats():
    """SSE streaming —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ GPU"""
    import torch

    async def generate():
        while True:
            if torch.cuda.is_available():
                gpus = []
                for i in range(torch.cuda.device_count()):
                    try:
                        allocated = torch.cuda.memory_allocated(i) / (1024**3)
                        reserved = torch.cuda.memory_reserved(i) / (1024**3)
                        gpus.append({
                            "id": i,
                            "allocated_gb": round(allocated, 2),
                            "reserved_gb": round(reserved, 2),
                        })
                    except Exception:
                        pass

                yield f"data: {json.dumps({'gpus': gpus, 'timestamp': datetime.now().isoformat()})}\n\n"
            else:
                yield f"data: {json.dumps({'available': False})}\n\n"

            await asyncio.sleep(5)

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
    )


@app.get("/admin/monitor/health")
async def admin_get_health():
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
    manager = get_service_manager()
    health = {
        "timestamp": datetime.now().isoformat(),
        "overall": "healthy",
        "components": {}
    }

    # Orchestrator
    health["components"]["orchestrator"] = {"status": "healthy", "uptime": "running"}

    # LLM
    if llm_service:
        try:
            if hasattr(llm_service, 'is_available') and llm_service.is_available():
                health["components"]["llm"] = {"status": "healthy", "backend": "vllm"}
            else:
                health["components"]["llm"] = {"status": "healthy", "backend": "gemini"}
        except Exception as e:
            health["components"]["llm"] = {"status": "unhealthy", "error": str(e)}
            health["overall"] = "degraded"
    else:
        health["components"]["llm"] = {"status": "unavailable"}
        health["overall"] = "degraded"

    # TTS
    if gulya_voice_service or voice_service:
        health["components"]["tts_xtts"] = {"status": "healthy"}
    else:
        health["components"]["tts_xtts"] = {"status": "unavailable"}

    if piper_service:
        health["components"]["tts_piper"] = {"status": "healthy"}
    else:
        health["components"]["tts_piper"] = {"status": "unavailable"}

    # vLLM external process
    vllm_status = manager.get_service_status("vllm")
    if vllm_status["is_running"]:
        health["components"]["vllm_process"] = {"status": "healthy", "pid": vllm_status["pid"]}
    else:
        health["components"]["vllm_process"] = {"status": "stopped"}

    return health


@app.get("/admin/monitor/metrics")
async def admin_get_metrics():
    """–ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
    import psutil

    metrics = {
        "timestamp": datetime.now().isoformat(),
        "system": {
            "cpu_percent": psutil.cpu_percent(interval=0.1),
            "memory_percent": psutil.virtual_memory().percent,
            "memory_used_gb": round(psutil.virtual_memory().used / (1024**3), 2),
            "memory_total_gb": round(psutil.virtual_memory().total / (1024**3), 2),
        },
        "streaming_tts": streaming_tts_manager.get_stats() if streaming_tts_manager else None,
    }

    # LLM –º–µ—Ç—Ä–∏–∫–∏
    if llm_service:
        metrics["llm"] = {
            "history_length": len(getattr(llm_service, 'conversation_history', [])),
            "faq_count": len(getattr(llm_service, 'faq', {})),
        }

    return metrics


@app.get("/admin/monitor/errors")
async def admin_get_errors():
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –æ—à–∏–±–∫–∏"""
    manager = get_service_manager()
    return {
        "errors": manager.last_errors,
        "timestamp": datetime.now().isoformat()
    }


@app.post("/admin/monitor/metrics/reset")
async def admin_reset_metrics():
    """–°–±—Ä–æ—Å–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏"""
    # –û—á–∏—â–∞–µ–º –∫—ç—à TTS
    if streaming_tts_manager:
        with streaming_tts_manager._cache_lock:
            streaming_tts_manager._cache.clear()

    return {"status": "ok", "message": "Metrics reset"}


@app.get("/admin/monitor/system")
async def admin_get_system_status():
    """–ü–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ: GPU, CPU, RAM, –¥–∏—Å–∫–∏, Docker, —Å–µ—Ç—å"""
    monitor = get_system_monitor()
    return monitor.get_full_status()


# ============== Chat Endpoints ==============

@app.get("/admin/chat/sessions")
async def admin_list_chat_sessions():
    """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —á–∞—Ç-—Å–µ—Å—Å–∏–π"""
    sessions = await async_chat_manager.list_sessions()
    return {"sessions": sessions}


@app.post("/admin/chat/sessions")
async def admin_create_chat_session(request: CreateSessionRequest):
    """–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é —á–∞—Ç-—Å–µ—Å—Å–∏—é"""
    session = await async_chat_manager.create_session(request.title, request.system_prompt)
    return {"session": session}


@app.get("/admin/chat/sessions/{session_id}")
async def admin_get_chat_session(session_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —á–∞—Ç-—Å–µ—Å—Å–∏—é"""
    session = await async_chat_manager.get_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    return {"session": session}


@app.put("/admin/chat/sessions/{session_id}")
async def admin_update_chat_session(session_id: str, request: UpdateSessionRequest):
    """–û–±–Ω–æ–≤–∏—Ç—å —á–∞—Ç-—Å–µ—Å—Å–∏—é"""
    session = await async_chat_manager.update_session(session_id, request.title, request.system_prompt)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    return {"session": session}


@app.delete("/admin/chat/sessions/{session_id}")
async def admin_delete_chat_session(session_id: str):
    """–£–¥–∞–ª–∏—Ç—å —á–∞—Ç-—Å–µ—Å—Å–∏—é"""
    if not await async_chat_manager.delete_session(session_id):
        raise HTTPException(status_code=404, detail="Session not found")
    return {"status": "ok"}


@app.post("/admin/chat/sessions/{session_id}/messages")
async def admin_send_chat_message(session_id: str, request: SendMessageRequest):
    """–û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç (non-streaming)"""
    session = await async_chat_manager.get_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")

    if not llm_service:
        raise HTTPException(status_code=503, detail="LLM service not available")

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_msg = await async_chat_manager.add_message(session_id, 'user', request.content)

    # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è LLM
    default_prompt = None
    if hasattr(llm_service, 'get_system_prompt'):
        default_prompt = llm_service.get_system_prompt()
    messages = await async_chat_manager.get_messages_for_llm(session_id, default_prompt)

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    try:
        response_text = llm_service.generate_response_from_messages(messages, stream=False)
        if hasattr(response_text, '__iter__') and not isinstance(response_text, str):
            response_text = ''.join(response_text)

        assistant_msg = await async_chat_manager.add_message(session_id, 'assistant', response_text)
        return {"message": user_msg, "response": assistant_msg}

    except Exception as e:
        logger.error(f"‚ùå Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/admin/chat/sessions/{session_id}/stream")
async def admin_stream_chat_message(session_id: str, request: SendMessageRequest):
    """–û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –ø–æ–ª—É—á–∏—Ç—å streaming –æ—Ç–≤–µ—Ç"""
    session = await async_chat_manager.get_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")

    if not llm_service:
        raise HTTPException(status_code=503, detail="LLM service not available")

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_msg = await async_chat_manager.add_message(session_id, 'user', request.content)

    # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è LLM
    default_prompt = None
    if hasattr(llm_service, 'get_system_prompt'):
        default_prompt = llm_service.get_system_prompt()
    messages = await async_chat_manager.get_messages_for_llm(session_id, default_prompt)

    async def generate_stream():
        full_response = []
        try:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            yield f"data: {json.dumps({'type': 'user_message', 'message': user_msg}, ensure_ascii=False)}\n\n"

            # Streaming –æ—Ç–≤–µ—Ç
            for chunk in llm_service.generate_response_from_messages(messages, stream=True):
                full_response.append(chunk)
                yield f"data: {json.dumps({'type': 'chunk', 'content': chunk}, ensure_ascii=False)}\n\n"

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç
            response_text = ''.join(full_response)
            assistant_msg = await async_chat_manager.add_message(session_id, 'assistant', response_text)

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            yield f"data: {json.dumps({'type': 'assistant_message', 'message': assistant_msg}, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"

        except Exception as e:
            logger.error(f"‚ùå Chat stream error: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': str(e)})}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
    )


@app.put("/admin/chat/sessions/{session_id}/messages/{message_id}")
async def admin_edit_chat_message(session_id: str, message_id: str, request: EditMessageRequest):
    """–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"""
    session = await async_chat_manager.get_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")

    # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ
    message = None
    message_index = -1
    for i, msg in enumerate(session['messages']):
        if msg['id'] == message_id:
            message = msg
            message_index = i
            break

    if not message:
        raise HTTPException(status_code=404, detail="Message not found")

    if message['role'] != 'user':
        raise HTTPException(status_code=400, detail="Can only edit user messages")

    # –£–¥–∞–ª—è–µ–º —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –≤—Å–µ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ
    await async_chat_manager.delete_message(session_id, message_id)

    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    user_msg = await async_chat_manager.add_message(session_id, 'user', request.content)

    # –ï—Å–ª–∏ –±—ã–ª –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π
    if not llm_service:
        return {"message": user_msg}

    default_prompt = None
    if hasattr(llm_service, 'get_system_prompt'):
        default_prompt = llm_service.get_system_prompt()
    messages = await async_chat_manager.get_messages_for_llm(session_id, default_prompt)

    try:
        response_text = llm_service.generate_response_from_messages(messages, stream=False)
        if hasattr(response_text, '__iter__') and not isinstance(response_text, str):
            response_text = ''.join(response_text)

        assistant_msg = await async_chat_manager.add_message(session_id, 'assistant', response_text)
        return {"message": user_msg, "response": assistant_msg}

    except Exception as e:
        logger.error(f"‚ùå Chat regenerate error: {e}")
        return {"message": user_msg, "error": str(e)}


@app.delete("/admin/chat/sessions/{session_id}/messages/{message_id}")
async def admin_delete_chat_message(session_id: str, message_id: str):
    """–£–¥–∞–ª–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –≤—Å–µ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ"""
    if not await async_chat_manager.delete_message(session_id, message_id):
        raise HTTPException(status_code=404, detail="Message not found")
    return {"status": "ok"}


@app.post("/admin/chat/sessions/{session_id}/messages/{message_id}/regenerate")
async def admin_regenerate_chat_response(session_id: str, message_id: str):
    """–ü–µ—Ä–µ–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ"""
    session = await async_chat_manager.get_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")

    if not llm_service:
        raise HTTPException(status_code=503, detail="LLM service not available")

    # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    message_index = -1
    for i, msg in enumerate(session['messages']):
        if msg['id'] == message_id:
            message_index = i
            break

    if message_index == -1:
        raise HTTPException(status_code=404, detail="Message not found")

    # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ - —É–¥–∞–ª—è–µ–º –µ–≥–æ (–æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞)
    if message_index + 1 < len(session['messages']):
        next_msg = session['messages'][message_index + 1]
        await async_chat_manager.delete_message(session_id, next_msg['id'])

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –æ—Ç–≤–µ—Ç
    default_prompt = None
    if hasattr(llm_service, 'get_system_prompt'):
        default_prompt = llm_service.get_system_prompt()
    llm_messages = await async_chat_manager.get_messages_for_llm(session_id, default_prompt)

    try:
        response_text = llm_service.generate_response_from_messages(llm_messages, stream=False)
        if hasattr(response_text, '__iter__') and not isinstance(response_text, str):
            response_text = ''.join(response_text)

        assistant_msg = await async_chat_manager.add_message(session_id, 'assistant', response_text)
        return {"response": assistant_msg}

    except Exception as e:
        logger.error(f"‚ùå Chat regenerate error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============== Model Management API ==============

@app.get("/admin/models/list")
async def admin_list_models():
    """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    manager = get_model_manager()
    return {"models": manager.get_cached_models()}


@app.post("/admin/models/scan")
async def admin_scan_models(request: Request):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"""
    data = await request.json() if request.headers.get('content-type') == 'application/json' else {}
    include_system = data.get('include_system', False)

    manager = get_model_manager()
    if manager.scan_all_models(include_system=include_system):
        return {"status": "ok", "message": "Scan started"}
    else:
        return {"status": "error", "message": "Scan already in progress"}


@app.post("/admin/models/scan/cancel")
async def admin_cancel_scan():
    """–û—Ç–º–µ–Ω–∏—Ç—å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ"""
    manager = get_model_manager()
    manager.cancel_scan()
    return {"status": "ok", "message": "Scan cancelled"}


@app.get("/admin/models/scan/status")
async def admin_scan_status():
    """–°—Ç–∞—Ç—É—Å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è"""
    manager = get_model_manager()
    return {"status": manager.get_scan_progress()}


@app.post("/admin/models/download")
async def admin_download_model(request: Request):
    """–°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å —Å HuggingFace"""
    data = await request.json()
    repo_id = data.get('repo_id')
    revision = data.get('revision', 'main')

    if not repo_id:
        raise HTTPException(status_code=400, detail="repo_id required")

    manager = get_model_manager()
    if manager.download_model(repo_id, revision):
        return {"status": "ok", "message": f"Download started: {repo_id}"}
    else:
        return {"status": "error", "message": "Download already in progress"}


@app.post("/admin/models/download/cancel")
async def admin_cancel_download():
    """–û—Ç–º–µ–Ω–∏—Ç—å –∑–∞–≥—Ä—É–∑–∫—É"""
    manager = get_model_manager()
    manager.cancel_download()
    return {"status": "ok", "message": "Download cancelled"}


@app.get("/admin/models/download/status")
async def admin_download_status():
    """–°—Ç–∞—Ç—É—Å –∑–∞–≥—Ä—É–∑–∫–∏"""
    manager = get_model_manager()
    return {"status": manager.get_download_progress()}


@app.delete("/admin/models/delete")
async def admin_delete_model(path: str):
    """–£–¥–∞–ª–∏—Ç—å –º–æ–¥–µ–ª—å"""
    manager = get_model_manager()
    result = manager.delete_model(path)
    if result['status'] == 'ok':
        return result
    else:
        raise HTTPException(status_code=400, detail=result.get('error', 'Delete failed'))


@app.get("/admin/models/search")
async def admin_search_huggingface(query: str, limit: int = 20):
    """–ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π –Ω–∞ HuggingFace"""
    manager = get_model_manager()
    results = manager.search_huggingface(query, limit)
    return {"results": results}


@app.get("/admin/models/details/{repo_id:path}")
async def admin_get_model_details(repo_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª–∏ –º–æ–¥–µ–ª–∏ —Å HuggingFace"""
    manager = get_model_manager()
    details = manager.get_model_details(repo_id)
    if details:
        return {"details": details}
    else:
        raise HTTPException(status_code=404, detail="Model not found")


# ============== Widget Config Endpoints ==============

WIDGET_CONFIG_FILE = Path("widget_config.json")

def get_widget_config() -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤–∏–¥–∂–µ—Ç–∞"""
    default_config = {
        "enabled": False,
        "title": "AI –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç",
        "greeting": "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ö–æ–º–ø–∞–Ω–∏—è –®–∞–µ—Ä–≤—ç–π –î–∏-–ò–¥–∂–∏—Ç–∞–ª, —á–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?",
        "placeholder": "–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ...",
        "primary_color": "#6366f1",
        "position": "right",
        "allowed_domains": ["shaerware.digital", "shaerware.github.io", "localhost"],
        "tunnel_url": ""
    }
    if WIDGET_CONFIG_FILE.exists():
        try:
            config = json.loads(WIDGET_CONFIG_FILE.read_text(encoding='utf-8'))
            return {**default_config, **config}
        except Exception:
            pass
    return default_config


def save_widget_config(config: dict):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤–∏–¥–∂–µ—Ç–∞"""
    WIDGET_CONFIG_FILE.write_text(json.dumps(config, indent=2, ensure_ascii=False), encoding='utf-8')


@app.get("/admin/widget/config")
async def admin_get_widget_config():
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤–∏–¥–∂–µ—Ç–∞"""
    config = await async_config_manager.get_widget()
    return {"config": config}


@app.post("/admin/widget/config")
async def admin_save_widget_config(request: AdminWidgetConfigRequest):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤–∏–¥–∂–µ—Ç–∞"""
    config = {
        "enabled": request.enabled,
        "title": request.title,
        "greeting": request.greeting,
        "placeholder": request.placeholder,
        "primary_color": request.primary_color,
        "position": request.position,
        "allowed_domains": request.allowed_domains,
        "tunnel_url": request.tunnel_url
    }
    await async_config_manager.set_widget(config)
    return {"status": "ok", "config": config}


@app.get("/widget.js")
async def get_widget_script(request: Request):
    """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–π —Å–∫—Ä–∏–ø—Ç –≤–∏–¥–∂–µ—Ç–∞"""
    config = await async_config_manager.get_widget()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∫–ª—é—á–µ–Ω –ª–∏ –≤–∏–¥–∂–µ—Ç
    if not config.get("enabled", False):
        return Response(
            content="// Widget is disabled",
            media_type="application/javascript"
        )

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–µ)
    origin = request.headers.get("origin", "") or request.headers.get("referer", "")
    allowed_domains = config.get("allowed_domains", [])
    if allowed_domains and origin:
        origin_domain = origin.replace("https://", "").replace("http://", "").split("/")[0].split(":")[0]
        if not any(d in origin_domain for d in allowed_domains):
            return Response(
                content=f"// Widget not allowed for domain: {origin_domain}",
                media_type="application/javascript"
            )

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º API URL
    api_url = config.get("tunnel_url", "").strip()
    if not api_url:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–π —Ö–æ—Å—Ç –µ—Å–ª–∏ tunnel_url –Ω–µ —É–∫–∞–∑–∞–Ω
        api_url = str(request.base_url).rstrip("/")

    # –ß–∏—Ç–∞–µ–º –±–∞–∑–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –≤–∏–¥–∂–µ—Ç–∞
    widget_path = Path(__file__).parent / "web-widget" / "ai-chat-widget.js"
    if not widget_path.exists():
        return Response(
            content="// Widget script not found",
            media_type="application/javascript",
            status_code=404
        )

    widget_js = widget_path.read_text(encoding='utf-8')

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∫—Ä–∏–ø—Ç —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    settings_js = f"""
// Auto-generated widget settings
window.aiChatSettings = {{
  apiUrl: '{api_url}',
  title: '{config.get("title", "AI –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç")}',
  greeting: '{config.get("greeting", "").replace("'", "\\'")}',
  placeholder: '{config.get("placeholder", "").replace("'", "\\'")}',
  primaryColor: '{config.get("primary_color", "#6366f1")}',
  position: '{config.get("position", "right")}'
}};

"""
    full_script = settings_js + widget_js

    return Response(
        content=full_script,
        media_type="application/javascript",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Access-Control-Allow-Origin": "*"
        }
    )


# ============== Telegram Bot Endpoints ==============

TELEGRAM_CONFIG_FILE = Path("telegram_config.json")

# Telegram bot process management
_telegram_bot_process = None
_telegram_bot_task = None

def get_telegram_config() -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Telegram –±–æ—Ç–∞"""
    default_config = {
        "enabled": False,
        "bot_token": "",
        "api_url": "http://localhost:8002",
        "allowed_users": [],
        "admin_users": [],
        "welcome_message": "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ–º–ø–∞–Ω–∏–∏ –®–∞–µ—Ä–≤—ç–π. –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?",
        "unauthorized_message": "–ò–∑–≤–∏–Ω–∏—Ç–µ, —É –≤–∞—Å –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ —ç—Ç–æ–º—É –±–æ—Ç—É.",
        "error_message": "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
        "typing_enabled": True
    }
    if TELEGRAM_CONFIG_FILE.exists():
        try:
            config = json.loads(TELEGRAM_CONFIG_FILE.read_text(encoding='utf-8'))
            return {**default_config, **config}
        except Exception:
            pass
    return default_config


def save_telegram_config(config: dict):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Telegram –±–æ—Ç–∞"""
    TELEGRAM_CONFIG_FILE.write_text(json.dumps(config, indent=2, ensure_ascii=False), encoding='utf-8')


@app.get("/admin/telegram/config")
async def admin_get_telegram_config():
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Telegram –±–æ—Ç–∞"""
    config = await async_config_manager.get_telegram()
    # –ú–∞—Å–∫–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    if config.get("bot_token"):
        token = config["bot_token"]
        if len(token) > 10:
            config["bot_token_masked"] = token[:4] + "..." + token[-4:]
        else:
            config["bot_token_masked"] = "***"
    else:
        config["bot_token_masked"] = ""
    return {"config": config}


@app.post("/admin/telegram/config")
async def admin_save_telegram_config(request: AdminTelegramConfigRequest):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Telegram –±–æ—Ç–∞"""
    # Load existing config to preserve token if not provided
    existing = await async_config_manager.get_telegram()

    config = {
        "enabled": request.enabled,
        "bot_token": request.bot_token if request.bot_token else existing.get("bot_token", ""),
        "api_url": request.api_url,
        "allowed_users": request.allowed_users,
        "admin_users": request.admin_users,
        "welcome_message": request.welcome_message,
        "unauthorized_message": request.unauthorized_message,
        "error_message": request.error_message,
        "typing_enabled": request.typing_enabled
    }
    await async_config_manager.set_telegram(config)
    return {"status": "ok", "config": config}


@app.get("/admin/telegram/status")
async def admin_get_telegram_status():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å Telegram –±–æ—Ç–∞"""
    global _telegram_bot_process

    config = await async_config_manager.get_telegram()
    running = False

    if _telegram_bot_process is not None:
        if _telegram_bot_process.poll() is None:
            running = True
        else:
            _telegram_bot_process = None

    # Count sessions from database
    sessions = await async_telegram_manager.get_all_sessions()
    sessions_count = len(sessions)

    return {
        "status": {
            "running": running,
            "enabled": config.get("enabled", False),
            "has_token": bool(config.get("bot_token")),
            "active_sessions": sessions_count,
            "allowed_users_count": len(config.get("allowed_users", [])),
            "admin_users_count": len(config.get("admin_users", [])),
            "pid": _telegram_bot_process.pid if _telegram_bot_process else None
        }
    }


@app.post("/admin/telegram/start")
async def admin_start_telegram_bot():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å Telegram –±–æ—Ç–∞"""
    global _telegram_bot_process

    config = await async_config_manager.get_telegram()

    if not config.get("bot_token"):
        raise HTTPException(status_code=400, detail="Bot token not configured")

    if not config.get("enabled"):
        raise HTTPException(status_code=400, detail="Bot is disabled in config")

    # Check if already running
    if _telegram_bot_process is not None and _telegram_bot_process.poll() is None:
        return {"status": "already_running", "pid": _telegram_bot_process.pid}

    # Start bot process
    import subprocess
    bot_script = Path(__file__).parent / "telegram_bot_service.py"
    bot_log = Path(__file__).parent / "logs" / "telegram_bot.log"

    # Ensure logs directory exists
    bot_log.parent.mkdir(exist_ok=True)

    if not bot_script.exists():
        raise HTTPException(status_code=500, detail="Bot script not found")

    try:
        # Open log file for bot output
        log_file = open(bot_log, "a", encoding="utf-8")
        _telegram_bot_process = subprocess.Popen(
            ["python3", "-u", str(bot_script)],  # -u for unbuffered output
            cwd=str(Path(__file__).parent),
            stdout=log_file,
            stderr=subprocess.STDOUT,  # Redirect stderr to stdout (both to log file)
            start_new_session=True  # Detach from parent process group
        )
        logger.info(f"Started Telegram bot with PID {_telegram_bot_process.pid}, logs: {bot_log}")
        return {"status": "started", "pid": _telegram_bot_process.pid}
    except Exception as e:
        logger.error(f"Failed to start Telegram bot: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/admin/telegram/stop")
async def admin_stop_telegram_bot():
    """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Telegram –±–æ—Ç–∞"""
    global _telegram_bot_process

    if _telegram_bot_process is None:
        return {"status": "not_running"}

    if _telegram_bot_process.poll() is not None:
        _telegram_bot_process = None
        return {"status": "not_running"}

    try:
        _telegram_bot_process.terminate()
        _telegram_bot_process.wait(timeout=5)
        logger.info("Telegram bot stopped")
    except Exception as e:
        logger.error(f"Error stopping bot: {e}")
        _telegram_bot_process.kill()

    _telegram_bot_process = None
    return {"status": "stopped"}


@app.post("/admin/telegram/restart")
async def admin_restart_telegram_bot():
    """–ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å Telegram –±–æ—Ç–∞"""
    await admin_stop_telegram_bot()
    await asyncio.sleep(1)
    return await admin_start_telegram_bot()


@app.delete("/admin/telegram/sessions")
async def admin_clear_telegram_sessions():
    """–û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ —Å–µ—Å—Å–∏–∏ Telegram"""
    count = await async_telegram_manager.clear_all()
    return {"status": "ok", "message": f"Cleared {count} sessions"}


@app.get("/admin/telegram/sessions")
async def admin_get_telegram_sessions():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–µ—Å—Å–∏–π Telegram"""
    sessions = await async_telegram_manager.get_sessions_dict()
    return {"sessions": sessions}


# ============== Audit Log API ==============

@app.get("/admin/audit/logs")
async def admin_get_audit_logs(
    action: Optional[str] = None,
    resource: Optional[str] = None,
    user_id: Optional[str] = None,
    from_date: Optional[str] = None,
    to_date: Optional[str] = None,
    limit: int = 100,
    offset: int = 0,
):
    """Get audit logs with optional filters."""
    from datetime import datetime
    from db.database import AsyncSessionLocal
    from db.repositories import AuditRepository

    # Parse dates if provided
    from_dt = None
    to_dt = None
    if from_date:
        try:
            from_dt = datetime.fromisoformat(from_date.replace('Z', '+00:00'))
        except ValueError:
            pass
    if to_date:
        try:
            to_dt = datetime.fromisoformat(to_date.replace('Z', '+00:00'))
        except ValueError:
            pass

    async with AsyncSessionLocal() as session:
        repo = AuditRepository(session)
        logs = await repo.get_logs(
            action=action,
            resource=resource,
            user_id=user_id,
            from_date=from_dt,
            to_date=to_dt,
            limit=limit,
            offset=offset,
        )
        return {"logs": logs, "count": len(logs)}


@app.get("/admin/audit/stats")
async def admin_get_audit_stats():
    """Get audit log statistics."""
    from db.database import AsyncSessionLocal
    from db.repositories import AuditRepository

    async with AsyncSessionLocal() as session:
        repo = AuditRepository(session)
        stats = await repo.get_stats()
        return {"stats": stats}


@app.get("/admin/audit/export")
async def admin_export_audit_logs(
    from_date: Optional[str] = None,
    to_date: Optional[str] = None,
    format: str = "json",  # json or csv
):
    """Export audit logs as JSON or CSV."""
    from datetime import datetime
    from db.database import AsyncSessionLocal
    from db.repositories import AuditRepository
    import csv
    import io

    # Parse dates
    from_dt = None
    to_dt = None
    if from_date:
        try:
            from_dt = datetime.fromisoformat(from_date.replace('Z', '+00:00'))
        except ValueError:
            pass
    if to_date:
        try:
            to_dt = datetime.fromisoformat(to_date.replace('Z', '+00:00'))
        except ValueError:
            pass

    async with AsyncSessionLocal() as session:
        repo = AuditRepository(session)
        logs = await repo.export_logs(from_date=from_dt, to_date=to_dt)

    if format == "csv":
        # Convert to CSV
        output = io.StringIO()
        if logs:
            writer = csv.DictWriter(output, fieldnames=logs[0].keys())
            writer.writeheader()
            writer.writerows(logs)
        csv_content = output.getvalue()
        return Response(
            content=csv_content,
            media_type="text/csv",
            headers={"Content-Disposition": "attachment; filename=audit_logs.csv"}
        )
    else:
        # Return as JSON
        return {"logs": logs, "count": len(logs)}


@app.post("/admin/audit/cleanup")
async def admin_cleanup_audit_logs(days: int = 90):
    """Delete audit logs older than specified days."""
    from db.database import AsyncSessionLocal
    from db.repositories import AuditRepository

    if days < 7:
        raise HTTPException(status_code=400, detail="Minimum retention period is 7 days")

    async with AsyncSessionLocal() as session:
        repo = AuditRepository(session)
        deleted = await repo.cleanup_old_logs(days=days)
        return {"status": "ok", "deleted": deleted, "retention_days": days}


# ============== Static Files for Vue Admin ==============

DEV_MODE = os.getenv("DEV_MODE", "").lower() in ("1", "true", "yes")
VITE_DEV_URL = os.getenv("VITE_DEV_URL", "http://localhost:5173")

if DEV_MODE:
    # Dev mode: proxy to Vite dev server for hot reload
    import httpx

    @app.api_route("/admin/{path:path}", methods=["GET", "HEAD"])
    async def proxy_to_vite(path: str, request: Request):
        """Proxy static files to Vite dev server"""
        async with httpx.AsyncClient() as client:
            url = f"{VITE_DEV_URL}/admin/{path}"
            try:
                resp = await client.get(url, headers=dict(request.headers))
                return Response(
                    content=resp.content,
                    status_code=resp.status_code,
                    headers=dict(resp.headers)
                )
            except httpx.ConnectError:
                return Response(
                    content=b"Vite dev server not running. Start with: cd admin && npm run dev",
                    status_code=503
                )

    @app.get("/admin")
    async def proxy_admin_root():
        """Redirect /admin to /admin/"""
        return RedirectResponse(url="/admin/")

    logger.info(f"üîß DEV MODE: Proxying /admin/* to Vite at {VITE_DEV_URL}")
else:
    # Production: serve built Vue app
    admin_dist_path = Path(__file__).parent / "admin" / "dist"
    if admin_dist_path.exists():
        from fastapi.staticfiles import StaticFiles
        app.mount("/admin", StaticFiles(directory=str(admin_dist_path), html=True), name="admin")
        logger.info(f"üìÇ Vue admin mounted at /admin from {admin_dist_path}")


if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv()
    port = int(os.getenv("ORCHESTRATOR_PORT", 8002))
    logger.info(f"üéØ –ó–∞–ø—É—Å–∫ Orchestrator –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    uvicorn.run(
        "orchestrator:app",
        host="0.0.0.0",
        port=port,
        reload=False,
        log_level="info"
    )
