#!/usr/bin/env python3
"""
Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€ - ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð¸Ñ€ÑƒÐµÑ‚ Ð²ÑÐµ ÑÐµÑ€Ð²Ð¸ÑÑ‹
STT (Whisper) -> LLM (Gemini) -> TTS (XTTS v2)
"""

import asyncio
import hashlib
import json
import logging
import os
import re
import threading
import time
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

import numpy as np
import soundfile as sf
import uvicorn
from fastapi import Depends, FastAPI, File, HTTPException, Request, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, RedirectResponse, Response, StreamingResponse
from pydantic import BaseModel

# Modular routers
from app.routers import (
    audit,
    auth,
    bot_sales,
    chat,
    faq,
    github_webhook,
    gsm,
    llm,
    monitor,
    services,
    stt,
    telegram,
    tts,
    widget,
    yoomoney_webhook,
)
from auth_manager import (
    LoginRequest,
    LoginResponse,
    User,
    authenticate_user,
    create_access_token,
    get_auth_status,
    get_current_user,
)

# Cloud LLM service for multi-provider support
from cloud_llm_service import PROVIDER_TYPES, CloudLLMService

# Database integration
from db.integration import (
    async_audit_logger,
    async_cloud_provider_manager,
    async_config_manager,
    async_faq_manager,
    async_preset_manager,
    async_widget_instance_manager,
    get_database_status,
    init_database,
    shutdown_database,
)
from finetune_manager import get_finetune_manager
from llm_service import LLMService
from model_manager import get_model_manager

# Multi-bot manager
from piper_tts_service import PiperTTSService
from service_manager import get_service_manager
from stt_service import STTService
from system_monitor import get_system_monitor
from tts_finetune_manager import get_tts_finetune_manager

# Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹ Ð½Ð°ÑˆÐ¸Ñ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²
from voice_clone_service import VoiceCloneService


# vLLM Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ - Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð°Ñ Llama Ñ‡ÐµÑ€ÐµÐ· vLLM)
try:
    from vllm_llm_service import VLLMLLMService

    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False
    VLLMLLMService = None

# OpenVoice Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ - Ð´Ð»Ñ GPU P104-100)
try:
    from openvoice_service import OpenVoiceService

    OPENVOICE_AVAILABLE = True
except ImportError:
    OPENVOICE_AVAILABLE = False
    OpenVoiceService = None

# ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ ÐºÐ°ÐºÐ¾Ð¹ LLM backend Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ
LLM_BACKEND = os.getenv("LLM_BACKEND", "gemini").lower()  # "gemini" Ð¸Ð»Ð¸ "vllm"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============== Streaming TTS Manager ==============
class StreamingTTSManager:
    """
    ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€ Ð´Ð»Ñ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð° TTS Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ streaming LLM.

    ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð°:
    1. Ð’Ð¾ Ð²Ñ€ÐµÐ¼Ñ streaming chat/completions - Ð½Ð°ÐºÐ°Ð¿Ð»Ð¸Ð²Ð°ÐµÐ¼ Ñ‚ÐµÐºÑÑ‚ Ð¸ Ð¿Ñ€Ð¸ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ð¸
       Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÑÐ¸Ð½Ñ‚ÐµÐ· Ð² Ñ„Ð¾Ð½Ð¾Ð²Ð¾Ð¼ Ð¿Ð¾Ñ‚Ð¾ÐºÐµ
    2. Ð¥Ñ€Ð°Ð½Ð¸Ð¼ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ñ‹ Ð² ÐºÑÑˆÐµ Ð¿Ð¾ Ñ…ÑÑˆÑƒ Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ Ñ‚ÐµÐºÑÑ‚Ð°
    3. ÐŸÑ€Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐµ /v1/audio/speech - ÑÐºÐ»ÐµÐ¸Ð²Ð°ÐµÐ¼ Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹Ðµ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ñ‹
    """

    def __init__(self, max_cache_size: int = 50, cache_ttl: int = 300):
        self.max_cache_size = max_cache_size
        self.cache_ttl = cache_ttl  # ÑÐµÐºÑƒÐ½Ð´

        # ÐšÑÑˆ: response_hash -> {"segments": [...], "full_audio": np.array, "timestamp": float}
        self._cache: OrderedDict[str, Dict] = OrderedDict()
        self._cache_lock = threading.Lock()

        # Ð¢ÐµÐºÑƒÑ‰Ð¸Ðµ ÑÐµÑÑÐ¸Ð¸ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð°: session_id -> {"text": str, "segments": [...], "futures": [...]}
        self._active_sessions: Dict[str, Dict] = {}
        self._session_lock = threading.Lock()

        # Thread pool Ð´Ð»Ñ Ñ„Ð¾Ð½Ð¾Ð²Ð¾Ð³Ð¾ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð°
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="tts_")

        # Ð ÐµÐ³ÑƒÐ»ÑÑ€ÐºÐ° Ð´Ð»Ñ Ñ€Ð°Ð·Ð±Ð¸ÐµÐ½Ð¸Ñ Ð½Ð° Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ
        self._sentence_pattern = re.compile(r"([^.!?]*[.!?]+)")

        logger.info("ðŸŽ™ï¸ StreamingTTSManager Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½")

    def _get_text_hash(self, text: str) -> str:
        """Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÑ‚ Ñ…ÑÑˆ Ñ‚ÐµÐºÑÑ‚Ð° Ð´Ð»Ñ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ"""
        normalized = text.strip().lower()
        return hashlib.md5(normalized.encode()).hexdigest()[:16]

    def _clean_old_cache(self):
        """Ð£Ð´Ð°Ð»ÑÐµÑ‚ ÑƒÑÑ‚Ð°Ñ€ÐµÐ²ÑˆÐ¸Ðµ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð¸Ð· ÐºÑÑˆÐ°"""
        now = time.time()
        with self._cache_lock:
            keys_to_delete = []
            for key, value in self._cache.items():
                if now - value.get("timestamp", 0) > self.cache_ttl:
                    keys_to_delete.append(key)
            for key in keys_to_delete:
                del self._cache[key]
                logger.debug(f"ðŸ—‘ï¸ Ð£Ð´Ð°Ð»Ñ‘Ð½ ÑƒÑÑ‚Ð°Ñ€ÐµÐ²ÑˆÐ¸Ð¹ ÐºÑÑˆ: {key}")

            # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€ ÐºÑÑˆÐ°
            while len(self._cache) > self.max_cache_size:
                self._cache.popitem(last=False)

    def start_session(self, session_id: str) -> None:
        """ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÑ‚ Ð½Ð¾Ð²ÑƒÑŽ ÑÐµÑÑÐ¸ÑŽ streaming ÑÐ¸Ð½Ñ‚ÐµÐ·Ð°"""
        with self._session_lock:
            self._active_sessions[session_id] = {
                "text_buffer": "",
                "full_text": "",
                "segments": [],  # [(text, audio_data, sample_rate), ...]
                "pending_futures": [],
                "start_time": time.time(),
            }
        logger.info(f"ðŸŽ¬ ÐÐ°Ñ‡Ð°Ñ‚Ð° ÑÐµÑÑÐ¸Ñ TTS: {session_id}")

    def add_text_chunk(self, session_id: str, chunk: str, voice_service) -> None:
        """
        Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÑ‚ chunk Ñ‚ÐµÐºÑÑ‚Ð° Ð¸ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ ÑÐ¸Ð½Ñ‚ÐµÐ· Ð¿Ñ€Ð¸ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ.
        Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ÑÑ Ð¸Ð· streaming LLM response.
        """
        with self._session_lock:
            if session_id not in self._active_sessions:
                return

            session = self._active_sessions[session_id]
            session["text_buffer"] += chunk
            session["full_text"] += chunk

            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÐµÑÑ‚ÑŒ Ð»Ð¸ Ð·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ
            buffer = session["text_buffer"]
            sentences = self._sentence_pattern.findall(buffer)

            if sentences:
                # Ð¡Ð¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ°Ð¶Ð´Ð¾Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½Ð½Ð¾Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ
                for sentence in sentences:
                    sentence = sentence.strip()
                    if len(sentence) > 3:  # Ð˜Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ðµ
                        future = self._executor.submit(
                            self._synthesize_segment, sentence, voice_service, session_id
                        )
                        session["pending_futures"].append((sentence, future))
                        logger.info(f"ðŸ”„ Ð—Ð°Ð¿ÑƒÑ‰ÐµÐ½ ÑÐ¸Ð½Ñ‚ÐµÐ·: '{sentence[:40]}...'")

                # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Ð¸Ð· Ð±ÑƒÑ„ÐµÑ€Ð°
                last_sentence = sentences[-1]
                idx = buffer.rfind(last_sentence) + len(last_sentence)
                session["text_buffer"] = buffer[idx:]

    def _synthesize_segment(self, text: str, voice_service, session_id: str) -> tuple:
        """Ð¡Ð¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€ÑƒÐµÑ‚ Ð¾Ð´Ð¸Ð½ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚ (Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ÑÑ Ð² thread pool)"""
        try:
            wav, sr = voice_service.synthesize(
                text=text,
                preset="natural",
                preprocess_text=True,
                split_sentences=False,  # Ð£Ð¶Ðµ Ñ€Ð°Ð·Ð±Ð¸Ð»Ð¸
            )
            logger.info(f"âœ… Ð¡Ð¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€Ð¾Ð²Ð°Ð½ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚: '{text[:30]}...'")
            return (text, wav, sr)
        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¸Ð½Ñ‚ÐµÐ·Ð° ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°: {e}")
            return (text, None, None)

    def finish_session(self, session_id: str, voice_service) -> None:
        """
        Ð—Ð°Ð²ÐµÑ€ÑˆÐ°ÐµÑ‚ ÑÐµÑÑÐ¸ÑŽ: ÑÐ¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€ÑƒÐµÑ‚ Ð¾ÑÑ‚Ð°Ð²ÑˆÐ¸Ð¹ÑÑ Ñ‚ÐµÐºÑÑ‚ Ð¸ ÐºÑÑˆÐ¸Ñ€ÑƒÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚.
        """
        with self._session_lock:
            if session_id not in self._active_sessions:
                return

            session = self._active_sessions[session_id]

            # Ð¡Ð¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€ÑƒÐµÐ¼ Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ðº Ð±ÑƒÑ„ÐµÑ€Ð° ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ
            remaining = session["text_buffer"].strip()
            if remaining and len(remaining) > 3:
                future = self._executor.submit(
                    self._synthesize_segment, remaining, voice_service, session_id
                )
                session["pending_futures"].append((remaining, future))
                logger.info(f"ðŸ”„ Ð—Ð°Ð¿ÑƒÑ‰ÐµÐ½ ÑÐ¸Ð½Ñ‚ÐµÐ· Ð¾ÑÑ‚Ð°Ñ‚ÐºÐ°: '{remaining[:40]}...'")

            # Ð–Ð´Ñ‘Ð¼ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ Ð²ÑÐµÑ… futures
            for text, future in session["pending_futures"]:
                try:
                    result = future.result(timeout=60)
                    if result[1] is not None:
                        session["segments"].append(result)
                except Exception as e:
                    logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð° ÑÐ¸Ð½Ñ‚ÐµÐ·Ð°: {e}")

            # Ð¡ÐºÐ»ÐµÐ¸Ð²Ð°ÐµÐ¼ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ñ‹
            full_text = session["full_text"]
            if session["segments"]:
                self._cache_full_audio(full_text, session["segments"])

            elapsed = time.time() - session["start_time"]
            logger.info(
                f"âœ… Ð¡ÐµÑÑÐ¸Ñ {session_id} Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð° Ð·Ð° {elapsed:.2f}s, "
                f"ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð²: {len(session['segments'])}"
            )

            # Ð£Ð´Ð°Ð»ÑÐµÐ¼ ÑÐµÑÑÐ¸ÑŽ
            del self._active_sessions[session_id]

    def _cache_full_audio(self, full_text: str, segments: list) -> None:
        """Ð¡ÐºÐ»ÐµÐ¸Ð²Ð°ÐµÑ‚ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¸ ÐºÑÑˆÐ¸Ñ€ÑƒÐµÑ‚ Ð¿Ð¾Ð»Ð½Ð¾Ðµ Ð°ÑƒÐ´Ð¸Ð¾"""
        if not segments:
            return

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ sample rate Ð¸Ð· Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°
        sample_rate = segments[0][2]

        # Ð¡ÐºÐ»ÐµÐ¸Ð²Ð°ÐµÐ¼ Ð°ÑƒÐ´Ð¸Ð¾ Ñ Ð½ÐµÐ±Ð¾Ð»ÑŒÑˆÐ¸Ð¼Ð¸ Ð¿Ð°ÑƒÐ·Ð°Ð¼Ð¸
        pause_samples = int(0.1 * sample_rate)  # 100ms Ð¿Ð°ÑƒÐ·Ð°
        pause = np.zeros(pause_samples, dtype=np.float32)

        audio_parts = []
        for text, wav, sr in segments:
            if wav is not None:
                if isinstance(wav, list):
                    wav = np.array(wav, dtype=np.float32)
                audio_parts.append(wav)
                audio_parts.append(pause)

        if audio_parts:
            full_audio = np.concatenate(audio_parts[:-1])  # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÑŽÑŽ Ð¿Ð°ÑƒÐ·Ñƒ

            text_hash = self._get_text_hash(full_text)
            with self._cache_lock:
                self._cache[text_hash] = {
                    "full_audio": full_audio,
                    "sample_rate": sample_rate,
                    "full_text": full_text,
                    "timestamp": time.time(),
                    "segments_count": len(segments),
                }
                logger.info(
                    f"ðŸ’¾ Ð—Ð°ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ Ð°ÑƒÐ´Ð¸Ð¾: {text_hash} ({len(full_audio) / sample_rate:.2f}s)"
                )

            self._clean_old_cache()

    def get_cached_audio(self, text: str) -> Optional[tuple]:
        """
        ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ Ð·Ð°ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð°ÑƒÐ´Ð¸Ð¾ Ð´Ð»Ñ Ñ‚ÐµÐºÑÑ‚Ð°.
        Returns: (audio_data, sample_rate) Ð¸Ð»Ð¸ None
        """
        text_hash = self._get_text_hash(text)

        with self._cache_lock:
            if text_hash in self._cache:
                cached = self._cache[text_hash]
                logger.info(f"âš¡ Cache HIT: {text_hash}")
                return (cached["full_audio"], cached["sample_rate"])

        logger.info(f"âŒ Cache MISS: {text_hash}")
        return None

    def get_stats(self) -> dict:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð°"""
        with self._cache_lock:
            cache_size = len(self._cache)
        with self._session_lock:
            active_sessions = len(self._active_sessions)

        return {
            "cache_size": cache_size,
            "active_sessions": active_sessions,
            "max_cache_size": self.max_cache_size,
        }


# Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€ streaming TTS
streaming_tts_manager: Optional[StreamingTTSManager] = None

app = FastAPI(title="AI Secretary Orchestrator", version="1.0.0")

# CORS Ð´Ð»Ñ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ð¸Ð· Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð°
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include modular routers
# NOTE: These routers use the ServiceContainer from app.dependencies
# which is populated in startup_event
app.include_router(auth.router)
app.include_router(audit.router)
app.include_router(services.router)
app.include_router(monitor.router)
app.include_router(faq.router)
app.include_router(stt.router)
app.include_router(llm.router)
app.include_router(tts.router)
app.include_router(chat.router)
app.include_router(telegram.router)
app.include_router(widget.router)
app.include_router(gsm.router)
app.include_router(bot_sales.router)
app.include_router(github_webhook.router)
app.include_router(yoomoney_webhook.router)

# Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ðµ ÑÐµÑ€Ð²Ð¸ÑÑ‹
voice_service: Optional[VoiceCloneService] = None  # XTTS (Ð›Ð¸Ð´Ð¸Ñ) - GPU CC >= 7.0
gulya_voice_service: Optional[VoiceCloneService] = None  # XTTS (Ð“ÑƒÐ»Ñ) - GPU CC >= 7.0
piper_service: Optional[PiperTTSService] = None  # Piper (Dmitri, Irina) - CPU
openvoice_service: Optional["OpenVoiceService"] = None  # OpenVoice v2 (Ð›Ð¸Ð´Ð¸Ñ) - GPU CC 6.1+
stt_service: Optional[STTService] = None
llm_service: Optional[LLMService] = None

# ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ð³Ð¾Ð»Ð¾ÑÐ°
# engine: "xtts" (Ð›Ð¸Ð´Ð¸Ñ/Ð“ÑƒÐ»Ñ Ð½Ð° GPU CC>=7.0), "piper" (Dmitri/Irina Ð½Ð° CPU), "openvoice" (Ð›Ð¸Ð´Ð¸Ñ Ð½Ð° GPU CC 6.1+)
# ÐŸÐ¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð“ÑƒÐ»ÑŽ (XTTS) ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð°, Ð¸Ð½Ð°Ñ‡Ðµ Piper
current_voice_config = {
    "engine": "xtts",
    "voice": "gulya",  # gulya / lidia / dmitri / irina / lidia_openvoice
}

# ÐŸÐ°Ð¿ÐºÐ° Ð´Ð»Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð²
TEMP_DIR = Path("./temp")
TEMP_DIR.mkdir(exist_ok=True)

# ÐŸÐ°Ð¿ÐºÐ° Ð´Ð»Ñ Ð»Ð¾Ð³Ð¾Ð² Ð·Ð²Ð¾Ð½ÐºÐ¾Ð²
CALLS_LOG_DIR = Path("./calls_log")
CALLS_LOG_DIR.mkdir(exist_ok=True)


# Helper functions for loading data from database at startup
async def _reload_llm_faq():
    """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ FAQ Ð¸Ð· Ð‘Ð” Ð¸ Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÑ‚ LLM ÑÐµÑ€Ð²Ð¸Ñ."""
    if llm_service and hasattr(llm_service, "reload_faq"):
        faq_dict = await async_faq_manager.get_all()
        llm_service.reload_faq(faq_dict)


async def _reload_voice_presets():
    """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð¿Ñ€ÐµÑÐµÑ‚Ñ‹ Ð¸Ð· Ð‘Ð” Ð¸ Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÑ‚ voice ÑÐµÑ€Ð²Ð¸ÑÑ‹."""
    presets_dict = await async_preset_manager.get_custom()
    for svc in [voice_service, gulya_voice_service]:
        if svc and hasattr(svc, "reload_presets"):
            svc.reload_presets(presets_dict)


async def _auto_start_telegram_bots():
    """Auto-start Telegram bots that have auto_start=True."""
    from db.integration import async_bot_instance_manager
    from multi_bot_manager import MultiBotManager

    try:
        instances = await async_bot_instance_manager.get_auto_start_instances()
        if not instances:
            logger.info("ðŸ“± No Telegram bots configured for auto-start")
            return

        manager = MultiBotManager()
        started = 0
        for instance in instances:
            instance_id = instance["id"]
            try:
                result = await manager.start_bot(instance_id)
                if result.get("status") in ["started", "already_running"]:
                    started += 1
                    logger.info(f"ðŸ“± Auto-started Telegram bot: {instance['name']}")
                else:
                    logger.warning(f"ðŸ“± Failed to auto-start bot {instance_id}: {result}")
            except Exception as e:
                logger.error(f"ðŸ“± Error auto-starting bot {instance_id}: {e}")

        if started > 0:
            logger.info(f"ðŸ“± Auto-started {started}/{len(instances)} Telegram bots")
    except Exception as e:
        logger.error(f"ðŸ“± Error during Telegram bot auto-start: {e}")


class ConversationRequest(BaseModel):
    text: str
    session_id: Optional[str] = None


class TTSRequest(BaseModel):
    text: str
    language: str = "ru"


class OpenAISpeechRequest(BaseModel):
    """OpenAI-compatible TTS request for OpenWebUI integration"""

    model: str = "lidia-voice"
    input: str
    voice: str = "lidia"
    response_format: str = "wav"
    speed: float = 1.0


class ChatMessage(BaseModel):
    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    """OpenAI-compatible chat completion request"""

    model: str = "gulya-secretary-qwen"  # Format: {persona}-secretary-{backend}
    messages: List[ChatMessage]
    stream: bool = False
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None


@app.on_event("startup")
async def startup_event():
    """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²ÑÐµÑ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð² Ð¿Ñ€Ð¸ ÑÑ‚Ð°Ñ€Ñ‚Ðµ"""
    global \
        voice_service, \
        gulya_voice_service, \
        piper_service, \
        openvoice_service, \
        stt_service, \
        llm_service, \
        streaming_tts_manager

    logger.info("ðŸš€ Ð—Ð°Ð¿ÑƒÑÐº AI Secretary Orchestrator")

    # Initialize database first
    await init_database()

    try:
        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Piper TTS (Dmitri, Irina) - CPU, Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¼
        logger.info("ðŸ“¦ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Piper TTS Service (CPU)...")
        try:
            piper_service = PiperTTSService()
        except Exception as e:
            logger.warning(f"âš ï¸ Piper TTS Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½: {e}")
            piper_service = None

        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ OpenVoice v2 (Ð›Ð¸Ð´Ð¸Ñ) - GPU CC 6.1+ (P104-100)
        if OPENVOICE_AVAILABLE:
            logger.info("ðŸ“¦ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° OpenVoice TTS Service (GPU CC 6.1+)...")
            try:
                openvoice_service = OpenVoiceService()
                logger.info("âœ… OpenVoice v2 Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ (P104-100)")
            except Exception as e:
                logger.warning(f"âš ï¸ OpenVoice Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½: {e}")
                openvoice_service = None
        else:
            logger.info("â­ï¸ OpenVoice Ð½Ðµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½ (Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼)")
            openvoice_service = None

        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ XTTS (Ð“ÑƒÐ»Ñ) - GPU CC >= 7.0, Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ
        logger.info("ðŸ“¦ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Voice Clone Service (XTTS - Ð“ÑƒÐ»Ñ)...")
        try:
            gulya_voice_service = VoiceCloneService(voice_samples_dir="./Ð“ÑƒÐ»Ñ")
            logger.info(
                f"âœ… XTTS (Ð“ÑƒÐ»Ñ) Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½: {len(gulya_voice_service.voice_samples)} Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð²"
            )
        except Exception as e:
            logger.warning(f"âš ï¸ XTTS (Ð“ÑƒÐ»Ñ) Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½: {e}")
            gulya_voice_service = None

        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ XTTS (Ð›Ð¸Ð´Ð¸Ñ) - GPU CC >= 7.0, Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾
        logger.info("ðŸ“¦ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Voice Clone Service (XTTS - Ð›Ð¸Ð´Ð¸Ñ)...")
        try:
            voice_service = VoiceCloneService(voice_samples_dir="./Ð›Ð¸Ð´Ð¸Ñ")
            logger.info(f"âœ… XTTS (Ð›Ð¸Ð´Ð¸Ñ) Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½: {len(voice_service.voice_samples)} Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð²")
        except Exception as e:
            logger.warning(f"âš ï¸ XTTS (Ð›Ð¸Ð´Ð¸Ñ) Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ (Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ GPU CC >= 7.0): {e}")
            voice_service = None

        # Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð³Ð¾Ð»Ð¾Ñ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ
        global current_voice_config
        if gulya_voice_service:
            current_voice_config = {"engine": "xtts", "voice": "gulya"}
            logger.info("ðŸŽ¤ Ð“Ð¾Ð»Ð¾Ñ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ: Ð“ÑƒÐ»Ñ (XTTS)")
        elif voice_service:
            current_voice_config = {"engine": "xtts", "voice": "lidia"}
            logger.info("ðŸŽ¤ Ð“Ð¾Ð»Ð¾Ñ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ: Ð›Ð¸Ð´Ð¸Ñ (XTTS)")
        elif piper_service:
            current_voice_config = {"engine": "piper", "voice": "dmitri"}
            logger.info("ðŸŽ¤ Ð“Ð¾Ð»Ð¾Ñ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ: Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸Ð¹ (Piper)")

        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ LLM Service (vLLM Ð¸Ð»Ð¸ Gemini)
        if LLM_BACKEND == "vllm" and VLLM_AVAILABLE:
            logger.info("ðŸ“¦ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° vLLM LLM Service (Llama-3.1-8B)...")
            try:
                llm_service = VLLMLLMService()
                if llm_service.is_available():
                    logger.info("âœ… vLLM Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½")
                else:
                    logger.warning("âš ï¸ vLLM Ð½Ðµ Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÐµÑ‚, Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Gemini...")
                    llm_service = LLMService()
            except Exception as e:
                logger.warning(f"âš ï¸ vLLM Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ ({e}), Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Gemini")
                llm_service = LLMService()
        else:
            logger.info("ðŸ“¦ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Gemini LLM Service...")
            llm_service = LLMService()

        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Streaming TTS Manager
        logger.info("ðŸ“¦ Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Streaming TTS Manager...")
        streaming_tts_manager = StreamingTTSManager(max_cache_size=50, cache_ttl=300)

        # STT Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ñ‘Ð½ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ - Ð´Ð»Ñ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ð¾Ð³Ð¾ Ñ‡Ð°Ñ‚Ð° Ð½Ðµ Ð½ÑƒÐ¶ÐµÐ½
        # ÐœÐ¾Ð´ÐµÐ»ÑŒ faster-whisper Ð·Ð°Ð²Ð¸ÑÐ°ÐµÑ‚ Ð¿Ñ€Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐµ
        logger.info("â­ï¸ STT Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ñ‘Ð½ (Ð´Ð»Ñ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ð¾Ð³Ð¾ Ñ‡Ð°Ñ‚Ð° Ð½Ðµ Ð½ÑƒÐ¶ÐµÐ½)")
        stt_service = None

        # Load FAQ and presets from database into services
        logger.info("ðŸ“¦ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° FAQ Ð¸ Ð¿Ñ€ÐµÑÐµÑ‚Ð¾Ð² Ð¸Ð· Ð‘Ð”...")
        try:
            await _reload_llm_faq()
            await _reload_voice_presets()
            logger.info("âœ… FAQ Ð¸ Ð¿Ñ€ÐµÑÐµÑ‚Ñ‹ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð¸Ð· Ð‘Ð”")
        except Exception as e:
            logger.warning(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· Ð‘Ð”: {e}")

        # Check for deprecated legacy JSON files
        legacy_files = [
            ("typical_responses.json", "FAQ"),
            ("custom_presets.json", "TTS presets"),
            ("chat_sessions.json", "chat sessions"),
            ("widget_config.json", "widget config"),
            ("telegram_config.json", "telegram config"),
        ]
        found_legacy = []
        for filename, description in legacy_files:
            if Path(filename).exists():
                found_legacy.append(f"{filename} ({description})")

        if found_legacy:
            logger.warning("=" * 60)
            logger.warning("âš ï¸  DEPRECATED: ÐÐ°Ð¹Ð´ÐµÐ½Ñ‹ legacy JSON Ñ„Ð°Ð¹Ð»Ñ‹:")
            for f in found_legacy:
                logger.warning(f"    â€¢ {f}")
            logger.warning("    Ð”Ð°Ð½Ð½Ñ‹Ðµ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ñ…Ñ€Ð°Ð½ÑÑ‚ÑÑ Ð² SQLite (data/secretary.db).")
            logger.warning("    Legacy Ñ„Ð°Ð¹Ð»Ñ‹ Ð¼Ð¾Ð¶Ð½Ð¾ ÑƒÐ´Ð°Ð»Ð¸Ñ‚ÑŒ Ð¿Ð¾ÑÐ»Ðµ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸:")
            logger.warning("    python scripts/migrate_json_to_db.py")
            logger.warning("=" * 60)

        # Populate service container for modular routers
        from app.dependencies import get_container

        container = get_container()
        container.voice_service = voice_service
        container.gulya_voice_service = gulya_voice_service
        container.piper_service = piper_service
        container.openvoice_service = openvoice_service
        container.stt_service = stt_service
        container.llm_service = llm_service
        container.streaming_tts_manager = streaming_tts_manager
        container.current_voice_config = current_voice_config
        logger.info("âœ… Service container populated for modular routers")

        # Auto-start Telegram bots that were running before restart
        await _auto_start_telegram_bots()

        logger.info("âœ… ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ ÑÐµÑ€Ð²Ð¸ÑÑ‹ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾")

    except Exception as e:
        logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {e}")
        raise


@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    logger.info("ðŸ›‘ Shutting down AI Secretary Orchestrator")
    await shutdown_database()
    logger.info("âœ… Shutdown complete")


@app.get("/")
async def root():
    """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ€Ð°Ð±Ð¾Ñ‚Ð¾ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸"""
    return {
        "status": "ok",
        "service": "AI Secretary Orchestrator",
        "endpoints": {
            "health": "/health",
            "process_call": "/process_call (POST)",
            "tts": "/tts (POST)",
            "stt": "/stt (POST)",
            "chat": "/chat (POST)",
        },
    }


@app.get("/health")
async def health_check():
    """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÑ Ð²ÑÐµÑ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²"""
    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ llm_service Ð¸Ð· container (Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¾Ð±Ð½Ð¾Ð²Ð»Ñ‘Ð½ Ñ‡ÐµÑ€ÐµÐ· router)
    from app.dependencies import get_container

    container = get_container()
    current_llm_service = container.llm_service if container.llm_service else llm_service

    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ñ‚Ð¸Ð¿ LLM ÑÐµÑ€Ð²Ð¸ÑÐ°
    llm_backend_type = "unknown"
    if current_llm_service:
        if hasattr(current_llm_service, "api_url"):  # vLLM
            llm_backend_type = f"vllm ({current_llm_service.model_name})"
        elif hasattr(current_llm_service, "model_name"):  # Gemini
            llm_backend_type = f"gemini ({current_llm_service.model_name})"

    services_status = {
        "voice_clone_xtts_gulya": gulya_voice_service is not None,
        "voice_clone_xtts_lidia": voice_service is not None,
        "voice_clone_openvoice": openvoice_service is not None,
        "piper_tts": piper_service is not None,
        "stt": stt_service is not None,
        "llm": current_llm_service is not None,
        "llm_backend": llm_backend_type,
        "streaming_tts": streaming_tts_manager is not None,
        "current_voice": current_voice_config,
    }

    # Ð”Ð»Ñ health check Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð»ÑŽÐ±Ð¾Ð¹ TTS + llm
    any_tts = (
        services_status["voice_clone_xtts_gulya"]
        or services_status["voice_clone_xtts_lidia"]
        or services_status["voice_clone_openvoice"]
        or services_status["piper_tts"]
    )
    core_ok = any_tts and services_status["llm"]

    # Get database status
    db_status = await get_database_status()

    result = {
        "status": "healthy" if core_ok else "degraded",
        "services": services_status,
        "database": db_status.get("database", {}),
        "timestamp": datetime.now().isoformat(),
    }

    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ streaming TTS ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½
    if streaming_tts_manager is not None:
        result["streaming_tts_stats"] = streaming_tts_manager.get_stats()

    return result


def synthesize_with_current_voice(text: str, output_path: str, language: str = "ru"):
    """
    Ð¡Ð¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€ÑƒÐµÑ‚ Ñ€ÐµÑ‡ÑŒ Ñ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¼ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ð¼ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð¼.
    Ð£Ñ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ current_voice_config.

    Engines:
    - piper: CPU, Ð±Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹, Ð¿Ñ€ÐµÐ´Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ðµ Ð³Ð¾Ð»Ð¾ÑÐ° (dmitri, irina)
    - openvoice: GPU CC 6.1+, ÐºÐ»Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð³Ð¾Ð»Ð¾ÑÐ° (lidia_openvoice)
    - xtts: GPU CC >= 7.0, Ð»ÑƒÑ‡ÑˆÐµÐµ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ»Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ (gulya, lidia)
    """
    engine = current_voice_config["engine"]
    voice = current_voice_config["voice"]

    if engine == "piper" and piper_service:
        logger.info(f"ðŸŽ™ï¸ Piper ÑÐ¸Ð½Ñ‚ÐµÐ· ({voice}): '{text[:40]}...'")
        piper_service.synthesize_to_file(text, output_path, voice=voice)
    elif engine == "openvoice" and openvoice_service:
        logger.info(f"ðŸŽ™ï¸ OpenVoice ÑÐ¸Ð½Ñ‚ÐµÐ· (Ð›Ð¸Ð´Ð¸Ñ): '{text[:40]}...'")
        openvoice_service.synthesize_to_file(text, output_path, language=language)
    elif engine == "xtts" and voice == "gulya" and gulya_voice_service:
        logger.info(f"ðŸŽ™ï¸ XTTS ÑÐ¸Ð½Ñ‚ÐµÐ· (Ð“ÑƒÐ»Ñ): '{text[:40]}...'")
        gulya_voice_service.synthesize_to_file(text, output_path, language=language)
    elif engine == "xtts" and voice == "lidia" and voice_service:
        logger.info(f"ðŸŽ™ï¸ XTTS ÑÐ¸Ð½Ñ‚ÐµÐ· (Ð›Ð¸Ð´Ð¸Ñ): '{text[:40]}...'")
        voice_service.synthesize_to_file(text, output_path, language=language)
    elif gulya_voice_service:
        # Fallback to Ð“ÑƒÐ»Ñ if available (default)
        logger.info(f"ðŸŽ™ï¸ XTTS ÑÐ¸Ð½Ñ‚ÐµÐ· (Ð“ÑƒÐ»Ñ fallback): '{text[:40]}...'")
        gulya_voice_service.synthesize_to_file(text, output_path, language=language)
    elif voice_service:
        # Fallback to Ð›Ð¸Ð´Ð¸Ñ if available
        logger.info(f"ðŸŽ™ï¸ XTTS ÑÐ¸Ð½Ñ‚ÐµÐ· (Ð›Ð¸Ð´Ð¸Ñ fallback): '{text[:40]}...'")
        voice_service.synthesize_to_file(text, output_path, language=language)
    elif openvoice_service:
        # Fallback to OpenVoice if XTTS not available
        logger.info(f"ðŸŽ™ï¸ OpenVoice ÑÐ¸Ð½Ñ‚ÐµÐ· (fallback): '{text[:40]}...'")
        openvoice_service.synthesize_to_file(text, output_path, language=language)
    elif piper_service:
        # Fallback to Piper
        logger.info(f"ðŸŽ™ï¸ Piper ÑÐ¸Ð½Ñ‚ÐµÐ· (fallback): '{text[:40]}...'")
        piper_service.synthesize_to_file(text, output_path, voice="irina")
    else:
        raise RuntimeError("No TTS service available")


@app.post("/tts")
async def text_to_speech(request: TTSRequest):
    """
    Ð¡Ð¸Ð½Ñ‚ÐµÐ· Ñ€ÐµÑ‡Ð¸ Ñ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¼ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ð¼ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð¼
    """
    if not voice_service and not piper_service:
        raise HTTPException(status_code=503, detail="No TTS service initialized")

    try:
        # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ð¾Ðµ Ð¸Ð¼Ñ Ñ„Ð°Ð¹Ð»Ð°
        output_file = TEMP_DIR / f"tts_{datetime.now().timestamp()}.wav"

        # Ð¡Ð¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€ÑƒÐµÐ¼ Ñ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¼ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð¼
        synthesize_with_current_voice(
            text=request.text, output_path=str(output_file), language=request.language
        )

        # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ñ„Ð°Ð¹Ð»
        return FileResponse(path=output_file, media_type="audio/wav", filename="response.wav")

    except Exception as e:
        logger.error(f"âŒ TTS Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/stt")
async def speech_to_text(audio: UploadFile = File(...)):
    """
    Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÑ‡Ð¸ Ð¸Ð· Ð°ÑƒÐ´Ð¸Ð¾ Ñ„Ð°Ð¹Ð»Ð°
    """
    if not stt_service:
        raise HTTPException(status_code=503, detail="STT service not initialized")

    try:
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð»
        temp_audio = TEMP_DIR / f"stt_{datetime.now().timestamp()}_{audio.filename}"

        with open(temp_audio, "wb") as f:
            content = await audio.read()
            f.write(content)

        # Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°ÐµÐ¼
        result = stt_service.transcribe(temp_audio, language="ru")

        # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð»
        temp_audio.unlink()

        return {
            "text": result["text"],
            "language": result["language"],
            "segments_count": len(result.get("segments", [])),
        }

    except Exception as e:
        logger.error(f"âŒ STT Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat")
async def chat(request: ConversationRequest):
    """
    ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ LLM (Gemini)
    """
    if not llm_service:
        raise HTTPException(status_code=503, detail="LLM service not initialized")

    try:
        response = llm_service.generate_response(request.text)

        return {"response": response, "session_id": request.session_id}

    except Exception as e:
        logger.error(f"âŒ LLM Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/process_call")
async def process_call(audio: UploadFile = File(...)):
    """
    ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ñ†Ð¸ÐºÐ» Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð·Ð²Ð¾Ð½ÐºÐ°:
    1. STT - Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÑ‡Ð¸
    2. LLM - Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð°
    3. TTS - ÑÐ¸Ð½Ñ‚ÐµÐ· Ñ€ÐµÑ‡Ð¸

    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð°ÑƒÐ´Ð¸Ð¾ Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð¼ ÑÐµÐºÑ€ÐµÑ‚Ð°Ñ€Ñ
    """
    call_id = f"call_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    logger.info(f"ðŸ“ž ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð·Ð²Ð¾Ð½ÐºÐ° {call_id}")

    try:
        # 1. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð²Ñ…Ð¾Ð´ÑÑ‰Ð¸Ð¹ Ð°ÑƒÐ´Ð¸Ð¾
        input_audio = CALLS_LOG_DIR / f"{call_id}_input.wav"
        with open(input_audio, "wb") as f:
            content = await audio.read()
            f.write(content)

        # 2. Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°ÐµÐ¼ Ñ€ÐµÑ‡ÑŒ (STT)
        logger.info(f"ðŸŽ§ STT Ð´Ð»Ñ {call_id}")
        stt_result = stt_service.transcribe(input_audio, language="ru")
        recognized_text = stt_result["text"]
        logger.info(f"ðŸ“ Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð½Ð¾: {recognized_text}")

        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸ÑŽ
        with open(CALLS_LOG_DIR / f"{call_id}_transcript.txt", "w") as f:
            f.write(f"USER: {recognized_text}\n")

        # 3. Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚ (LLM)
        logger.info(f"ðŸ¤– LLM Ð´Ð»Ñ {call_id}")
        llm_response = llm_service.generate_response(recognized_text)
        logger.info(f"ðŸ’¬ ÐžÑ‚Ð²ÐµÑ‚: {llm_response}")

        # Ð”Ð¾Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸ÑŽ
        with open(CALLS_LOG_DIR / f"{call_id}_transcript.txt", "a") as f:
            f.write(f"ASSISTANT: {llm_response}\n")

        # 4. Ð¡Ð¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€ÑƒÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚ (TTS)
        logger.info(f"ðŸŽ™ï¸  TTS Ð´Ð»Ñ {call_id}")
        output_audio = CALLS_LOG_DIR / f"{call_id}_output.wav"
        voice_service.synthesize_to_file(
            text=llm_response, output_path=str(output_audio), language="ru"
        )

        logger.info(f"âœ… Ð—Ð²Ð¾Ð½Ð¾Ðº {call_id} Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½")

        # 5. Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð°ÑƒÐ´Ð¸Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚
        return FileResponse(
            path=output_audio,
            media_type="audio/wav",
            filename=f"{call_id}_response.wav",
            headers={
                "X-Call-ID": call_id,
                "X-Recognized-Text": recognized_text,
                "X-Response-Text": llm_response,
            },
        )

    except Exception as e:
        logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð·Ð²Ð¾Ð½ÐºÐ° {call_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/reset_conversation")
async def reset_conversation():
    """Ð¡Ð±Ñ€Ð¾Ñ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð°"""
    if llm_service:
        llm_service.reset_conversation()
        return {"status": "ok", "message": "Conversation history reset"}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== OpenAI-Compatible Endpoints for OpenWebUI ==============


@app.get("/v1/models")
@app.get("/v1/models/")
async def list_models():
    """OpenAI-compatible models list for OpenWebUI"""
    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ backend Ð¸ ÑÑƒÑ„Ñ„Ð¸ÐºÑ Ð´Ð»Ñ Ð¸Ð¼ÐµÐ½Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸
    if llm_service and hasattr(llm_service, "api_url"):
        # vLLM backend - Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
        model_name = getattr(llm_service, "model_name", "unknown")
        if model_name == "lydia" or "qwen" in model_name.lower():
            backend_suffix = "qwen"
            backend_desc = "Qwen2.5-7B + LoRA"
        elif "llama" in model_name.lower():
            backend_suffix = "llama"
            backend_desc = "Llama-3.1-8B"
        else:
            backend_suffix = "vllm"
            backend_desc = model_name
    else:
        backend_suffix = "gemini"
        backend_desc = "Gemini"

    return {
        "object": "list",
        "data": [
            {
                "id": f"gulya-secretary-{backend_suffix}",
                "object": "model",
                "created": 1700000000,
                "owned_by": "ai-secretary",
                "permission": [],
                "root": f"gulya-secretary-{backend_suffix}",
                "parent": None,
                "description": f"Ð“ÑƒÐ»Ñ - Ñ†Ð¸Ñ„Ñ€Ð¾Ð²Ð¾Ð¹ ÑÐµÐºÑ€ÐµÑ‚Ð°Ñ€ÑŒ ({backend_desc})",
            },
            {
                "id": f"lidia-secretary-{backend_suffix}",
                "object": "model",
                "created": 1700000000,
                "owned_by": "ai-secretary",
                "permission": [],
                "root": f"lidia-secretary-{backend_suffix}",
                "parent": None,
                "description": f"Ð›Ð¸Ð´Ð¸Ñ - Ñ†Ð¸Ñ„Ñ€Ð¾Ð²Ð¾Ð¹ ÑÐµÐºÑ€ÐµÑ‚Ð°Ñ€ÑŒ ({backend_desc})",
            },
        ],
    }


@app.get("/v1/voices")
async def list_voices():
    """List available voices"""
    voices = []
    if gulya_voice_service:
        voices.append({"voice_id": "gulya", "name": "Ð“ÑƒÐ»Ñ", "language": "ru"})
    if voice_service:
        voices.append({"voice_id": "lidia", "name": "Ð›Ð¸Ð´Ð¸Ñ", "language": "ru"})
    if piper_service:
        voices.append({"voice_id": "dmitri", "name": "Ð”Ð¼Ð¸Ñ‚Ñ€Ð¸Ð¹", "language": "ru"})
        voices.append({"voice_id": "irina", "name": "Ð˜Ñ€Ð¸Ð½Ð°", "language": "ru"})
    return {"voices": voices}


@app.post("/v1/audio/speech")
async def openai_speech(request: OpenAISpeechRequest):
    """
    OpenAI-compatible TTS endpoint for OpenWebUI integration
    POST /v1/audio/speech

    ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ: ÑÐ½Ð°Ñ‡Ð°Ð»Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ ÐºÑÑˆ streaming TTS manager.
    Ð•ÑÐ»Ð¸ Ð°ÑƒÐ´Ð¸Ð¾ ÑƒÐ¶Ðµ Ð±Ñ‹Ð»Ð¾ Ð¿Ñ€ÐµÐ´ÑÐ¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ streaming LLM - Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¼Ð³Ð½Ð¾Ð²ÐµÐ½Ð½Ð¾.
    """
    if not voice_service and not piper_service:
        raise HTTPException(status_code=503, detail="No TTS service initialized")

    try:
        output_file = TEMP_DIR / f"speech_{datetime.now().timestamp()}.wav"
        start_time = time.time()

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÑÑˆ streaming TTS (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ XTTS)
        cached_audio = None
        if current_voice_config["engine"] == "xtts" and streaming_tts_manager is not None:
            cached_audio = streaming_tts_manager.get_cached_audio(request.input)

        if cached_audio is not None:
            # Cache HIT - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ñ€ÐµÐ´ÑÐ¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð°ÑƒÐ´Ð¸Ð¾
            audio_data, sample_rate = cached_audio
            sf.write(str(output_file), audio_data, sample_rate)
            elapsed = time.time() - start_time
            logger.info(f"âš¡ TTS Ð¸Ð· ÐºÑÑˆÐ° Ð·Ð° {elapsed:.3f}s (vs ~5-10s Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ ÑÐ¸Ð½Ñ‚ÐµÐ·)")
        else:
            # Cache MISS - ÑÐ¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€ÑƒÐµÐ¼ Ñ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¼ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð¼
            synthesize_with_current_voice(
                text=request.input, output_path=str(output_file), language="ru"
            )
            elapsed = time.time() - start_time
            logger.info(f"ðŸŽ™ï¸ TTS ÑÐ¸Ð½Ñ‚ÐµÐ·Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð·Ð° {elapsed:.2f}s")

        return FileResponse(path=output_file, media_type="audio/wav", filename="speech.wav")

    except Exception as e:
        logger.error(f"âŒ OpenAI TTS Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """
    OpenAI-compatible chat completions endpoint for OpenWebUI
    Supports both streaming and non-streaming responses.
    ÐŸÑ€Ð¸ streaming - Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ñ„Ð¾Ð½Ð¾Ð²Ñ‹Ð¹ ÑÐ¸Ð½Ñ‚ÐµÐ· TTS Ð¿Ð¾ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸ÑÐ¼.
    """
    if not llm_service:
        raise HTTPException(status_code=503, detail="LLM service not initialized")

    logger.info(
        f"ðŸ’¬ Chat completions request: stream={request.stream}, messages={len(request.messages)}"
    )

    # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Pydantic Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð² dict
    messages = [{"role": m.role, "content": m.content} for m in request.messages]

    if request.stream:
        # Streaming response (SSE) Ñ Ñ„Ð¾Ð½Ð¾Ð²Ñ‹Ð¼ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð¾Ð¼ TTS
        async def generate_stream():
            created = int(time.time())
            chunk_id = f"chatcmpl-{created}"
            session_id = f"tts-{created}"

            # ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ ÑÐµÑÑÐ¸ÑŽ streaming TTS ÐµÑÐ»Ð¸ ÑÐµÑ€Ð²Ð¸ÑÑ‹ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹
            use_streaming_tts = streaming_tts_manager is not None and voice_service is not None

            if use_streaming_tts:
                streaming_tts_manager.start_session(session_id)
                logger.info(f"ðŸŽ¬ Streaming TTS Ð°ÐºÑ‚Ð¸Ð²Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð´Ð»Ñ ÑÐµÑÑÐ¸Ð¸ {session_id}")

            try:
                for text_chunk in llm_service.generate_response_from_messages(
                    messages, stream=True
                ):
                    # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ chunk ÐºÐ»Ð¸ÐµÐ½Ñ‚Ñƒ
                    chunk_data = {
                        "id": chunk_id,
                        "object": "chat.completion.chunk",
                        "created": created,
                        "model": request.model,
                        "choices": [
                            {"index": 0, "delta": {"content": text_chunk}, "finish_reason": None}
                        ],
                    }
                    yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"

                    # ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ chunk Ð² streaming TTS manager
                    if use_streaming_tts and text_chunk:
                        streaming_tts_manager.add_text_chunk(session_id, text_chunk, voice_service)

                # Final chunk
                final_chunk = {
                    "id": chunk_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": request.model,
                    "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                yield "data: [DONE]\n\n"

                # Ð—Ð°Ð²ÐµÑ€ÑˆÐ°ÐµÐ¼ ÑÐµÑÑÐ¸ÑŽ TTS (ÑÐºÐ»ÐµÐ¸Ð²Ð°ÐµÑ‚ Ð¸ ÐºÑÑˆÐ¸Ñ€ÑƒÐµÑ‚ Ð°ÑƒÐ´Ð¸Ð¾)
                if use_streaming_tts:
                    # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð² Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾Ð¼ Ð¿Ð¾Ñ‚Ð¾ÐºÐµ Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ response
                    threading.Thread(
                        target=streaming_tts_manager.finish_session,
                        args=(session_id, voice_service),
                        daemon=True,
                    ).start()

            except Exception as e:
                logger.error(f"âŒ Streaming error: {e}")
                error_chunk = {"error": {"message": str(e), "type": "server_error"}}
                yield f"data: {json.dumps(error_chunk)}\n\n"

        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            },
        )
    else:
        # Non-streaming response
        try:
            response_text = llm_service.generate_response_from_messages(messages, stream=False)

            # Consume generator if it returns one
            if hasattr(response_text, "__iter__") and not isinstance(response_text, str):
                response_text = "".join(response_text)

            return {
                "id": f"chatcmpl-{int(time.time())}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": request.model,
                "choices": [
                    {
                        "index": 0,
                        "message": {"role": "assistant", "content": response_text},
                        "finish_reason": "stop",
                    }
                ],
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
            }
        except Exception as e:
            logger.error(f"âŒ Chat completions error: {e}")
            raise HTTPException(status_code=500, detail=str(e))


# ============== Admin Web Interface ==============
# Vue 3 admin panel served from /admin (see Static Files section below)


# ============== Admin API Endpoints ==============

# ============== Auth Endpoints ==============


@app.post("/admin/auth/login", response_model=LoginResponse)
async def admin_login(request: LoginRequest):
    """
    Authenticate user and return JWT token.

    Default credentials: admin / admin
    Set ADMIN_USERNAME and ADMIN_PASSWORD_HASH env vars for production.
    """
    user = authenticate_user(request.username, request.password)
    if not user:
        raise HTTPException(status_code=401, detail="Invalid username or password")

    token, expires_in = create_access_token(user.username, user.role)

    # Audit log
    await async_audit_logger.log(
        action="login", resource="auth", user_id=user.username, details={"role": user.role}
    )

    return LoginResponse(access_token=token, token_type="bearer", expires_in=expires_in)


@app.get("/admin/auth/me")
async def admin_get_current_user(user: User = Depends(get_current_user)):
    """Get current authenticated user info"""
    return {"username": user.username, "role": user.role}


@app.get("/admin/auth/status")
async def admin_auth_status():
    """Get authentication configuration status"""
    return get_auth_status()


# ============== Admin Models ==============


class AdminTTSPresetRequest(BaseModel):
    """Ð—Ð°Ð¿Ñ€Ð¾Ñ Ð½Ð° Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð¿Ñ€ÐµÑÐµÑ‚Ð° TTS"""

    preset: str  # warm, calm, energetic, natural, neutral


class AdminLLMPromptRequest(BaseModel):
    """Ð—Ð°Ð¿Ñ€Ð¾Ñ Ð½Ð° Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°"""

    prompt: str


class AdminLLMModelRequest(BaseModel):
    """Ð—Ð°Ð¿Ñ€Ð¾Ñ Ð½Ð° Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ LLM"""

    model: str  # gemini-2.5-flash, gemini-2.5-pro


class AdminTTSTestRequest(BaseModel):
    """Ð—Ð°Ð¿Ñ€Ð¾Ñ Ð½Ð° Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ ÑÐ¸Ð½Ñ‚ÐµÐ·"""

    text: str
    preset: str = "natural"


@app.get("/admin/status")
async def admin_status():
    """ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ ÑÑ‚Ð°Ñ‚ÑƒÑ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð´Ð»Ñ Ð°Ð´Ð¼Ð¸Ð½ÐºÐ¸"""
    import torch

    status = {
        "orchestrator": "running",
        "services": {
            "voice_clone": voice_service is not None,
            "llm": llm_service is not None,
            "stt": stt_service is not None,
            "streaming_tts": streaming_tts_manager is not None,
            "piper_tts": piper_service is not None,
        },
        "gpu": None,
        "streaming_tts_stats": None,
        "llm_config": None,
        "tts_config": None,
    }

    # GPU Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ
    if torch.cuda.is_available():
        gpu_info = []
        for i in range(torch.cuda.device_count()):
            try:
                name = torch.cuda.get_device_name(i)
                total = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                allocated = torch.cuda.memory_allocated(i) / (1024**3)
                gpu_info.append(
                    {
                        "id": i,
                        "name": name,
                        "total_gb": round(total, 2),
                        "used_gb": round(allocated, 2),
                    }
                )
            except Exception:
                pass
        status["gpu"] = gpu_info

    # Streaming TTS ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
    if streaming_tts_manager:
        status["streaming_tts_stats"] = streaming_tts_manager.get_stats()

    # LLM ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ
    if llm_service:
        if hasattr(llm_service, "get_config"):
            status["llm_config"] = llm_service.get_config()
        else:
            # Ð”Ð»Ñ vLLM Ð¸ Ð´Ñ€ÑƒÐ³Ð¸Ñ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð² Ð±ÐµÐ· get_config
            status["llm_config"] = {
                "model_name": getattr(llm_service, "model_name", "unknown"),
                "api_url": getattr(llm_service, "api_url", None),
                "backend": "vllm" if hasattr(llm_service, "api_url") else "gemini",
            }

    # TTS ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ
    xtts_svc = gulya_voice_service or voice_service
    if xtts_svc:
        status["tts_config"] = {
            "device": xtts_svc.device,
            "default_preset": xtts_svc.default_preset,
            "samples_count": len(xtts_svc.voice_samples),
            "cache_dir": str(xtts_svc.cache_dir),
        }

    return status


@app.get("/admin/llm/prompt")
async def admin_get_llm_prompt():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ LLM"""
    if llm_service:
        persona = getattr(llm_service, "current_persona", None) or os.getenv(
            "SECRETARY_PERSONA", "gulya"
        )
        return {
            "prompt": llm_service.system_prompt,
            "model": llm_service.model_name,
            "persona": persona,
        }
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.post("/admin/llm/prompt")
async def admin_set_llm_prompt(request: AdminLLMPromptRequest):
    """Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð½Ð¾Ð²Ñ‹Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ LLM"""
    if llm_service:
        llm_service.set_system_prompt(request.prompt)
        return {
            "status": "ok",
            "prompt": request.prompt[:100] + "..." if len(request.prompt) > 100 else request.prompt,
        }
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.get("/admin/llm/model")
async def admin_get_llm_model():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑƒÑ‰ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ LLM"""
    if llm_service:
        return {"model": llm_service.model_name}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.post("/admin/llm/model")
async def admin_set_llm_model(request: AdminLLMModelRequest):
    """Ð˜Ð·Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ LLM"""
    allowed_models = ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash"]

    if request.model not in allowed_models:
        raise HTTPException(
            status_code=400,
            detail=f"ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ: {request.model}. Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ: {allowed_models}",
        )

    if llm_service:
        try:
            llm_service.set_model(request.model)
            return {"status": "ok", "model": request.model}
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.delete("/admin/llm/history")
async def admin_clear_llm_history():
    """ÐžÑ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð° LLM"""
    if llm_service:
        count = len(llm_service.conversation_history)
        llm_service.reset_conversation()
        return {"status": "ok", "cleared_messages": count}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.get("/admin/llm/history")
async def admin_get_llm_history():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð° LLM"""
    if llm_service:
        return {
            "history": llm_service.conversation_history,
            "count": len(llm_service.conversation_history),
        }
    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== Voice Selection API ==============


class AdminVoiceRequest(BaseModel):
    voice: str  # gulya / lidia / dmitri / irina


@app.get("/admin/voices")
async def admin_get_voices():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¿Ð¸ÑÐ¾Ðº Ð²ÑÐµÑ… Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²"""
    voices = []

    # XTTS Ð³Ð¾Ð»Ð¾Ñ (Ð“ÑƒÐ»Ñ) - Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ GPU CC >= 7.0 (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ)
    if gulya_voice_service:
        voices.append(
            {
                "id": "gulya",
                "name": "Ð“ÑƒÐ»Ñ (XTTS)",
                "engine": "xtts",
                "description": "ÐšÐ»Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð³Ð¾Ð»Ð¾Ñ Ð“ÑƒÐ»Ð¸ (XTTS v2, GPU CC >= 7.0)",
                "available": True,
                "samples_count": len(gulya_voice_service.voice_samples),
                "default": True,
            }
        )

    # XTTS Ð³Ð¾Ð»Ð¾Ñ (Ð›Ð¸Ð´Ð¸Ñ) - Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ GPU CC >= 7.0
    if voice_service:
        voices.append(
            {
                "id": "lidia",
                "name": "Ð›Ð¸Ð´Ð¸Ñ (XTTS)",
                "engine": "xtts",
                "description": "ÐšÐ»Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð³Ð¾Ð»Ð¾Ñ Ð›Ð¸Ð´Ð¸Ð¸ (XTTS v2, GPU CC >= 7.0)",
                "available": True,
                "samples_count": len(voice_service.voice_samples),
            }
        )

    # OpenVoice Ð³Ð¾Ð»Ð¾Ñ (Ð›Ð¸Ð´Ð¸Ñ) - Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð½Ð° GPU CC 6.1+
    if openvoice_service:
        voices.append(
            {
                "id": "lidia_openvoice",
                "name": "Ð›Ð¸Ð´Ð¸Ñ (OpenVoice)",
                "engine": "openvoice",
                "description": "ÐšÐ»Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð³Ð¾Ð»Ð¾Ñ (OpenVoice v2, GPU CC 6.1+)",
                "available": True,
                "samples_count": len(openvoice_service.voice_samples)
                if openvoice_service.voice_samples
                else 0,
            }
        )

    # Piper Ð³Ð¾Ð»Ð¾ÑÐ° (CPU)
    if piper_service:
        piper_voices = piper_service.get_available_voices()
        for voice_id, info in piper_voices.items():
            voices.append(
                {
                    "id": voice_id,
                    "name": info["name"],
                    "engine": "piper",
                    "description": info["description"],
                    "available": info["available"],
                }
            )

    return {
        "voices": voices,
        "current": current_voice_config,
    }


@app.get("/admin/voice")
async def admin_get_current_voice():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ð¹ Ð³Ð¾Ð»Ð¾Ñ"""
    return current_voice_config


@app.post("/admin/voice")
async def admin_set_voice(request: AdminVoiceRequest):
    """Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ð³Ð¾Ð»Ð¾Ñ"""
    global current_voice_config

    voice_id = request.voice.lower()

    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚ÑŒ Ð³Ð¾Ð»Ð¾ÑÐ°
    if voice_id == "gulya":
        if not gulya_voice_service:
            raise HTTPException(
                status_code=503, detail="XTTS service (Ð“ÑƒÐ»Ñ) not available (requires GPU CC >= 7.0)"
            )
        current_voice_config = {"engine": "xtts", "voice": "gulya"}
        logger.info("ðŸŽ¤ Ð“Ð¾Ð»Ð¾Ñ Ð¸Ð·Ð¼ÐµÐ½Ñ‘Ð½ Ð½Ð°: Ð“ÑƒÐ»Ñ (XTTS)")

    elif voice_id == "lidia":
        if not voice_service:
            raise HTTPException(
                status_code=503,
                detail="XTTS service (Ð›Ð¸Ð´Ð¸Ñ) not available (requires GPU CC >= 7.0)",
            )
        current_voice_config = {"engine": "xtts", "voice": "lidia"}
        logger.info("ðŸŽ¤ Ð“Ð¾Ð»Ð¾Ñ Ð¸Ð·Ð¼ÐµÐ½Ñ‘Ð½ Ð½Ð°: Ð›Ð¸Ð´Ð¸Ñ (XTTS)")

    elif voice_id == "lidia_openvoice":
        if not openvoice_service:
            raise HTTPException(status_code=503, detail="OpenVoice service not available")
        current_voice_config = {"engine": "openvoice", "voice": "lidia_openvoice"}
        logger.info("ðŸŽ¤ Ð“Ð¾Ð»Ð¾Ñ Ð¸Ð·Ð¼ÐµÐ½Ñ‘Ð½ Ð½Ð°: Ð›Ð¸Ð´Ð¸Ñ (OpenVoice)")

    elif voice_id in ["dmitri", "irina"]:
        if not piper_service:
            raise HTTPException(status_code=503, detail="Piper TTS service not available")
        piper_voices = piper_service.get_available_voices()
        if voice_id not in piper_voices or not piper_voices[voice_id]["available"]:
            raise HTTPException(status_code=400, detail=f"Voice model not found: {voice_id}")
        current_voice_config = {"engine": "piper", "voice": voice_id}
        logger.info(f"ðŸŽ¤ Ð“Ð¾Ð»Ð¾Ñ Ð¸Ð·Ð¼ÐµÐ½Ñ‘Ð½ Ð½Ð°: {piper_voices[voice_id]['name']} (Piper)")

    else:
        raise HTTPException(
            status_code=400,
            detail=f"Unknown voice: {voice_id}. Available: gulya, lidia, lidia_openvoice, dmitri, irina",
        )

    # Sync with service container for modular routers
    from app.dependencies import get_container

    container = get_container()
    container.current_voice_config = current_voice_config

    return {"status": "ok", **current_voice_config}


@app.post("/admin/voice/test")
async def admin_test_voice(request: AdminVoiceRequest):
    """Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ ÑÐ¸Ð½Ñ‚ÐµÐ· Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ð¼ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð¼"""
    voice_id = request.voice.lower()
    test_text = "Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ! Ð­Ñ‚Ð¾ Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð³Ð¾Ð»Ð¾ÑÐ°."

    output_path = TEMP_DIR / f"voice_test_{voice_id}_{int(time.time())}.wav"

    try:
        if voice_id == "gulya":
            if not gulya_voice_service:
                raise HTTPException(
                    status_code=503, detail="XTTS (Ð“ÑƒÐ»Ñ) not available (requires GPU CC >= 7.0)"
                )
            gulya_voice_service.synthesize_to_file(test_text, str(output_path), preset="natural")

        elif voice_id == "lidia":
            if not voice_service:
                raise HTTPException(
                    status_code=503, detail="XTTS (Ð›Ð¸Ð´Ð¸Ñ) not available (requires GPU CC >= 7.0)"
                )
            voice_service.synthesize_to_file(test_text, str(output_path), preset="natural")

        elif voice_id == "lidia_openvoice":
            if not openvoice_service:
                raise HTTPException(status_code=503, detail="OpenVoice not available")
            openvoice_service.synthesize_to_file(test_text, str(output_path), language="ru")

        elif voice_id in ["dmitri", "irina"]:
            if not piper_service:
                raise HTTPException(status_code=503, detail="Piper not available")
            piper_service.synthesize_to_file(test_text, str(output_path), voice=voice_id)

        else:
            raise HTTPException(
                status_code=400,
                detail=f"Unknown voice: {voice_id}. Available: gulya, lidia, lidia_openvoice, dmitri, irina",
            )

        return FileResponse(output_path, media_type="audio/wav", filename=f"test_{voice_id}.wav")

    except Exception as e:
        logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ð³Ð¾ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð°: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def get_current_tts_service():
    """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ TTS ÑÐµÑ€Ð²Ð¸Ñ Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸"""
    engine = current_voice_config["engine"]
    voice = current_voice_config["voice"]

    if engine == "xtts" and voice == "gulya":
        return gulya_voice_service, {"preset": "natural"}
    elif engine == "xtts" and voice == "lidia":
        return voice_service, {"preset": "natural"}
    elif engine == "piper":
        return piper_service, {"voice": voice}
    else:
        # Default to gulya if available
        return gulya_voice_service or voice_service, {"preset": "natural"}


# ============== Extended Admin API Endpoints ==============


# Pydantic models for new endpoints
class AdminBackendRequest(BaseModel):
    backend: str  # "vllm", "gemini", or "cloud:{provider_id}"
    stop_unused: bool = False  # ÐžÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð½ÐµÐ¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼Ñ‹Ð¹ ÑÐµÑ€Ð²Ð¸Ñ (vLLM) Ð´Ð»Ñ Ð¾ÑÐ²Ð¾Ð±Ð¾Ð¶Ð´ÐµÐ½Ð¸Ñ GPU


class CloudProviderCreate(BaseModel):
    """Create cloud LLM provider"""

    name: str
    provider_type: str  # gemini, kimi, openai, claude, deepseek, custom
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    model_name: str = ""
    enabled: bool = True
    is_default: bool = False
    config: Optional[Dict] = None
    description: Optional[str] = None


class CloudProviderUpdate(BaseModel):
    """Update cloud LLM provider"""

    name: Optional[str] = None
    provider_type: Optional[str] = None
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    model_name: Optional[str] = None
    enabled: Optional[bool] = None
    is_default: Optional[bool] = None
    config: Optional[Dict] = None
    description: Optional[str] = None


class AdminPersonaRequest(BaseModel):
    persona: str  # "gulya" or "lidia"


class AdminLLMParamsRequest(BaseModel):
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    repetition_penalty: Optional[float] = None


class AdminXTTSParamsRequest(BaseModel):
    temperature: Optional[float] = None
    repetition_penalty: Optional[float] = None
    top_k: Optional[int] = None
    top_p: Optional[float] = None
    speed: Optional[float] = None
    gpt_cond_len: Optional[int] = None
    gpt_cond_chunk_len: Optional[int] = None


class AdminPiperParamsRequest(BaseModel):
    speed: float = 1.0


class AdminCustomPresetRequest(BaseModel):
    name: str
    params: dict


class AdminFAQRequest(BaseModel):
    trigger: str
    response: str


class AdminFAQTestRequest(BaseModel):
    text: str


class AdminWidgetConfigRequest(BaseModel):
    enabled: bool = True
    title: str = "AI ÐÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚"
    greeting: str = "Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ! ÐšÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ñ Ð¨Ð°ÐµÑ€Ð²ÑÐ¹ Ð”Ð¸-Ð˜Ð´Ð¶Ð¸Ñ‚Ð°Ð», Ñ‡ÐµÐ¼ Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ?"
    placeholder: str = "Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ..."
    primary_color: str = "#6366f1"
    position: str = "right"  # "left" or "right"
    allowed_domains: List[str] = []
    tunnel_url: str = ""


class AdminTelegramConfigRequest(BaseModel):
    enabled: bool = False
    bot_token: str = ""
    api_url: str = "http://localhost:8002"
    allowed_users: List[int] = []
    admin_users: List[int] = []
    welcome_message: str = "Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ! Ð¯ AI-Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸ Ð¨Ð°ÐµÑ€Ð²ÑÐ¹. Ð§ÐµÐ¼ Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ?"
    unauthorized_message: str = "Ð˜Ð·Ð²Ð¸Ð½Ð¸Ñ‚Ðµ, Ñƒ Ð²Ð°Ñ Ð½ÐµÑ‚ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº ÑÑ‚Ð¾Ð¼Ñƒ Ð±Ð¾Ñ‚Ñƒ."
    error_message: str = "ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ°. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð¿Ð¾Ð·Ð¶Ðµ."
    typing_enabled: bool = True


class AdminFinetuneConfigRequest(BaseModel):
    lora_rank: Optional[int] = None
    lora_alpha: Optional[int] = None
    batch_size: Optional[int] = None
    gradient_accumulation_steps: Optional[int] = None
    learning_rate: Optional[float] = None
    num_epochs: Optional[int] = None
    max_seq_length: Optional[int] = None
    output_dir: Optional[str] = None


class AdminAdapterRequest(BaseModel):
    adapter: str


class AdminAuditQueryRequest(BaseModel):
    action: Optional[str] = None
    resource: Optional[str] = None
    user_id: Optional[str] = None
    from_date: Optional[str] = None  # ISO format
    to_date: Optional[str] = None  # ISO format
    limit: int = 100
    offset: int = 0


# ============== Chat Models & Manager ==============


class ChatMessageModel(BaseModel):
    id: str
    role: str
    content: str
    timestamp: str
    edited: bool = False


class ChatSessionModel(BaseModel):
    id: str
    title: str
    messages: List[ChatMessageModel]
    system_prompt: Optional[str] = None
    created: str
    updated: str


class CreateSessionRequest(BaseModel):
    title: Optional[str] = None
    system_prompt: Optional[str] = None


class UpdateSessionRequest(BaseModel):
    title: Optional[str] = None
    system_prompt: Optional[str] = None


class LLMOverrideConfig(BaseModel):
    llm_backend: Optional[str] = None  # "vllm", "gemini", or "cloud:provider-id"
    system_prompt: Optional[str] = None
    llm_params: Optional[dict] = None


class SendMessageRequest(BaseModel):
    content: str
    llm_override: Optional[LLMOverrideConfig] = None


class EditMessageRequest(BaseModel):
    content: str


# ============== Services Endpoints ==============


@app.get("/admin/services/status")
async def admin_services_status():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð²ÑÐµÑ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²"""
    manager = get_service_manager()
    status = manager.get_all_status()

    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½Ð¸Ñ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð² Ð¸Ð· orchestrator
    status["services"]["xtts_gulya"]["is_running"] = gulya_voice_service is not None
    status["services"]["xtts_lidia"]["is_running"] = voice_service is not None
    status["services"]["piper"]["is_running"] = piper_service is not None
    status["services"]["openvoice"]["is_running"] = openvoice_service is not None
    status["services"]["orchestrator"]["is_running"] = True

    return status


@app.post("/admin/services/{service}/start")
async def admin_start_service(service: str):
    """Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ ÑÐµÑ€Ð²Ð¸Ñ"""
    manager = get_service_manager()
    return await manager.start_service(service)


@app.post("/admin/services/{service}/stop")
async def admin_stop_service(service: str):
    """ÐžÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ ÑÐµÑ€Ð²Ð¸Ñ"""
    manager = get_service_manager()
    return await manager.stop_service(service)


@app.post("/admin/services/{service}/restart")
async def admin_restart_service(service: str):
    """ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ ÑÐµÑ€Ð²Ð¸Ñ"""
    manager = get_service_manager()
    return await manager.restart_service(service)


@app.post("/admin/services/start-all")
async def admin_start_all_services():
    """Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ð²ÑÐµ Ð²Ð½ÐµÑˆÐ½Ð¸Ðµ ÑÐµÑ€Ð²Ð¸ÑÑ‹"""
    manager = get_service_manager()
    results = {}
    for service in ["vllm"]:  # Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ð²Ð½ÐµÑˆÐ½Ð¸Ðµ ÑÐµÑ€Ð²Ð¸ÑÑ‹
        results[service] = await manager.start_service(service)
    return {"status": "ok", "results": results}


@app.post("/admin/services/stop-all")
async def admin_stop_all_services():
    """ÐžÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð²ÑÐµ Ð²Ð½ÐµÑˆÐ½Ð¸Ðµ ÑÐµÑ€Ð²Ð¸ÑÑ‹"""
    manager = get_service_manager()
    results = {}
    for service in ["vllm"]:
        results[service] = await manager.stop_service(service)
    return {"status": "ok", "results": results}


# ============== Logs Endpoints ==============


@app.get("/admin/logs")
async def admin_list_logs():
    """Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð»Ð¾Ð³Ð¾Ð²"""
    manager = get_service_manager()
    return {"logs": manager.get_available_logs()}


@app.get("/admin/logs/{logfile}")
async def admin_read_log(
    logfile: str, lines: int = 100, offset: int = 0, search: Optional[str] = None
):
    """ÐŸÑ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ Ð»Ð¾Ð³ Ñ„Ð°Ð¹Ð»"""
    manager = get_service_manager()
    return manager.read_log(logfile, lines=lines, offset=offset, search=search)


@app.get("/admin/logs/stream/{logfile}")
async def admin_stream_log(logfile: str):
    """SSE streaming Ð»Ð¾Ð³Ð¾Ð²"""
    manager = get_service_manager()

    async def generate():
        async for data in manager.stream_log(logfile):
            yield f"data: {data}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"},
    )


# ============== LLM Enhanced Endpoints ==============


@app.get("/admin/llm/backend")
async def admin_get_llm_backend():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ LLM backend"""
    if llm_service:
        # Detect backend type
        if isinstance(llm_service, CloudLLMService):
            backend = f"cloud:{llm_service.provider_id}"
        elif hasattr(llm_service, "api_url"):
            backend = "vllm"
        else:
            backend = "gemini"

        return {
            "backend": backend,
            "model": getattr(llm_service, "model_name", "unknown"),
            "api_url": getattr(llm_service, "api_url", None),
            "provider_type": getattr(llm_service, "provider_type", None),
        }
    return {"backend": "none", "error": "LLM service not initialized"}


@app.get("/admin/llm/models")
async def admin_get_llm_models():
    """
    ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¿Ð¸ÑÐ¾Ðº Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ vLLM Ð¸ Ñ‚ÐµÐºÑƒÑ‰ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ.
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ Qwen, Llama, DeepSeek Ð¸ Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ….
    """
    from vllm_llm_service import AVAILABLE_MODELS

    result = {
        "available_models": AVAILABLE_MODELS,
        "current_model": None,
        "loaded_models": [],
        "backend": "none",
    }

    if llm_service:
        is_vllm = hasattr(llm_service, "api_url")
        result["backend"] = "vllm" if is_vllm else "gemini"

        if is_vllm and hasattr(llm_service, "get_current_model_info"):
            result["current_model"] = llm_service.get_current_model_info()
            result["loaded_models"] = llm_service.get_loaded_models()
        elif not is_vllm:
            # Gemini backend
            result["current_model"] = {
                "id": "gemini",
                "name": getattr(llm_service, "model_name", "gemini-2.0-flash"),
                "description": "Google Gemini API",
                "available": True,
            }

    return result


@app.post("/admin/llm/backend")
async def admin_set_llm_backend(
    request: AdminBackendRequest, user: User = Depends(get_current_user)
):
    """ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ LLM backend Ñ Ð³Ð¾Ñ€ÑÑ‡ÐµÐ¹ Ð¿ÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¾Ð¹ ÑÐµÑ€Ð²Ð¸ÑÐ°"""
    global LLM_BACKEND, llm_service

    # Check if it's a cloud provider
    if request.backend.startswith("cloud:"):
        provider_id = request.backend.split(":", 1)[1]
        return await _switch_to_cloud_provider(provider_id, request.stop_unused, user)

    if request.backend not in ["vllm", "gemini"]:
        raise HTTPException(
            status_code=400,
            detail="Invalid backend. Use 'vllm', 'gemini', or 'cloud:{provider_id}'",
        )

    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ Ð±ÑÐºÐµÐ½Ð´
    current_backend = "vllm" if (llm_service and hasattr(llm_service, "api_url")) else "gemini"
    if request.backend == current_backend:
        return {
            "status": "ok",
            "backend": request.backend,
            "message": f"Ð£Ð¶Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ {request.backend}",
        }

    manager = get_service_manager()
    stop_vllm = request.stop_unused if hasattr(request, "stop_unused") else False

    try:
        if request.backend == "vllm":
            # ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð° vLLM
            logger.info("ðŸ”„ ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð° vLLM...")

            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½ Ð»Ð¸ vLLM
            vllm_status = manager.get_service_status("vllm")

            if not vllm_status.get("is_running"):
                # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ vLLM
                logger.info("ðŸš€ Ð—Ð°Ð¿ÑƒÑÐº vLLM...")
                start_result = await manager.start_service("vllm")
                if start_result.get("status") != "ok":
                    raise HTTPException(
                        status_code=503,
                        detail=f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ vLLM: {start_result.get('message', 'Unknown error')}",
                    )

                # Ð–Ð´Ñ‘Ð¼ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸ vLLM (Ð´Ð¾ 120 ÑÐµÐºÑƒÐ½Ð´)
                logger.info("â³ ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸ vLLM...")
                import httpx

                # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ URL (ÑƒÐ´Ð°Ð»ÑÐµÐ¼ trailing /v1)
                vllm_url = os.getenv("VLLM_API_URL", "http://localhost:11434").rstrip("/")
                if vllm_url.endswith("/v1"):
                    vllm_url = vllm_url[:-3]

                for i in range(60):  # 60 * 2 = 120 ÑÐµÐºÑƒÐ½Ð´
                    await asyncio.sleep(2)
                    try:
                        async with httpx.AsyncClient() as client:
                            resp = await client.get(f"{vllm_url}/v1/models", timeout=5.0)
                            if resp.status_code == 200:
                                logger.info(f"âœ… vLLM Ð³Ð¾Ñ‚Ð¾Ð² (Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ° {i + 1})")
                                break
                    except Exception:
                        pass
                else:
                    raise HTTPException(
                        status_code=503, detail=f"vLLM Ð½Ðµ ÑÑ‚Ð°Ð» Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ Ð·Ð° 120 ÑÐµÐºÑƒÐ½Ð´ ({vllm_url})"
                    )

            # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð½Ð¾Ð²Ñ‹Ð¹ vLLM ÑÐµÑ€Ð²Ð¸Ñ
            if VLLMLLMService is None:
                raise HTTPException(status_code=503, detail="VLLMLLMService Ð½Ðµ Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½")

            new_service = VLLMLLMService()
            if not new_service.is_available():
                raise HTTPException(status_code=503, detail="vLLM Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½, Ð½Ð¾ Ð½Ðµ Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÐµÑ‚ Ð½Ð° API")

            llm_service = new_service
            LLM_BACKEND = "vllm"
            os.environ["LLM_BACKEND"] = "vllm"

            logger.info("âœ… ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾ Ð½Ð° vLLM")

            # Audit log
            await async_audit_logger.log(
                action="update",
                resource="config",
                resource_id="llm_backend",
                user_id=user.username,
                details={"backend": "vllm", "model": getattr(llm_service, "model_name", "unknown")},
            )

            return {
                "status": "ok",
                "backend": "vllm",
                "model": getattr(llm_service, "model_name", "unknown"),
                "message": "ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾ Ð½Ð° vLLM",
            }

        else:
            # ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð° Gemini
            logger.info("ðŸ”„ ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð° Gemini...")

            new_service = LLMService()
            llm_service = new_service
            LLM_BACKEND = "gemini"
            os.environ["LLM_BACKEND"] = "gemini"

            # ÐžÐ¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ Ð¾ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ vLLM Ð´Ð»Ñ Ð¾ÑÐ²Ð¾Ð±Ð¾Ð¶Ð´ÐµÐ½Ð¸Ñ GPU
            if stop_vllm:
                vllm_status = manager.get_service_status("vllm")
                if vllm_status.get("is_running"):
                    logger.info("ðŸ›‘ ÐžÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ vLLM Ð´Ð»Ñ Ð¾ÑÐ²Ð¾Ð±Ð¾Ð¶Ð´ÐµÐ½Ð¸Ñ GPU...")
                    await manager.stop_service("vllm")

            logger.info("âœ… ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾ Ð½Ð° Gemini")

            # Audit log
            await async_audit_logger.log(
                action="update",
                resource="config",
                resource_id="llm_backend",
                user_id=user.username,
                details={
                    "backend": "gemini",
                    "model": getattr(llm_service, "model_name", "unknown"),
                },
            )

            return {
                "status": "ok",
                "backend": "gemini",
                "model": getattr(llm_service, "model_name", "unknown"),
                "message": "ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾ Ð½Ð° Gemini" + (" (vLLM Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½)" if stop_vllm else ""),
            }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ð±ÑÐºÐµÐ½Ð´Ð°: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def _switch_to_cloud_provider(provider_id: str, stop_unused: bool, user: User):
    """Helper function to switch to a cloud provider"""
    global LLM_BACKEND, llm_service

    provider_config = await async_cloud_provider_manager.get_provider_with_key(provider_id)
    if not provider_config:
        raise HTTPException(status_code=404, detail=f"Provider {provider_id} not found")

    if not provider_config.get("enabled"):
        raise HTTPException(status_code=400, detail=f"Provider {provider_id} is disabled")

    if not provider_config.get("api_key"):
        raise HTTPException(
            status_code=400, detail=f"Provider {provider_id} has no API key configured"
        )

    try:
        new_service = CloudLLMService(provider_config)
        if not new_service.is_available():
            raise HTTPException(status_code=503, detail=f"Provider {provider_id} is not responding")

        llm_service = new_service
        LLM_BACKEND = f"cloud:{provider_id}"
        os.environ["LLM_BACKEND"] = LLM_BACKEND

        # Optionally stop vLLM to free GPU
        if stop_unused:
            manager = get_service_manager()
            vllm_status = manager.get_service_status("vllm")
            if vllm_status.get("is_running"):
                logger.info("ðŸ›‘ Stopping vLLM to free GPU memory...")
                await manager.stop_service("vllm")

        logger.info(f"âœ… Switched to cloud provider: {provider_config.get('name')}")

        # Audit log
        await async_audit_logger.log(
            action="update",
            resource="config",
            resource_id="llm_backend",
            user_id=user.username,
            details={
                "backend": f"cloud:{provider_id}",
                "provider_type": provider_config.get("provider_type"),
                "model": provider_config.get("model_name"),
            },
        )

        return {
            "status": "ok",
            "backend": f"cloud:{provider_id}",
            "provider_id": provider_id,
            "provider_type": provider_config.get("provider_type"),
            "model": provider_config.get("model_name"),
            "message": f"Switched to cloud provider: {provider_config.get('name')}",
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Error switching to cloud provider: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============== Cloud LLM Providers API ==============


@app.get("/admin/llm/providers")
async def admin_list_cloud_providers(enabled_only: bool = False):
    """List all cloud LLM providers"""
    providers = await async_cloud_provider_manager.list_providers(enabled_only)
    return {
        "providers": providers,
        "provider_types": PROVIDER_TYPES,
    }


@app.get("/admin/llm/providers/{provider_id}")
async def admin_get_cloud_provider(
    provider_id: str, include_key: bool = False, user: User = Depends(get_current_user)
):
    """Get cloud provider by ID"""
    if include_key:
        provider = await async_cloud_provider_manager.get_provider_with_key(provider_id)
    else:
        provider = await async_cloud_provider_manager.get_provider(provider_id)

    if not provider:
        raise HTTPException(status_code=404, detail="Provider not found")
    return {"provider": provider}


@app.post("/admin/llm/providers")
async def admin_create_cloud_provider(
    data: CloudProviderCreate, user: User = Depends(get_current_user)
):
    """Create new cloud LLM provider"""
    try:
        provider = await async_cloud_provider_manager.create_provider(
            name=data.name,
            provider_type=data.provider_type,
            api_key=data.api_key,
            base_url=data.base_url,
            model_name=data.model_name,
            enabled=data.enabled,
            is_default=data.is_default,
            config=data.config,
            description=data.description,
        )

        # Audit log
        await async_audit_logger.log(
            action="create",
            resource="cloud_provider",
            resource_id=provider["id"],
            user_id=user.username,
            details={"name": data.name, "provider_type": data.provider_type},
        )

        return {"status": "ok", "provider": provider}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@app.put("/admin/llm/providers/{provider_id}")
async def admin_update_cloud_provider(
    provider_id: str, data: CloudProviderUpdate, user: User = Depends(get_current_user)
):
    """Update cloud LLM provider"""
    # Filter out None values
    update_data = {k: v for k, v in data.model_dump().items() if v is not None}

    provider = await async_cloud_provider_manager.update_provider(provider_id, **update_data)
    if not provider:
        raise HTTPException(status_code=404, detail="Provider not found")

    # Audit log
    await async_audit_logger.log(
        action="update",
        resource="cloud_provider",
        resource_id=provider_id,
        user_id=user.username,
        details=update_data,
    )

    return {"status": "ok", "provider": provider}


@app.delete("/admin/llm/providers/{provider_id}")
async def admin_delete_cloud_provider(provider_id: str, user: User = Depends(get_current_user)):
    """Delete cloud LLM provider"""
    if not await async_cloud_provider_manager.delete_provider(provider_id):
        raise HTTPException(status_code=404, detail="Provider not found")

    # Audit log
    await async_audit_logger.log(
        action="delete",
        resource="cloud_provider",
        resource_id=provider_id,
        user_id=user.username,
    )

    return {"status": "ok", "message": f"Provider {provider_id} deleted"}


@app.post("/admin/llm/providers/{provider_id}/test")
async def admin_test_cloud_provider(provider_id: str, user: User = Depends(get_current_user)):
    """Test cloud provider connection"""
    provider_config = await async_cloud_provider_manager.get_provider_with_key(provider_id)
    if not provider_config:
        raise HTTPException(status_code=404, detail="Provider not found")

    if not provider_config.get("api_key"):
        return {
            "status": "error",
            "available": False,
            "message": "No API key configured",
        }

    try:
        service = CloudLLMService(provider_config)
        is_available = service.is_available()

        if is_available:
            # Quick test generation
            test_response = service.generate_response("Ð¡ÐºÐ°Ð¶Ð¸ 'Ñ‚ÐµÑÑ‚ Ð¾Ðº'", use_history=False)
            return {
                "status": "ok",
                "available": True,
                "test_response": test_response[:200] if test_response else "",
            }
        else:
            return {
                "status": "error",
                "available": False,
                "message": "Provider not responding",
            }
    except Exception as e:
        return {
            "status": "error",
            "available": False,
            "message": str(e),
        }


@app.post("/admin/llm/providers/{provider_id}/set-default")
async def admin_set_default_cloud_provider(
    provider_id: str, user: User = Depends(get_current_user)
):
    """Set cloud provider as default"""
    if not await async_cloud_provider_manager.set_default(provider_id):
        raise HTTPException(status_code=404, detail="Provider not found or disabled")

    await async_audit_logger.log(
        action="update",
        resource="cloud_provider",
        resource_id=provider_id,
        user_id=user.username,
        details={"is_default": True},
    )

    return {"status": "ok", "message": f"Provider {provider_id} set as default"}


@app.get("/admin/llm/personas")
async def admin_get_personas():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¿Ð¸ÑÐ¾Ðº Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð¿ÐµÑ€ÑÐ¾Ð½"""
    if llm_service and hasattr(llm_service, "get_available_personas"):
        return {"personas": llm_service.get_available_personas()}

    # Fallback Ð´Ð»Ñ Gemini LLM Service
    from vllm_llm_service import SECRETARY_PERSONAS

    return {
        "personas": {
            pid: {"name": p["name"], "full_name": p.get("full_name", p["name"])}
            for pid, p in SECRETARY_PERSONAS.items()
        }
    }


@app.get("/admin/llm/persona")
async def admin_get_current_persona():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑƒÑ‰ÑƒÑŽ Ð¿ÐµÑ€ÑÐ¾Ð½Ñƒ"""
    if llm_service:
        persona_id = getattr(llm_service, "persona_id", "gulya")
        persona = getattr(llm_service, "persona", {})
        return {
            "id": persona_id,
            "name": persona.get("name", "Unknown"),
        }
    return {"id": "none", "error": "LLM service not initialized"}


@app.post("/admin/llm/persona")
async def admin_set_persona(request: AdminPersonaRequest, user: User = Depends(get_current_user)):
    """Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð¿ÐµÑ€ÑÐ¾Ð½Ñƒ"""
    if llm_service and hasattr(llm_service, "set_persona"):
        success = llm_service.set_persona(request.persona)
        if success:
            # Audit log
            await async_audit_logger.log(
                action="update",
                resource="config",
                resource_id="llm_persona",
                user_id=user.username,
                details={"persona": request.persona},
            )
            return {"status": "ok", "persona": request.persona}
        raise HTTPException(status_code=400, detail=f"Persona not found: {request.persona}")
    raise HTTPException(status_code=503, detail="LLM service does not support personas")


@app.get("/admin/llm/params")
async def admin_get_llm_params():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ LLM"""
    if llm_service and hasattr(llm_service, "runtime_params"):
        return {"params": llm_service.runtime_params}

    # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ
    return {
        "params": {"temperature": 0.7, "max_tokens": 512, "top_p": 0.9, "repetition_penalty": 1.1}
    }


@app.post("/admin/llm/params")
async def admin_set_llm_params(request: AdminLLMParamsRequest):
    """Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ LLM"""
    if llm_service and hasattr(llm_service, "set_params"):
        params = {k: v for k, v in request.dict().items() if v is not None}
        llm_service.set_params(**params)
        return {"status": "ok", "params": llm_service.runtime_params}

    # Ð”Ð»Ñ vLLM ÑÐµÑ€Ð²Ð¸ÑÐ° Ð±ÐµÐ· set_params - ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ‚Ðµ
    if llm_service:
        if not hasattr(llm_service, "runtime_params"):
            llm_service.runtime_params = {}
        params = {k: v for k, v in request.dict().items() if v is not None}
        llm_service.runtime_params.update(params)
        return {"status": "ok", "params": llm_service.runtime_params}

    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.get("/admin/llm/prompt/{persona}")
async def admin_get_persona_prompt(persona: str):
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð¿ÐµÑ€ÑÐ¾Ð½Ñ‹"""
    try:
        from vllm_llm_service import SECRETARY_PERSONAS

        if persona in SECRETARY_PERSONAS:
            return {"persona": persona, "prompt": SECRETARY_PERSONAS[persona]["prompt"]}
        raise HTTPException(status_code=404, detail=f"Persona not found: {persona}")
    except ImportError:
        raise HTTPException(status_code=503, detail="vLLM service not available")


@app.post("/admin/llm/prompt/{persona}")
async def admin_set_persona_prompt(persona: str, request: AdminLLMPromptRequest):
    """Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð¿ÐµÑ€ÑÐ¾Ð½Ñ‹"""
    try:
        from vllm_llm_service import SECRETARY_PERSONAS

        if persona not in SECRETARY_PERSONAS:
            raise HTTPException(status_code=404, detail=f"Persona not found: {persona}")

        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚
        SECRETARY_PERSONAS[persona]["prompt"] = request.prompt

        # Ð•ÑÐ»Ð¸ ÑÑ‚Ð¾ Ñ‚ÐµÐºÑƒÑ‰Ð°Ñ Ð¿ÐµÑ€ÑÐ¾Ð½Ð° - Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð² ÑÐµÑ€Ð²Ð¸ÑÐµ
        if llm_service and hasattr(llm_service, "persona_id") and llm_service.persona_id == persona:
            llm_service.system_prompt = request.prompt

        return {"status": "ok", "persona": persona}
    except ImportError:
        raise HTTPException(status_code=503, detail="vLLM service not available")


@app.post("/admin/llm/prompt/{persona}/reset")
async def admin_reset_persona_prompt(persona: str):
    """Ð¡Ð±Ñ€Ð¾ÑÐ¸Ñ‚ÑŒ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð¿ÐµÑ€ÑÐ¾Ð½Ñ‹ Ð½Ð° Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ"""
    # TODO: Ð ÐµÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ñ‚ÑŒ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð¾Ð²
    raise HTTPException(status_code=501, detail="Not implemented yet")


# ============== Fine-tuning Endpoints ==============


@app.post("/admin/finetune/dataset/upload")
async def admin_upload_dataset(file: UploadFile = File(...)):
    """Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ (Telegram export JSON)"""
    manager = get_finetune_manager()
    content = await file.read()
    return await manager.upload_dataset(content, file.filename)


class DatasetProcessRequest(BaseModel):
    owner_name: Optional[str] = None
    transcribe_voice: Optional[bool] = None
    min_dialog_messages: Optional[int] = None
    max_message_length: Optional[int] = None
    max_dialog_length: Optional[int] = None
    include_groups: Optional[bool] = None
    output_name: Optional[str] = None


@app.post("/admin/finetune/dataset/process")
async def admin_process_dataset(request: Optional[DatasetProcessRequest] = None):
    """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚"""
    manager = get_finetune_manager()
    config = request.model_dump(exclude_none=True) if request else None
    return await manager.process_dataset(config)


@app.get("/admin/finetune/dataset/config")
async def admin_get_dataset_config():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°"""
    manager = get_finetune_manager()
    return {"config": manager.get_dataset_config()}


@app.post("/admin/finetune/dataset/config")
async def admin_set_dataset_config(request: DatasetProcessRequest):
    """Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°"""
    manager = get_finetune_manager()
    return manager.set_dataset_config(**request.model_dump(exclude_none=True))


@app.get("/admin/finetune/dataset/processing-status")
async def admin_get_processing_status():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°"""
    manager = get_finetune_manager()
    return {"status": manager.get_processing_status()}


@app.get("/admin/finetune/dataset/stats")
async def admin_get_dataset_stats():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°"""
    manager = get_finetune_manager()
    stats = manager.get_dataset_stats()
    return {
        "stats": {
            "total_sessions": stats.total_sessions,
            "total_messages": stats.total_messages,
            "total_tokens": stats.total_tokens,
            "avg_tokens_per_message": stats.avg_tokens_per_message,
            "file_path": stats.file_path,
            "file_size_mb": stats.file_size_mb,
            "modified": stats.modified,
        }
    }


@app.get("/admin/finetune/dataset/list")
async def admin_list_datasets():
    """Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð¾Ð²"""
    manager = get_finetune_manager()
    return {"datasets": manager.list_datasets()}


@app.post("/admin/finetune/dataset/augment")
async def admin_augment_dataset():
    """ÐÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚"""
    manager = get_finetune_manager()
    return await manager.augment_dataset()


class GenerateProjectDatasetRequest(BaseModel):
    include_tz: bool = True
    include_faq: bool = True
    include_docs: bool = True
    include_escalation: bool = True
    output_name: str = "project_dataset"


@app.post("/admin/finetune/dataset/generate-project")
async def admin_generate_project_dataset(request: GenerateProjectDatasetRequest):
    """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð¸Ð· Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð½Ñ‹Ñ… Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð² (Ð¢Ð—, FAQ, Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ, ÑÑÐºÐ°Ð»Ð°Ñ†Ð¸Ð¸)"""
    manager = get_finetune_manager()
    return await manager.generate_project_dataset(
        include_tz=request.include_tz,
        include_faq=request.include_faq,
        include_docs=request.include_docs,
        include_escalation=request.include_escalation,
        output_name=request.output_name,
    )


@app.get("/admin/finetune/config")
async def admin_get_finetune_config():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
    manager = get_finetune_manager()
    config = manager.get_config()
    return {
        "config": {
            "base_model": config.base_model,
            "lora_rank": config.lora_rank,
            "lora_alpha": config.lora_alpha,
            "lora_dropout": config.lora_dropout,
            "batch_size": config.batch_size,
            "gradient_accumulation_steps": config.gradient_accumulation_steps,
            "learning_rate": config.learning_rate,
            "num_epochs": config.num_epochs,
            "warmup_ratio": config.warmup_ratio,
            "max_seq_length": config.max_seq_length,
            "output_dir": config.output_dir,
        },
        "presets": {
            name: {
                "lora_rank": p.lora_rank,
                "batch_size": p.batch_size,
                "num_epochs": p.num_epochs,
            }
            for name, p in manager.get_config_presets().items()
        },
    }


@app.post("/admin/finetune/config")
async def admin_set_finetune_config(request: AdminFinetuneConfigRequest):
    """Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
    manager = get_finetune_manager()
    config = manager.get_config()

    # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿ÐµÑ€ÐµÐ´Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
    if request.lora_rank is not None:
        config.lora_rank = request.lora_rank
    if request.lora_alpha is not None:
        config.lora_alpha = request.lora_alpha
    if request.batch_size is not None:
        config.batch_size = request.batch_size
    if request.gradient_accumulation_steps is not None:
        config.gradient_accumulation_steps = request.gradient_accumulation_steps
    if request.learning_rate is not None:
        config.learning_rate = request.learning_rate
    if request.num_epochs is not None:
        config.num_epochs = request.num_epochs
    if request.max_seq_length is not None:
        config.max_seq_length = request.max_seq_length
    if request.output_dir is not None:
        config.output_dir = request.output_dir

    return manager.set_config(config)


@app.post("/admin/finetune/train/start")
async def admin_start_training():
    """Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ"""
    manager = get_finetune_manager()
    return await manager.start_training()


@app.post("/admin/finetune/train/stop")
async def admin_stop_training():
    """ÐžÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ"""
    manager = get_finetune_manager()
    return await manager.stop_training()


@app.get("/admin/finetune/train/status")
async def admin_get_training_status():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
    manager = get_finetune_manager()
    status = manager.get_training_status()
    return {
        "status": {
            "is_running": status.is_running,
            "current_step": status.current_step,
            "total_steps": status.total_steps,
            "current_epoch": status.current_epoch,
            "total_epochs": status.total_epochs,
            "loss": status.loss,
            "learning_rate": status.learning_rate,
            "elapsed_seconds": status.elapsed_seconds,
            "eta_seconds": status.eta_seconds,
            "error": status.error,
        }
    }


@app.get("/admin/finetune/train/log")
async def admin_stream_training_log():
    """SSE streaming Ð»Ð¾Ð³Ð° Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
    manager = get_finetune_manager()

    async def generate():
        async for data in manager.stream_training_log():
            yield f"data: {data}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"},
    )


@app.get("/admin/finetune/adapters")
async def admin_list_adapters():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¿Ð¸ÑÐ¾Ðº LoRA Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€Ð¾Ð²"""
    manager = get_finetune_manager()
    adapters = manager.list_adapters()
    return {
        "adapters": [
            {
                "name": a.name,
                "path": a.path,
                "size_mb": a.size_mb,
                "modified": a.modified,
                "active": a.active,
                "config": a.config,
            }
            for a in adapters
        ],
        "active": manager.active_adapter,
    }


@app.post("/admin/finetune/adapters/activate")
async def admin_activate_adapter(request: AdminAdapterRequest):
    """ÐÐºÑ‚Ð¸Ð²Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ LoRA Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€"""
    manager = get_finetune_manager()
    return await manager.activate_adapter(request.adapter)


@app.delete("/admin/finetune/adapters/{name}")
async def admin_delete_adapter(name: str):
    """Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ LoRA Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€"""
    manager = get_finetune_manager()
    return await manager.delete_adapter(name)


# ============== TTS Finetune Endpoints ==============


@app.get("/admin/tts-finetune/config")
async def admin_get_tts_finetune_config():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ TTS fine-tuning"""
    manager = get_tts_finetune_manager()
    return {"config": manager.get_config()}


@app.post("/admin/tts-finetune/config")
async def admin_set_tts_finetune_config(config: dict):
    """ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ TTS fine-tuning"""
    manager = get_tts_finetune_manager()
    return {"status": "ok", "config": manager.set_config(config)}


@app.get("/admin/tts-finetune/samples")
async def admin_get_tts_samples():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð² Ð³Ð¾Ð»Ð¾ÑÐ°"""
    manager = get_tts_finetune_manager()
    return {"samples": manager.get_samples()}


@app.post("/admin/tts-finetune/samples/upload")
async def admin_upload_tts_sample(file: UploadFile = File(...)):
    """Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¾Ð±Ñ€Ð°Ð·ÐµÑ† Ð³Ð¾Ð»Ð¾ÑÐ°"""
    manager = get_tts_finetune_manager()
    content = await file.read()
    sample = manager.add_sample(file.filename, content)
    return {
        "status": "ok",
        "sample": {
            "filename": sample.filename,
            "path": sample.path,
            "duration_sec": sample.duration_sec,
            "size_kb": sample.size_kb,
        },
    }


@app.delete("/admin/tts-finetune/samples/{filename}")
async def admin_delete_tts_sample(filename: str):
    """Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð¾Ð±Ñ€Ð°Ð·ÐµÑ† Ð³Ð¾Ð»Ð¾ÑÐ°"""
    manager = get_tts_finetune_manager()
    if manager.delete_sample(filename):
        return {"status": "ok", "message": f"Sample {filename} deleted"}
    raise HTTPException(status_code=404, detail="Sample not found")


@app.put("/admin/tts-finetune/samples/{filename}/transcript")
async def admin_update_tts_transcript(filename: str, request: dict):
    """ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸ÑŽ Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð°"""
    manager = get_tts_finetune_manager()
    transcript = request.get("transcript", "")
    sample = manager.update_transcript(filename, transcript)
    if not sample:
        raise HTTPException(status_code=404, detail="Sample not found")
    return {
        "status": "ok",
        "sample": {
            "filename": sample.filename,
            "transcript": sample.transcript,
            "transcript_edited": sample.transcript_edited,
        },
    }


@app.post("/admin/tts-finetune/transcribe")
async def admin_transcribe_tts_samples():
    """Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð±Ð°Ñ†Ð¸ÑŽ Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· Whisper"""
    manager = get_tts_finetune_manager()
    if manager.transcribe_samples():
        return {"status": "ok", "message": "Transcription started"}
    return {"status": "error", "message": "Already running or no samples to transcribe"}


@app.post("/admin/tts-finetune/prepare")
async def admin_prepare_tts_dataset():
    """ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð¸Ñ‚ÑŒ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ (Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ audio_codes)"""
    manager = get_tts_finetune_manager()
    if manager.prepare_dataset():
        return {"status": "ok", "message": "Dataset preparation started"}
    return {"status": "error", "message": "Already running or no samples with transcripts"}


@app.get("/admin/tts-finetune/processing-status")
async def admin_get_tts_processing_status():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸"""
    manager = get_tts_finetune_manager()
    return {"status": manager.get_processing_status()}


@app.post("/admin/tts-finetune/train/start")
async def admin_start_tts_training():
    """Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ TTS"""
    manager = get_tts_finetune_manager()
    if manager.start_training():
        return {"status": "ok", "message": "Training started"}
    return {"status": "error", "message": "Already running or dataset not prepared"}


@app.post("/admin/tts-finetune/train/stop")
async def admin_stop_tts_training():
    """ÐžÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ TTS"""
    manager = get_tts_finetune_manager()
    if manager.stop_training():
        return {"status": "ok", "message": "Training stopped"}
    return {"status": "error", "message": "Training not running"}


@app.get("/admin/tts-finetune/train/status")
async def admin_get_tts_training_status():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ TTS"""
    manager = get_tts_finetune_manager()
    return {"status": manager.get_training_status()}


@app.get("/admin/tts-finetune/train/log")
async def admin_get_tts_training_log():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð»Ð¾Ð³ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ TTS"""
    manager = get_tts_finetune_manager()
    return {"log": manager.get_training_log()}


@app.get("/admin/tts-finetune/models")
async def admin_get_tts_trained_models():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ñ… TTS Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹"""
    manager = get_tts_finetune_manager()
    return {"models": manager.get_trained_models()}


# ============== Monitoring Endpoints ==============


@app.get("/admin/monitor/gpu")
async def admin_get_gpu_stats():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ GPU"""
    import torch

    if not torch.cuda.is_available():
        return {"available": False, "gpus": []}

    gpus = []
    for i in range(torch.cuda.device_count()):
        try:
            name = torch.cuda.get_device_name(i)
            props = torch.cuda.get_device_properties(i)
            total_memory = props.total_memory / (1024**3)
            allocated = torch.cuda.memory_allocated(i) / (1024**3)
            reserved = torch.cuda.memory_reserved(i) / (1024**3)

            # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑƒÑ‚Ð¸Ð»Ð¸Ð·Ð°Ñ†Ð¸ÑŽ Ñ‡ÐµÑ€ÐµÐ· nvidia-smi
            utilization = None
            temperature = None
            try:
                import subprocess

                result = subprocess.run(
                    [
                        "nvidia-smi",
                        f"--id={i}",
                        "--query-gpu=utilization.gpu,temperature.gpu",
                        "--format=csv,noheader,nounits",
                    ],
                    capture_output=True,
                    text=True,
                    timeout=5,
                )
                if result.returncode == 0:
                    parts = result.stdout.strip().split(",")
                    if len(parts) >= 2:
                        utilization = int(parts[0].strip())
                        temperature = int(parts[1].strip())
            except Exception:
                pass

            gpus.append(
                {
                    "id": i,
                    "name": name,
                    "total_memory_gb": round(total_memory, 2),
                    "allocated_gb": round(allocated, 2),
                    "reserved_gb": round(reserved, 2),
                    "free_gb": round(total_memory - reserved, 2),
                    "utilization_percent": utilization,
                    "temperature_c": temperature,
                    "compute_capability": f"{props.major}.{props.minor}",
                }
            )
        except Exception as e:
            logger.warning(f"Error getting GPU {i} stats: {e}")

    return {"available": True, "gpus": gpus}


@app.get("/admin/monitor/gpu/stream")
async def admin_stream_gpu_stats():
    """SSE streaming ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ GPU"""
    import torch

    async def generate():
        while True:
            if torch.cuda.is_available():
                gpus = []
                for i in range(torch.cuda.device_count()):
                    try:
                        allocated = torch.cuda.memory_allocated(i) / (1024**3)
                        reserved = torch.cuda.memory_reserved(i) / (1024**3)
                        gpus.append(
                            {
                                "id": i,
                                "allocated_gb": round(allocated, 2),
                                "reserved_gb": round(reserved, 2),
                            }
                        )
                    except Exception:
                        pass

                yield f"data: {json.dumps({'gpus': gpus, 'timestamp': datetime.now().isoformat()})}\n\n"
            else:
                yield f"data: {json.dumps({'available': False})}\n\n"

            await asyncio.sleep(5)

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"},
    )


@app.get("/admin/monitor/health")
async def admin_get_health():
    """Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÑ Ð²ÑÐµÑ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²"""
    manager = get_service_manager()
    health = {"timestamp": datetime.now().isoformat(), "overall": "healthy", "components": {}}

    # Orchestrator
    health["components"]["orchestrator"] = {"status": "healthy", "uptime": "running"}

    # LLM
    if llm_service:
        try:
            if hasattr(llm_service, "is_available") and llm_service.is_available():
                health["components"]["llm"] = {"status": "healthy", "backend": "vllm"}
            else:
                health["components"]["llm"] = {"status": "healthy", "backend": "gemini"}
        except Exception as e:
            health["components"]["llm"] = {"status": "unhealthy", "error": str(e)}
            health["overall"] = "degraded"
    else:
        health["components"]["llm"] = {"status": "unavailable"}
        health["overall"] = "degraded"

    # TTS
    if gulya_voice_service or voice_service:
        health["components"]["tts_xtts"] = {"status": "healthy"}
    else:
        health["components"]["tts_xtts"] = {"status": "unavailable"}

    if piper_service:
        health["components"]["tts_piper"] = {"status": "healthy"}
    else:
        health["components"]["tts_piper"] = {"status": "unavailable"}

    # vLLM external process
    vllm_status = manager.get_service_status("vllm")
    if vllm_status["is_running"]:
        health["components"]["vllm_process"] = {"status": "healthy", "pid": vllm_status["pid"]}
    else:
        health["components"]["vllm_process"] = {"status": "stopped"}

    return health


@app.get("/admin/monitor/metrics")
async def admin_get_metrics():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹"""
    import psutil

    metrics = {
        "timestamp": datetime.now().isoformat(),
        "system": {
            "cpu_percent": psutil.cpu_percent(interval=0.1),
            "memory_percent": psutil.virtual_memory().percent,
            "memory_used_gb": round(psutil.virtual_memory().used / (1024**3), 2),
            "memory_total_gb": round(psutil.virtual_memory().total / (1024**3), 2),
        },
        "streaming_tts": streaming_tts_manager.get_stats() if streaming_tts_manager else None,
    }

    # LLM Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
    if llm_service:
        metrics["llm"] = {
            "history_length": len(getattr(llm_service, "conversation_history", [])),
            "faq_count": len(getattr(llm_service, "faq", {})),
        }

    return metrics


@app.get("/admin/monitor/errors")
async def admin_get_errors():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ Ð¾ÑˆÐ¸Ð±ÐºÐ¸"""
    manager = get_service_manager()
    return {"errors": manager.last_errors, "timestamp": datetime.now().isoformat()}


@app.post("/admin/monitor/metrics/reset")
async def admin_reset_metrics():
    """Ð¡Ð±Ñ€Ð¾ÑÐ¸Ñ‚ÑŒ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸"""
    # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ ÐºÑÑˆ TTS
    if streaming_tts_manager:
        with streaming_tts_manager._cache_lock:
            streaming_tts_manager._cache.clear()

    return {"status": "ok", "message": "Metrics reset"}


@app.get("/admin/monitor/system")
async def admin_get_system_status():
    """ÐŸÐ¾Ð»Ð½Ð°Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ: GPU, CPU, RAM, Ð´Ð¸ÑÐºÐ¸, Docker, ÑÐµÑ‚ÑŒ"""
    monitor = get_system_monitor()
    return monitor.get_full_status()


# ============== Model Management API ==============


@app.get("/admin/models/list")
async def admin_list_models():
    """Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð²ÑÐµÑ… Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹"""
    manager = get_model_manager()
    return {"models": manager.get_cached_models()}


@app.post("/admin/models/scan")
async def admin_scan_models(request: Request):
    """Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹"""
    data = await request.json() if request.headers.get("content-type") == "application/json" else {}
    include_system = data.get("include_system", False)

    manager = get_model_manager()
    if manager.scan_all_models(include_system=include_system):
        return {"status": "ok", "message": "Scan started"}
    else:
        return {"status": "error", "message": "Scan already in progress"}


@app.post("/admin/models/scan/cancel")
async def admin_cancel_scan():
    """ÐžÑ‚Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ"""
    manager = get_model_manager()
    manager.cancel_scan()
    return {"status": "ok", "message": "Scan cancelled"}


@app.get("/admin/models/scan/status")
async def admin_scan_status():
    """Ð¡Ñ‚Ð°Ñ‚ÑƒÑ ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ"""
    manager = get_model_manager()
    return {"status": manager.get_scan_progress()}


@app.post("/admin/models/download")
async def admin_download_model(request: Request):
    """Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ HuggingFace"""
    data = await request.json()
    repo_id = data.get("repo_id")
    revision = data.get("revision", "main")

    if not repo_id:
        raise HTTPException(status_code=400, detail="repo_id required")

    manager = get_model_manager()
    if manager.download_model(repo_id, revision):
        return {"status": "ok", "message": f"Download started: {repo_id}"}
    else:
        return {"status": "error", "message": "Download already in progress"}


@app.post("/admin/models/download/cancel")
async def admin_cancel_download():
    """ÐžÑ‚Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÑƒ"""
    manager = get_model_manager()
    manager.cancel_download()
    return {"status": "ok", "message": "Download cancelled"}


@app.get("/admin/models/download/status")
async def admin_download_status():
    """Ð¡Ñ‚Ð°Ñ‚ÑƒÑ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸"""
    manager = get_model_manager()
    return {"status": manager.get_download_progress()}


@app.delete("/admin/models/delete")
async def admin_delete_model(path: str):
    """Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ"""
    manager = get_model_manager()
    result = manager.delete_model(path)
    if result["status"] == "ok":
        return result
    else:
        raise HTTPException(status_code=400, detail=result.get("error", "Delete failed"))


@app.get("/admin/models/search")
async def admin_search_huggingface(query: str, limit: int = 20):
    """ÐŸÐ¾Ð¸ÑÐº Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð½Ð° HuggingFace"""
    manager = get_model_manager()
    results = manager.search_huggingface(query, limit)
    return {"results": results}


@app.get("/admin/models/details/{repo_id:path}")
async def admin_get_model_details(repo_id: str):
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´ÐµÑ‚Ð°Ð»Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ HuggingFace"""
    manager = get_model_manager()
    details = manager.get_model_details(repo_id)
    if details:
        return {"details": details}
    else:
        raise HTTPException(status_code=404, detail="Model not found")


# ============== Widget Script Endpoint ==============


def _escape_js_string(s: str) -> str:
    """Escape a string for safe use in JavaScript."""
    return s.replace("'", "\\'")


@app.get("/widget.js")
async def get_widget_script(request: Request, instance: Optional[str] = None):
    """Ð”Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼Ñ‹Ð¹ ÑÐºÑ€Ð¸Ð¿Ñ‚ Ð²Ð¸Ð´Ð¶ÐµÑ‚Ð°.

    Args:
        instance: Optional widget instance ID. If not provided, uses legacy config or 'default' instance.
    """
    config = None

    # Try to load from widget instance if specified
    if instance:
        instance_data = await async_widget_instance_manager.get_instance(instance)
        if instance_data:
            config = instance_data
        else:
            return Response(
                content=f"// Widget instance '{instance}' not found",
                media_type="application/javascript",
                status_code=404,
            )
    else:
        # Try default instance first, fallback to legacy config
        instance_data = await async_widget_instance_manager.get_instance("default")
        if instance_data:
            config = instance_data
        else:
            # Fallback to legacy config
            config = await async_config_manager.get_widget()

    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½ Ð»Ð¸ Ð²Ð¸Ð´Ð¶ÐµÑ‚
    if not config.get("enabled", False):
        return Response(content="// Widget is disabled", media_type="application/javascript")

    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð´Ð¾Ð¼ÐµÐ½ (ÐµÑÐ»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½Ñ‹ Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð½Ñ‹Ðµ)
    origin = request.headers.get("origin", "") or request.headers.get("referer", "")
    allowed_domains = config.get("allowed_domains", [])
    if allowed_domains and origin:
        origin_domain = (
            origin.replace("https://", "").replace("http://", "").split("/")[0].split(":")[0]
        )
        if not any(d in origin_domain for d in allowed_domains):
            return Response(
                content=f"// Widget not allowed for domain: {origin_domain}",
                media_type="application/javascript",
            )

    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ API URL
    api_url = config.get("tunnel_url", "").strip()
    if not api_url:
        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ Ñ…Ð¾ÑÑ‚ ÐµÑÐ»Ð¸ tunnel_url Ð½Ðµ ÑƒÐºÐ°Ð·Ð°Ð½
        api_url = str(request.base_url).rstrip("/")

    # Ð§Ð¸Ñ‚Ð°ÐµÐ¼ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¹ ÑÐºÑ€Ð¸Ð¿Ñ‚ Ð²Ð¸Ð´Ð¶ÐµÑ‚Ð°
    widget_path = Path(__file__).parent / "web-widget" / "ai-chat-widget.js"
    if not widget_path.exists():
        return Response(
            content="// Widget script not found",
            media_type="application/javascript",
            status_code=404,
        )

    widget_js = widget_path.read_text(encoding="utf-8")

    # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ ÑÐºÑ€Ð¸Ð¿Ñ‚ Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°Ð¼Ð¸
    instance_id = instance or config.get("id", "default")
    settings_js = f"""
// Auto-generated widget settings
// Instance: {instance_id}
window.aiChatSettings = {{
  apiUrl: '{api_url}',
  instanceId: '{instance_id}',
  title: '{config.get("title", "AI ÐÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚")}',
  greeting: '{_escape_js_string(config.get("greeting") or "")}',
  placeholder: '{_escape_js_string(config.get("placeholder") or "")}',
  primaryColor: '{config.get("primary_color", "#6366f1")}',
  position: '{config.get("position", "right")}'
}};

"""
    full_script = settings_js + widget_js

    return Response(
        content=full_script,
        media_type="application/javascript",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Access-Control-Allow-Origin": "*",
        },
    )


# ============== Static Files for Vue Admin ==============

DEV_MODE = os.getenv("DEV_MODE", "").lower() in ("1", "true", "yes")
VITE_DEV_URL = os.getenv("VITE_DEV_URL", "http://localhost:5173")

if DEV_MODE:
    # Dev mode: proxy to Vite dev server for hot reload
    import httpx

    @app.api_route("/admin/{path:path}", methods=["GET", "HEAD"])
    async def proxy_to_vite(path: str, request: Request):
        """Proxy static files to Vite dev server"""
        async with httpx.AsyncClient() as client:
            url = f"{VITE_DEV_URL}/admin/{path}"
            try:
                resp = await client.get(url, headers=dict(request.headers))
                return Response(
                    content=resp.content, status_code=resp.status_code, headers=dict(resp.headers)
                )
            except httpx.ConnectError:
                return Response(
                    content=b"Vite dev server not running. Start with: cd admin && npm run dev",
                    status_code=503,
                )

    @app.get("/admin")
    async def proxy_admin_root():
        """Redirect /admin to /admin/"""
        return RedirectResponse(url="/admin/")

    logger.info(f"ðŸ”§ DEV MODE: Proxying /admin/* to Vite at {VITE_DEV_URL}")
else:
    # Production: serve built Vue app
    admin_dist_path = Path(__file__).parent / "admin" / "dist"
    if admin_dist_path.exists():
        from fastapi.staticfiles import StaticFiles

        app.mount("/admin", StaticFiles(directory=str(admin_dist_path), html=True), name="admin")
        logger.info(f"ðŸ“‚ Vue admin mounted at /admin from {admin_dist_path}")


if __name__ == "__main__":
    from dotenv import load_dotenv

    load_dotenv()
    port = int(os.getenv("ORCHESTRATOR_PORT", 8002))
    logger.info(f"ðŸŽ¯ Ð—Ð°Ð¿ÑƒÑÐº Orchestrator Ð½Ð° Ð¿Ð¾Ñ€Ñ‚Ñƒ {port}")
    uvicorn.run("orchestrator:app", host="0.0.0.0", port=port, reload=False, log_level="info")
