#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä - –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤—Å–µ —Å–µ—Ä–≤–∏—Å—ã
STT (Whisper) -> LLM (Gemini) -> TTS (XTTS v2)
"""

import asyncio
import hashlib
import json
import logging
import os
import re
import threading
import time
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

import numpy as np
import soundfile as sf
import uvicorn
from fastapi import Depends, FastAPI, File, HTTPException, Request, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import (
    FileResponse,
    RedirectResponse,
    Response,
    StreamingResponse,
)
from pydantic import BaseModel
from slowapi import _rate_limit_exceeded_handler
from slowapi.errors import RateLimitExceeded

from app.rate_limiter import limiter

# Modular routers
from app.routers import (
    audit,
    auth,
    bot_sales,
    chat,
    faq,
    github_webhook,
    gsm,
    llm,
    monitor,
    services,
    stt,
    telegram,
    tts,
    usage,
    widget,
    yoomoney_webhook,
)
from app.security_headers import (
    SECURITY_HEADERS_ENABLED,
    SecurityHeadersMiddleware,
)
from auth_manager import (
    LoginRequest,
    LoginResponse,
    User,
    authenticate_user,
    create_access_token,
    get_auth_status,
    get_current_user,
)

# Cloud LLM service for multi-provider support
from cloud_llm_service import PROVIDER_TYPES, CloudLLMService

# Database integration
from db.integration import (
    async_audit_logger,
    async_cloud_provider_manager,
    async_config_manager,
    async_faq_manager,
    async_preset_manager,
    async_widget_instance_manager,
    get_database_status,
    init_database,
    shutdown_database,
)
from finetune_manager import get_finetune_manager
from llm_service import LLMService
from model_manager import get_model_manager

# Multi-bot manager
from piper_tts_service import PiperTTSService
from service_manager import get_service_manager
from stt_service import STTService
from system_monitor import get_system_monitor
from tts_finetune_manager import get_tts_finetune_manager

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
from voice_clone_service import VoiceCloneService


# vLLM –∏–º–ø–æ—Ä—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π - –ª–æ–∫–∞–ª—å–Ω–∞—è Llama —á–µ—Ä–µ–∑ vLLM)
try:
    from vllm_llm_service import VLLMLLMService

    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False
    VLLMLLMService = None

# OpenVoice –∏–º–ø–æ—Ä—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π - –¥–ª—è GPU P104-100)
try:
    from openvoice_service import OpenVoiceService

    OPENVOICE_AVAILABLE = True
except ImportError:
    OPENVOICE_AVAILABLE = False
    OpenVoiceService = None

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–æ–π LLM backend –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
LLM_BACKEND = os.getenv("LLM_BACKEND", "gemini").lower()  # "gemini" –∏–ª–∏ "vllm"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============== Streaming TTS Manager ==============
class StreamingTTSManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ TTS –≤–æ –≤—Ä–µ–º—è streaming LLM.

    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    1. –í–æ –≤—Ä–µ–º—è streaming chat/completions - –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –∏ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
       –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∑–∞–ø—É—Å–∫–∞–µ–º —Å–∏–Ω—Ç–µ–∑ –≤ —Ñ–æ–Ω–æ–≤–æ–º –ø–æ—Ç–æ–∫–µ
    2. –•—Ä–∞–Ω–∏–º —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã –≤ –∫—ç—à–µ –ø–æ —Ö—ç—à—É –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    3. –ü—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ /v1/audio/speech - —Å–∫–ª–µ–∏–≤–∞–µ–º –≥–æ—Ç–æ–≤—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
    """

    def __init__(self, max_cache_size: int = 50, cache_ttl: int = 300):
        self.max_cache_size = max_cache_size
        self.cache_ttl = cache_ttl  # —Å–µ–∫—É–Ω–¥

        # –ö—ç—à: response_hash -> {"segments": [...], "full_audio": np.array, "timestamp": float}
        self._cache: OrderedDict[str, Dict] = OrderedDict()
        self._cache_lock = threading.Lock()

        # –¢–µ–∫—É—â–∏–µ —Å–µ—Å—Å–∏–∏ —Å–∏–Ω—Ç–µ–∑–∞: session_id -> {"text": str, "segments": [...], "futures": [...]}
        self._active_sessions: Dict[str, Dict] = {}
        self._session_lock = threading.Lock()

        # Thread pool –¥–ª—è —Ñ–æ–Ω–æ–≤–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="tts_")

        # –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        self._sentence_pattern = re.compile(r"([^.!?]*[.!?]+)")

        logger.info("üéôÔ∏è StreamingTTSManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _get_text_hash(self, text: str) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö—ç—à —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        normalized = text.strip().lower()
        return hashlib.md5(normalized.encode()).hexdigest()[:16]

    def _clean_old_cache(self):
        """–£–¥–∞–ª—è–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –∫—ç—à–∞"""
        now = time.time()
        with self._cache_lock:
            keys_to_delete = []
            for key, value in self._cache.items():
                if now - value.get("timestamp", 0) > self.cache_ttl:
                    keys_to_delete.append(key)
            for key in keys_to_delete:
                del self._cache[key]
                logger.debug(f"üóëÔ∏è –£–¥–∞–ª—ë–Ω —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à: {key}")

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
            while len(self._cache) > self.max_cache_size:
                self._cache.popitem(last=False)

    def start_session(self, session_id: str) -> None:
        """–ù–∞—á–∏–Ω–∞–µ—Ç –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é streaming —Å–∏–Ω—Ç–µ–∑–∞"""
        with self._session_lock:
            self._active_sessions[session_id] = {
                "text_buffer": "",
                "full_text": "",
                "segments": [],  # [(text, audio_data, sample_rate), ...]
                "pending_futures": [],
                "start_time": time.time(),
            }
        logger.info(f"üé¨ –ù–∞—á–∞—Ç–∞ —Å–µ—Å—Å–∏—è TTS: {session_id}")

    def add_text_chunk(self, session_id: str, chunk: str, voice_service) -> None:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç chunk —Ç–µ–∫—Å—Ç–∞ –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç —Å–∏–Ω—Ç–µ–∑ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ streaming LLM response.
        """
        with self._session_lock:
            if session_id not in self._active_sessions:
                return

            session = self._active_sessions[session_id]
            session["text_buffer"] += chunk
            session["full_text"] += chunk

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            buffer = session["text_buffer"]
            sentences = self._sentence_pattern.findall(buffer)

            if sentences:
                # –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                for sentence in sentences:
                    sentence = sentence.strip()
                    if len(sentence) > 3:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
                        future = self._executor.submit(
                            self._synthesize_segment, sentence, voice_service, session_id
                        )
                        session["pending_futures"].append((sentence, future))
                        logger.info(f"üîÑ –ó–∞–ø—É—â–µ–Ω —Å–∏–Ω—Ç–µ–∑: '{sentence[:40]}...'")

                # –£–¥–∞–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –±—É—Ñ–µ—Ä–∞
                last_sentence = sentences[-1]
                idx = buffer.rfind(last_sentence) + len(last_sentence)
                session["text_buffer"] = buffer[idx:]

    def _synthesize_segment(self, text: str, voice_service, session_id: str) -> tuple:
        """–°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç (–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ thread pool)"""
        try:
            wav, sr = voice_service.synthesize(
                text=text,
                preset="natural",
                preprocess_text=True,
                split_sentences=False,  # –£–∂–µ —Ä–∞–∑–±–∏–ª–∏
            )
            logger.info(f"‚úÖ –°–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω —Å–µ–≥–º–µ–Ω—Ç: '{text[:30]}...'")
            return (text, wav, sr)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Å–µ–≥–º–µ–Ω—Ç–∞: {e}")
            return (text, None, None)

    def finish_session(self, session_id: str, voice_service) -> None:
        """
        –ó–∞–≤–µ—Ä—à–∞–µ—Ç —Å–µ—Å—Å–∏—é: —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –æ—Å—Ç–∞–≤—à–∏–π—Å—è —Ç–µ–∫—Å—Ç –∏ –∫—ç—à–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
        """
        with self._session_lock:
            if session_id not in self._active_sessions:
                return

            session = self._active_sessions[session_id]

            # –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º –æ—Å—Ç–∞—Ç–æ–∫ –±—É—Ñ–µ—Ä–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
            remaining = session["text_buffer"].strip()
            if remaining and len(remaining) > 3:
                future = self._executor.submit(
                    self._synthesize_segment, remaining, voice_service, session_id
                )
                session["pending_futures"].append((remaining, future))
                logger.info(f"üîÑ –ó–∞–ø—É—â–µ–Ω —Å–∏–Ω—Ç–µ–∑ –æ—Å—Ç–∞—Ç–∫–∞: '{remaining[:40]}...'")

            # –ñ–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö futures
            for text, future in session["pending_futures"]:
                try:
                    result = future.result(timeout=60)
                    if result[1] is not None:
                        session["segments"].append(result)
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å–∏–Ω—Ç–µ–∑–∞: {e}")

            # –°–∫–ª–µ–∏–≤–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã
            full_text = session["full_text"]
            if session["segments"]:
                self._cache_full_audio(full_text, session["segments"])

            elapsed = time.time() - session["start_time"]
            logger.info(
                f"‚úÖ –°–µ—Å—Å–∏—è {session_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed:.2f}s, "
                f"—Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(session['segments'])}"
            )

            # –£–¥–∞–ª—è–µ–º —Å–µ—Å—Å–∏—é
            del self._active_sessions[session_id]

    def _cache_full_audio(self, full_text: str, segments: list) -> None:
        """–°–∫–ª–µ–∏–≤–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã –∏ –∫—ç—à–∏—Ä—É–µ—Ç –ø–æ–ª–Ω–æ–µ –∞—É–¥–∏–æ"""
        if not segments:
            return

        # –ü–æ–ª—É—á–∞–µ–º sample rate –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
        sample_rate = segments[0][2]

        # –°–∫–ª–µ–∏–≤–∞–µ–º –∞—É–¥–∏–æ —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –ø–∞—É–∑–∞–º–∏
        pause_samples = int(0.1 * sample_rate)  # 100ms –ø–∞—É–∑–∞
        pause = np.zeros(pause_samples, dtype=np.float32)

        audio_parts = []
        for text, wav, sr in segments:
            if wav is not None:
                if isinstance(wav, list):
                    wav = np.array(wav, dtype=np.float32)
                audio_parts.append(wav)
                audio_parts.append(pause)

        if audio_parts:
            full_audio = np.concatenate(audio_parts[:-1])  # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –ø–∞—É–∑—É

            text_hash = self._get_text_hash(full_text)
            with self._cache_lock:
                self._cache[text_hash] = {
                    "full_audio": full_audio,
                    "sample_rate": sample_rate,
                    "full_text": full_text,
                    "timestamp": time.time(),
                    "segments_count": len(segments),
                }
                logger.info(
                    f"üíæ –ó–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–æ –∞—É–¥–∏–æ: {text_hash} ({len(full_audio) / sample_rate:.2f}s)"
                )

            self._clean_old_cache()

    def get_cached_audio(self, text: str) -> Optional[tuple]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ –¥–ª—è —Ç–µ–∫—Å—Ç–∞.
        Returns: (audio_data, sample_rate) –∏–ª–∏ None
        """
        text_hash = self._get_text_hash(text)

        with self._cache_lock:
            if text_hash in self._cache:
                cached = self._cache[text_hash]
                logger.info(f"‚ö° Cache HIT: {text_hash}")
                return (cached["full_audio"], cached["sample_rate"])

        logger.info(f"‚ùå Cache MISS: {text_hash}")
        return None

    def get_stats(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–µ–Ω–µ–¥–∂–µ—Ä–∞"""
        with self._cache_lock:
            cache_size = len(self._cache)
        with self._session_lock:
            active_sessions = len(self._active_sessions)

        return {
            "cache_size": cache_size,
            "active_sessions": active_sessions,
            "max_cache_size": self.max_cache_size,
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä streaming TTS
streaming_tts_manager: Optional[StreamingTTSManager] = None

app = FastAPI(title="AI Secretary Orchestrator", version="1.0.0")

# Rate limiting
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# CORS –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞
# Read allowed origins from env (comma-separated), default to "*" for development
CORS_ORIGINS_RAW = os.getenv("CORS_ORIGINS", "*")
CORS_ORIGINS = (
    ["*"]
    if CORS_ORIGINS_RAW == "*"
    else [origin.strip() for origin in CORS_ORIGINS_RAW.split(",") if origin.strip()]
)
app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Security headers
app.add_middleware(SecurityHeadersMiddleware, enabled=SECURITY_HEADERS_ENABLED)

# Include modular routers
# NOTE: These routers use the ServiceContainer from app.dependencies
# which is populated in startup_event
app.include_router(auth.router)
app.include_router(audit.router)
app.include_router(services.router)
app.include_router(monitor.router)
app.include_router(faq.router)
app.include_router(stt.router)
app.include_router(llm.router)
app.include_router(tts.router)
app.include_router(chat.router)
app.include_router(telegram.router)
app.include_router(usage.router)
app.include_router(widget.router)
app.include_router(gsm.router)
app.include_router(bot_sales.router)
app.include_router(github_webhook.router)
app.include_router(yoomoney_webhook.router)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã
voice_service: Optional[VoiceCloneService] = None  # XTTS (–õ–∏–¥–∏—è) - GPU CC >= 7.0
gulya_voice_service: Optional[VoiceCloneService] = None  # XTTS (–ì—É–ª—è) - GPU CC >= 7.0
piper_service: Optional[PiperTTSService] = None  # Piper (Dmitri, Irina) - CPU
openvoice_service: Optional["OpenVoiceService"] = None  # OpenVoice v2 (–õ–∏–¥–∏—è) - GPU CC 6.1+
stt_service: Optional[STTService] = None
llm_service: Optional[LLMService] = None

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–µ–∫—É—â–µ–≥–æ –≥–æ–ª–æ—Å–∞
# engine: "xtts" (–õ–∏–¥–∏—è/–ì—É–ª—è –Ω–∞ GPU CC>=7.0), "piper" (Dmitri/Irina –Ω–∞ CPU), "openvoice" (–õ–∏–¥–∏—è –Ω–∞ GPU CC 6.1+)
# –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º –ì—É–ª—é (XTTS) –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞, –∏–Ω–∞—á–µ Piper
current_voice_config = {
    "engine": "xtts",
    "voice": "gulya",  # gulya / lidia / dmitri / irina / lidia_openvoice
}

# –ü–∞–ø–∫–∞ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
TEMP_DIR = Path("./temp")
TEMP_DIR.mkdir(exist_ok=True)

# –ü–∞–ø–∫–∞ –¥–ª—è –ª–æ–≥–æ–≤ –∑–≤–æ–Ω–∫–æ–≤
CALLS_LOG_DIR = Path("./calls_log")
CALLS_LOG_DIR.mkdir(exist_ok=True)


# Helper functions for loading data from database at startup
async def _reload_llm_faq():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç FAQ –∏–∑ –ë–î –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç LLM —Å–µ—Ä–≤–∏—Å."""
    if llm_service and hasattr(llm_service, "reload_faq"):
        faq_dict = await async_faq_manager.get_all()
        llm_service.reload_faq(faq_dict)


async def _reload_voice_presets():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ—Å–µ—Ç—ã –∏–∑ –ë–î –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç voice —Å–µ—Ä–≤–∏—Å—ã."""
    presets_dict = await async_preset_manager.get_custom()
    for svc in [voice_service, gulya_voice_service]:
        if svc and hasattr(svc, "reload_presets"):
            svc.reload_presets(presets_dict)


async def _auto_start_telegram_bots():
    """Auto-start Telegram bots that have auto_start=True."""
    from db.integration import async_bot_instance_manager
    from multi_bot_manager import multi_bot_manager

    try:
        instances = await async_bot_instance_manager.get_auto_start_instances()
        if not instances:
            logger.info("üì± No Telegram bots configured for auto-start")
            return

        started = 0
        for instance in instances:
            instance_id = instance["id"]
            try:
                result = await multi_bot_manager.start_bot(instance_id)
                if result.get("status") in ["started", "already_running"]:
                    started += 1
                    logger.info(f"üì± Auto-started Telegram bot: {instance['name']}")
                else:
                    logger.warning(f"üì± Failed to auto-start bot {instance_id}: {result}")
            except Exception as e:
                logger.error(f"üì± Error auto-starting bot {instance_id}: {e}")

        if started > 0:
            logger.info(f"üì± Auto-started {started}/{len(instances)} Telegram bots")
    except Exception as e:
        logger.error(f"üì± Error during Telegram bot auto-start: {e}")


class ConversationRequest(BaseModel):
    text: str
    session_id: Optional[str] = None


class TTSRequest(BaseModel):
    text: str
    language: str = "ru"


class OpenAISpeechRequest(BaseModel):
    """OpenAI-compatible TTS request for OpenWebUI integration"""

    model: str = "lidia-voice"
    input: str
    voice: str = "lidia"
    response_format: str = "wav"
    speed: float = 1.0


class ChatMessage(BaseModel):
    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    """OpenAI-compatible chat completion request"""

    model: str = "gulya-secretary-qwen"  # Format: {persona}-secretary-{backend}
    messages: List[ChatMessage]
    stream: bool = False
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None


@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ"""
    global \
        voice_service, \
        gulya_voice_service, \
        piper_service, \
        openvoice_service, \
        stt_service, \
        llm_service, \
        streaming_tts_manager

    logger.info("üöÄ –ó–∞–ø—É—Å–∫ AI Secretary Orchestrator")

    # Initialize database first
    await init_database()

    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Piper TTS (Dmitri, Irina) - CPU, –∑–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–≤—ã–º
        logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ Piper TTS Service (CPU)...")
        try:
            piper_service = PiperTTSService()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Piper TTS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            piper_service = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenVoice v2 (–õ–∏–¥–∏—è) - GPU CC 6.1+ (P104-100)
        if OPENVOICE_AVAILABLE:
            logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ OpenVoice TTS Service (GPU CC 6.1+)...")
            try:
                openvoice_service = OpenVoiceService()
                logger.info("‚úÖ OpenVoice v2 –∑–∞–≥—Ä—É–∂–µ–Ω (P104-100)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è OpenVoice –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                openvoice_service = None
        else:
            logger.info("‚è≠Ô∏è OpenVoice –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º)")
            openvoice_service = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è XTTS (–ì—É–ª—è) - GPU CC >= 7.0, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ Voice Clone Service (XTTS - –ì—É–ª—è)...")
        try:
            gulya_voice_service = VoiceCloneService(voice_samples_dir="./–ì—É–ª—è")
            logger.info(
                f"‚úÖ XTTS (–ì—É–ª—è) –∑–∞–≥—Ä—É–∂–µ–Ω: {len(gulya_voice_service.voice_samples)} –æ–±—Ä–∞–∑—Ü–æ–≤"
            )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è XTTS (–ì—É–ª—è) –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            gulya_voice_service = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è XTTS (–õ–∏–¥–∏—è) - GPU CC >= 7.0, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
        logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ Voice Clone Service (XTTS - –õ–∏–¥–∏—è)...")
        try:
            voice_service = VoiceCloneService(voice_samples_dir="./–õ–∏–¥–∏—è")
            logger.info(f"‚úÖ XTTS (–õ–∏–¥–∏—è) –∑–∞–≥—Ä—É–∂–µ–Ω: {len(voice_service.voice_samples)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è XTTS (–õ–∏–¥–∏—è) –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (—Ç—Ä–µ–±—É–µ—Ç—Å—è GPU CC >= 7.0): {e}")
            voice_service = None

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≥–æ–ª–æ—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        global current_voice_config
        if gulya_voice_service:
            current_voice_config = {"engine": "xtts", "voice": "gulya"}
            logger.info("üé§ –ì–æ–ª–æ—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –ì—É–ª—è (XTTS)")
        elif voice_service:
            current_voice_config = {"engine": "xtts", "voice": "lidia"}
            logger.info("üé§ –ì–æ–ª–æ—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –õ–∏–¥–∏—è (XTTS)")
        elif piper_service:
            current_voice_config = {"engine": "piper", "voice": "dmitri"}
            logger.info("üé§ –ì–æ–ª–æ—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –î–º–∏—Ç—Ä–∏–π (Piper)")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM Service (vLLM –∏–ª–∏ Gemini)
        if LLM_BACKEND == "vllm" and VLLM_AVAILABLE:
            logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ vLLM LLM Service (Llama-3.1-8B)...")
            try:
                llm_service = VLLMLLMService()
                if llm_service.is_available():
                    logger.info("‚úÖ vLLM –ø–æ–¥–∫–ª—é—á–µ–Ω")
                else:
                    logger.warning("‚ö†Ô∏è vLLM –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç, –ø—Ä–æ–±—É–µ–º Gemini...")
                    llm_service = LLMService()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è vLLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({e}), –∏—Å–ø–æ–ª—å–∑—É–µ–º Gemini")
                llm_service = LLMService()
        else:
            logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ Gemini LLM Service...")
            llm_service = LLMService()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Streaming TTS Manager
        logger.info("üì¶ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Streaming TTS Manager...")
        streaming_tts_manager = StreamingTTSManager(max_cache_size=50, cache_ttl=300)

        # STT –æ—Ç–∫–ª—é—á—ë–Ω –≤—Ä–µ–º–µ–Ω–Ω–æ - –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞—Ç–∞ –Ω–µ –Ω—É–∂–µ–Ω
        # –ú–æ–¥–µ–ª—å faster-whisper –∑–∞–≤–∏—Å–∞–µ—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
        logger.info("‚è≠Ô∏è STT –æ—Ç–∫–ª—é—á—ë–Ω (–¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞—Ç–∞ –Ω–µ –Ω—É–∂–µ–Ω)")
        stt_service = None

        # Load FAQ and presets from database into services
        logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ FAQ –∏ –ø—Ä–µ—Å–µ—Ç–æ–≤ –∏–∑ –ë–î...")
        try:
            await _reload_llm_faq()
            await _reload_voice_presets()
            logger.info("‚úÖ FAQ –∏ –ø—Ä–µ—Å–µ—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –ë–î")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î: {e}")

        # Check for deprecated legacy JSON files
        legacy_files = [
            ("typical_responses.json", "FAQ"),
            ("custom_presets.json", "TTS presets"),
            ("chat_sessions.json", "chat sessions"),
            ("widget_config.json", "widget config"),
            ("telegram_config.json", "telegram config"),
        ]
        found_legacy = []
        for filename, description in legacy_files:
            if Path(filename).exists():
                found_legacy.append(f"{filename} ({description})")

        if found_legacy:
            logger.warning("=" * 60)
            logger.warning("‚ö†Ô∏è  DEPRECATED: –ù–∞–π–¥–µ–Ω—ã legacy JSON —Ñ–∞–π–ª—ã:")
            for f in found_legacy:
                logger.warning(f"    ‚Ä¢ {f}")
            logger.warning("    –î–∞–Ω–Ω—ã–µ —Ç–µ–ø–µ—Ä—å —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ SQLite (data/secretary.db).")
            logger.warning("    Legacy —Ñ–∞–π–ª—ã –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –ø–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–∏–≥—Ä–∞—Ü–∏–∏:")
            logger.warning("    python scripts/migrate_json_to_db.py")
            logger.warning("=" * 60)

        # Populate service container for modular routers
        from app.dependencies import get_container

        container = get_container()
        container.voice_service = voice_service
        container.gulya_voice_service = gulya_voice_service
        container.piper_service = piper_service
        container.openvoice_service = openvoice_service
        container.stt_service = stt_service
        container.llm_service = llm_service
        container.streaming_tts_manager = streaming_tts_manager
        container.current_voice_config = current_voice_config
        logger.info("‚úÖ Service container populated for modular routers")

        # Auto-start Telegram bots that were running before restart
        await _auto_start_telegram_bots()

        logger.info("‚úÖ –û—Å–Ω–æ–≤–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        raise


@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    logger.info("üõë Shutting down AI Secretary Orchestrator")
    await shutdown_database()
    logger.info("‚úÖ Shutdown complete")


@app.get("/")
async def root():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏"""
    return {
        "status": "ok",
        "service": "AI Secretary Orchestrator",
        "endpoints": {
            "health": "/health",
            "process_call": "/process_call (POST)",
            "tts": "/tts (POST)",
            "stt": "/stt (POST)",
            "chat": "/chat (POST)",
        },
    }


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤"""
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º llm_service –∏–∑ container (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±–Ω–æ–≤–ª—ë–Ω —á–µ—Ä–µ–∑ router)
    from app.dependencies import get_container

    container = get_container()
    current_llm_service = container.llm_service if container.llm_service else llm_service

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø LLM —Å–µ—Ä–≤–∏—Å–∞
    llm_backend_type = "unknown"
    if current_llm_service:
        if hasattr(current_llm_service, "api_url"):  # vLLM
            llm_backend_type = f"vllm ({current_llm_service.model_name})"
        elif hasattr(current_llm_service, "model_name"):  # Gemini
            llm_backend_type = f"gemini ({current_llm_service.model_name})"

    services_status = {
        "voice_clone_xtts_gulya": gulya_voice_service is not None,
        "voice_clone_xtts_lidia": voice_service is not None,
        "voice_clone_openvoice": openvoice_service is not None,
        "piper_tts": piper_service is not None,
        "stt": stt_service is not None,
        "llm": current_llm_service is not None,
        "llm_backend": llm_backend_type,
        "streaming_tts": streaming_tts_manager is not None,
        "current_voice": current_voice_config,
    }

    # –î–ª—è health check –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª—é–±–æ–π TTS + llm
    any_tts = (
        services_status["voice_clone_xtts_gulya"]
        or services_status["voice_clone_xtts_lidia"]
        or services_status["voice_clone_openvoice"]
        or services_status["piper_tts"]
    )
    core_ok = any_tts and services_status["llm"]

    # Get database status
    db_status = await get_database_status()

    result = {
        "status": "healthy" if core_ok else "degraded",
        "services": services_status,
        "database": db_status.get("database", {}),
        "timestamp": datetime.now().isoformat(),
    }

    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É streaming TTS –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
    if streaming_tts_manager is not None:
        result["streaming_tts_stats"] = streaming_tts_manager.get_stats()

    return result


def synthesize_with_current_voice(text: str, output_path: str, language: str = "ru"):
    """
    –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç —Ä–µ—á—å —Å —Ç–µ–∫—É—â–∏–º –≤—ã–±—Ä–∞–Ω–Ω—ã–º –≥–æ–ª–æ—Å–æ–º.
    –£—á–∏—Ç—ã–≤–∞–µ—Ç current_voice_config.

    Engines:
    - piper: CPU, –±—ã—Å—Ç—Ä—ã–π, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≥–æ–ª–æ—Å–∞ (dmitri, irina)
    - openvoice: GPU CC 6.1+, –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–∞ (lidia_openvoice)
    - xtts: GPU CC >= 7.0, –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (gulya, lidia)
    """
    engine = current_voice_config["engine"]
    voice = current_voice_config["voice"]

    if engine == "piper" and piper_service:
        logger.info(f"üéôÔ∏è Piper —Å–∏–Ω—Ç–µ–∑ ({voice}): '{text[:40]}...'")
        piper_service.synthesize_to_file(text, output_path, voice=voice)
    elif engine == "openvoice" and openvoice_service:
        logger.info(f"üéôÔ∏è OpenVoice —Å–∏–Ω—Ç–µ–∑ (–õ–∏–¥–∏—è): '{text[:40]}...'")
        openvoice_service.synthesize_to_file(text, output_path, language=language)
    elif engine == "xtts" and voice == "gulya" and gulya_voice_service:
        logger.info(f"üéôÔ∏è XTTS —Å–∏–Ω—Ç–µ–∑ (–ì—É–ª—è): '{text[:40]}...'")
        gulya_voice_service.synthesize_to_file(text, output_path, language=language)
    elif engine == "xtts" and voice == "lidia" and voice_service:
        logger.info(f"üéôÔ∏è XTTS —Å–∏–Ω—Ç–µ–∑ (–õ–∏–¥–∏—è): '{text[:40]}...'")
        voice_service.synthesize_to_file(text, output_path, language=language)
    elif gulya_voice_service:
        # Fallback to –ì—É–ª—è if available (default)
        logger.info(f"üéôÔ∏è XTTS —Å–∏–Ω—Ç–µ–∑ (–ì—É–ª—è fallback): '{text[:40]}...'")
        gulya_voice_service.synthesize_to_file(text, output_path, language=language)
    elif voice_service:
        # Fallback to –õ–∏–¥–∏—è if available
        logger.info(f"üéôÔ∏è XTTS —Å–∏–Ω—Ç–µ–∑ (–õ–∏–¥–∏—è fallback): '{text[:40]}...'")
        voice_service.synthesize_to_file(text, output_path, language=language)
    elif openvoice_service:
        # Fallback to OpenVoice if XTTS not available
        logger.info(f"üéôÔ∏è OpenVoice —Å–∏–Ω—Ç–µ–∑ (fallback): '{text[:40]}...'")
        openvoice_service.synthesize_to_file(text, output_path, language=language)
    elif piper_service:
        # Fallback to Piper
        logger.info(f"üéôÔ∏è Piper —Å–∏–Ω—Ç–µ–∑ (fallback): '{text[:40]}...'")
        piper_service.synthesize_to_file(text, output_path, voice="irina")
    else:
        raise RuntimeError("No TTS service available")


@app.post("/tts")
async def text_to_speech(request: TTSRequest):
    """
    –°–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏ —Å —Ç–µ–∫—É—â–∏–º –≤—ã–±—Ä–∞–Ω–Ω—ã–º –≥–æ–ª–æ—Å–æ–º
    """
    if not voice_service and not piper_service:
        raise HTTPException(status_code=503, detail="No TTS service initialized")

    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        output_file = TEMP_DIR / f"tts_{datetime.now().timestamp()}.wav"

        # –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º —Å —Ç–µ–∫—É—â–∏–º –≥–æ–ª–æ—Å–æ–º
        synthesize_with_current_voice(
            text=request.text, output_path=str(output_file), language=request.language
        )

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∞–π–ª
        return FileResponse(path=output_file, media_type="audio/wav", filename="response.wav")

    except Exception as e:
        logger.error(f"‚ùå TTS Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/stt")
async def speech_to_text(audio: UploadFile = File(...)):
    """
    –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∏–∑ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞
    """
    if not stt_service:
        raise HTTPException(status_code=503, detail="STT service not initialized")

    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        temp_audio = TEMP_DIR / f"stt_{datetime.now().timestamp()}_{audio.filename}"

        with open(temp_audio, "wb") as f:
            content = await audio.read()
            f.write(content)

        # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º
        result = stt_service.transcribe(temp_audio, language="ru")

        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        temp_audio.unlink()

        return {
            "text": result["text"],
            "language": result["language"],
            "segments_count": len(result.get("segments", [])),
        }

    except Exception as e:
        logger.error(f"‚ùå STT Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat")
async def chat(request: ConversationRequest):
    """
    –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç LLM (Gemini)
    """
    if not llm_service:
        raise HTTPException(status_code=503, detail="LLM service not initialized")

    try:
        response = llm_service.generate_response(request.text)

        return {"response": response, "session_id": request.session_id}

    except Exception as e:
        logger.error(f"‚ùå LLM Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/process_call")
async def process_call(audio: UploadFile = File(...)):
    """
    –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–≤–æ–Ω–∫–∞:
    1. STT - —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
    2. LLM - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    3. TTS - —Å–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞—É–¥–∏–æ —Å –æ—Ç–≤–µ—Ç–æ–º —Å–µ–∫—Ä–µ—Ç–∞—Ä—è
    """
    call_id = f"call_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    logger.info(f"üìû –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–≤–æ–Ω–∫–∞ {call_id}")

    try:
        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ö–æ–¥—è—â–∏–π –∞—É–¥–∏–æ
        input_audio = CALLS_LOG_DIR / f"{call_id}_input.wav"
        with open(input_audio, "wb") as f:
            content = await audio.read()
            f.write(content)

        # 2. –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Ä–µ—á—å (STT)
        logger.info(f"üéß STT –¥–ª—è {call_id}")
        stt_result = stt_service.transcribe(input_audio, language="ru")
        recognized_text = stt_result["text"]
        logger.info(f"üìù –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {recognized_text}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
        with open(CALLS_LOG_DIR / f"{call_id}_transcript.txt", "w") as f:
            f.write(f"USER: {recognized_text}\n")

        # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç (LLM)
        logger.info(f"ü§ñ LLM –¥–ª—è {call_id}")
        llm_response = llm_service.generate_response(recognized_text)
        logger.info(f"üí¨ –û—Ç–≤–µ—Ç: {llm_response}")

        # –î–æ–ø–æ–ª–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
        with open(CALLS_LOG_DIR / f"{call_id}_transcript.txt", "a") as f:
            f.write(f"ASSISTANT: {llm_response}\n")

        # 4. –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç (TTS)
        logger.info(f"üéôÔ∏è  TTS –¥–ª—è {call_id}")
        output_audio = CALLS_LOG_DIR / f"{call_id}_output.wav"
        voice_service.synthesize_to_file(
            text=llm_response, output_path=str(output_audio), language="ru"
        )

        logger.info(f"‚úÖ –ó–≤–æ–Ω–æ–∫ {call_id} –æ–±—Ä–∞–±–æ—Ç–∞–Ω")

        # 5. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∞—É–¥–∏–æ –æ—Ç–≤–µ—Ç
        return FileResponse(
            path=output_audio,
            media_type="audio/wav",
            filename=f"{call_id}_response.wav",
            headers={
                "X-Call-ID": call_id,
                "X-Recognized-Text": recognized_text,
                "X-Response-Text": llm_response,
            },
        )

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–≤–æ–Ω–∫–∞ {call_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/reset_conversation")
async def reset_conversation():
    """–°–±—Ä–æ—Å –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞"""
    if llm_service:
        llm_service.reset_conversation()
        return {"status": "ok", "message": "Conversation history reset"}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== OpenAI-Compatible Endpoints for OpenWebUI ==============


@app.get("/v1/models")
@app.get("/v1/models/")
async def list_models():
    """OpenAI-compatible models list for OpenWebUI"""
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º backend –∏ —Å—É—Ñ—Ñ–∏–∫—Å –¥–ª—è –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏
    if llm_service and hasattr(llm_service, "api_url"):
        # vLLM backend - –ø—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å
        model_name = getattr(llm_service, "model_name", "unknown")
        if model_name == "lydia" or "qwen" in model_name.lower():
            backend_suffix = "qwen"
            backend_desc = "Qwen2.5-7B + LoRA"
        elif "llama" in model_name.lower():
            backend_suffix = "llama"
            backend_desc = "Llama-3.1-8B"
        else:
            backend_suffix = "vllm"
            backend_desc = model_name
    else:
        backend_suffix = "gemini"
        backend_desc = "Gemini"

    return {
        "object": "list",
        "data": [
            {
                "id": f"gulya-secretary-{backend_suffix}",
                "object": "model",
                "created": 1700000000,
                "owned_by": "ai-secretary",
                "permission": [],
                "root": f"gulya-secretary-{backend_suffix}",
                "parent": None,
                "description": f"–ì—É–ª—è - —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å–µ–∫—Ä–µ—Ç–∞—Ä—å ({backend_desc})",
            },
            {
                "id": f"lidia-secretary-{backend_suffix}",
                "object": "model",
                "created": 1700000000,
                "owned_by": "ai-secretary",
                "permission": [],
                "root": f"lidia-secretary-{backend_suffix}",
                "parent": None,
                "description": f"–õ–∏–¥–∏—è - —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å–µ–∫—Ä–µ—Ç–∞—Ä—å ({backend_desc})",
            },
        ],
    }


@app.get("/v1/voices")
async def list_voices():
    """List available voices"""
    voices = []
    if gulya_voice_service:
        voices.append({"voice_id": "gulya", "name": "–ì—É–ª—è", "language": "ru"})
    if voice_service:
        voices.append({"voice_id": "lidia", "name": "–õ–∏–¥–∏—è", "language": "ru"})
    if piper_service:
        voices.append({"voice_id": "dmitri", "name": "–î–º–∏—Ç—Ä–∏–π", "language": "ru"})
        voices.append({"voice_id": "irina", "name": "–ò—Ä–∏–Ω–∞", "language": "ru"})
    return {"voices": voices}


@app.post("/v1/audio/speech")
async def openai_speech(request: OpenAISpeechRequest):
    """
    OpenAI-compatible TTS endpoint for OpenWebUI integration
    POST /v1/audio/speech

    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫—ç—à streaming TTS manager.
    –ï—Å–ª–∏ –∞—É–¥–∏–æ —É–∂–µ –±—ã–ª–æ –ø—Ä–µ–¥—Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–æ –≤–æ –≤—Ä–µ–º—è streaming LLM - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–≥–Ω–æ–≤–µ–Ω–Ω–æ.
    """
    if not voice_service and not piper_service:
        raise HTTPException(status_code=503, detail="No TTS service initialized")

    try:
        output_file = TEMP_DIR / f"speech_{datetime.now().timestamp()}.wav"
        start_time = time.time()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à streaming TTS (—Ç–æ–ª—å–∫–æ –¥–ª—è XTTS)
        cached_audio = None
        if current_voice_config["engine"] == "xtts" and streaming_tts_manager is not None:
            cached_audio = streaming_tts_manager.get_cached_audio(request.input)

        if cached_audio is not None:
            # Cache HIT - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥—Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ
            audio_data, sample_rate = cached_audio
            sf.write(str(output_file), audio_data, sample_rate)
            elapsed = time.time() - start_time
            logger.info(f"‚ö° TTS –∏–∑ –∫—ç—à–∞ –∑–∞ {elapsed:.3f}s (vs ~5-10s –æ–±—ã—á–Ω—ã–π —Å–∏–Ω—Ç–µ–∑)")
        else:
            # Cache MISS - —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º —Å —Ç–µ–∫—É—â–∏–º –≥–æ–ª–æ—Å–æ–º
            synthesize_with_current_voice(
                text=request.input, output_path=str(output_file), language="ru"
            )
            elapsed = time.time() - start_time
            logger.info(f"üéôÔ∏è TTS —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω –∑–∞ {elapsed:.2f}s")

        return FileResponse(path=output_file, media_type="audio/wav", filename="speech.wav")

    except Exception as e:
        logger.error(f"‚ùå OpenAI TTS Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """
    OpenAI-compatible chat completions endpoint for OpenWebUI
    Supports both streaming and non-streaming responses.
    –ü—Ä–∏ streaming - –∑–∞–ø—É—Å–∫–∞–µ—Ç —Ñ–æ–Ω–æ–≤—ã–π —Å–∏–Ω—Ç–µ–∑ TTS –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º.
    """
    if not llm_service:
        raise HTTPException(status_code=503, detail="LLM service not initialized")

    logger.info(
        f"üí¨ Chat completions request: stream={request.stream}, messages={len(request.messages)}"
    )

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Pydantic –º–æ–¥–µ–ª–∏ –≤ dict
    messages = [{"role": m.role, "content": m.content} for m in request.messages]

    if request.stream:
        # Streaming response (SSE) —Å —Ñ–æ–Ω–æ–≤—ã–º —Å–∏–Ω—Ç–µ–∑–æ–º TTS
        async def generate_stream():
            created = int(time.time())
            chunk_id = f"chatcmpl-{created}"
            session_id = f"tts-{created}"

            # –ù–∞—á–∏–Ω–∞–µ–º —Å–µ—Å—Å–∏—é streaming TTS –µ—Å–ª–∏ —Å–µ—Ä–≤–∏—Å—ã –¥–æ—Å—Ç—É–ø–Ω—ã
            use_streaming_tts = streaming_tts_manager is not None and voice_service is not None

            if use_streaming_tts:
                streaming_tts_manager.start_session(session_id)
                logger.info(f"üé¨ Streaming TTS –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Å–µ—Å—Å–∏–∏ {session_id}")

            try:
                for text_chunk in llm_service.generate_response_from_messages(
                    messages, stream=True
                ):
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º chunk –∫–ª–∏–µ–Ω—Ç—É
                    chunk_data = {
                        "id": chunk_id,
                        "object": "chat.completion.chunk",
                        "created": created,
                        "model": request.model,
                        "choices": [
                            {"index": 0, "delta": {"content": text_chunk}, "finish_reason": None}
                        ],
                    }
                    yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"

                    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º chunk –≤ streaming TTS manager
                    if use_streaming_tts and text_chunk:
                        streaming_tts_manager.add_text_chunk(session_id, text_chunk, voice_service)

                # Final chunk
                final_chunk = {
                    "id": chunk_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": request.model,
                    "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                yield "data: [DONE]\n\n"

                # –ó–∞–≤–µ—Ä—à–∞–µ–º —Å–µ—Å—Å–∏—é TTS (—Å–∫–ª–µ–∏–≤–∞–µ—Ç –∏ –∫—ç—à–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ)
                if use_streaming_tts:
                    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å response
                    threading.Thread(
                        target=streaming_tts_manager.finish_session,
                        args=(session_id, voice_service),
                        daemon=True,
                    ).start()

            except Exception as e:
                logger.error(f"‚ùå Streaming error: {e}")
                error_chunk = {"error": {"message": str(e), "type": "server_error"}}
                yield f"data: {json.dumps(error_chunk)}\n\n"

        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            },
        )
    else:
        # Non-streaming response
        try:
            response_text = llm_service.generate_response_from_messages(messages, stream=False)

            # Consume generator if it returns one
            if hasattr(response_text, "__iter__") and not isinstance(response_text, str):
                response_text = "".join(response_text)

            return {
                "id": f"chatcmpl-{int(time.time())}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": request.model,
                "choices": [
                    {
                        "index": 0,
                        "message": {"role": "assistant", "content": response_text},
                        "finish_reason": "stop",
                    }
                ],
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
            }
        except Exception as e:
            logger.error(f"‚ùå Chat completions error: {e}")
            raise HTTPException(status_code=500, detail=str(e))


# ============== Admin Web Interface ==============
# Vue 3 admin panel served from /admin (see Static Files section below)


# ============== Admin API Endpoints ==============

# ============== Auth Endpoints ==============


@app.post("/admin/auth/login", response_model=LoginResponse)
async def admin_login(request: LoginRequest):
    """
    Authenticate user and return JWT token.

    Default credentials: admin / admin
    Set ADMIN_USERNAME and ADMIN_PASSWORD_HASH env vars for production.
    """
    user = authenticate_user(request.username, request.password)
    if not user:
        raise HTTPException(status_code=401, detail="Invalid username or password")

    token, expires_in = create_access_token(user.username, user.role)

    # Audit log
    await async_audit_logger.log(
        action="login", resource="auth", user_id=user.username, details={"role": user.role}
    )

    return LoginResponse(access_token=token, token_type="bearer", expires_in=expires_in)


@app.get("/admin/auth/me")
async def admin_get_current_user(user: User = Depends(get_current_user)):
    """Get current authenticated user info"""
    return {"username": user.username, "role": user.role}


@app.get("/admin/auth/status")
async def admin_auth_status():
    """Get authentication configuration status"""
    return get_auth_status()


# ============== Admin Models ==============


class AdminTTSPresetRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ—Å–µ—Ç–∞ TTS"""

    preset: str  # warm, calm, energetic, natural, neutral


class AdminLLMPromptRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""

    prompt: str


class AdminLLMModelRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ LLM"""

    model: str  # gemini-2.5-flash, gemini-2.5-pro


class AdminTTSTestRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã–π —Å–∏–Ω—Ç–µ–∑"""

    text: str
    preset: str = "natural"


@app.get("/admin/status")
async def admin_status():
    """–ü–æ–ª–Ω—ã–π —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –∞–¥–º–∏–Ω–∫–∏"""
    import torch

    status = {
        "orchestrator": "running",
        "services": {
            "voice_clone": voice_service is not None,
            "llm": llm_service is not None,
            "stt": stt_service is not None,
            "streaming_tts": streaming_tts_manager is not None,
            "piper_tts": piper_service is not None,
        },
        "gpu": None,
        "streaming_tts_stats": None,
        "llm_config": None,
        "tts_config": None,
    }

    # GPU –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    if torch.cuda.is_available():
        gpu_info = []
        for i in range(torch.cuda.device_count()):
            try:
                name = torch.cuda.get_device_name(i)
                total = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                allocated = torch.cuda.memory_allocated(i) / (1024**3)
                gpu_info.append(
                    {
                        "id": i,
                        "name": name,
                        "total_gb": round(total, 2),
                        "used_gb": round(allocated, 2),
                    }
                )
            except Exception:
                pass
        status["gpu"] = gpu_info

    # Streaming TTS —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if streaming_tts_manager:
        status["streaming_tts_stats"] = streaming_tts_manager.get_stats()

    # LLM –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    if llm_service:
        if hasattr(llm_service, "get_config"):
            status["llm_config"] = llm_service.get_config()
        else:
            # –î–ª—è vLLM –∏ –¥—Ä—É–≥–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –±–µ–∑ get_config
            status["llm_config"] = {
                "model_name": getattr(llm_service, "model_name", "unknown"),
                "api_url": getattr(llm_service, "api_url", None),
                "backend": "vllm" if hasattr(llm_service, "api_url") else "gemini",
            }

    # TTS –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    xtts_svc = gulya_voice_service or voice_service
    if xtts_svc:
        status["tts_config"] = {
            "device": xtts_svc.device,
            "default_preset": xtts_svc.default_preset,
            "samples_count": len(xtts_svc.voice_samples),
            "cache_dir": str(xtts_svc.cache_dir),
        }

    return status


@app.get("/admin/llm/prompt")
async def admin_get_llm_prompt():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç LLM"""
    if llm_service:
        persona = getattr(llm_service, "current_persona", None) or os.getenv(
            "SECRETARY_PERSONA", "gulya"
        )
        return {
            "prompt": llm_service.system_prompt,
            "model": llm_service.model_name,
            "persona": persona,
        }
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.post("/admin/llm/prompt")
async def admin_set_llm_prompt(request: AdminLLMPromptRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–æ–≤—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç LLM"""
    if llm_service:
        llm_service.set_system_prompt(request.prompt)
        return {
            "status": "ok",
            "prompt": request.prompt[:100] + "..." if len(request.prompt) > 100 else request.prompt,
        }
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.get("/admin/llm/model")
async def admin_get_llm_model():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å LLM"""
    if llm_service:
        return {"model": llm_service.model_name}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.post("/admin/llm/model")
async def admin_set_llm_model(request: AdminLLMModelRequest):
    """–ò–∑–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å LLM"""
    allowed_models = ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash"]

    if request.model not in allowed_models:
        raise HTTPException(
            status_code=400,
            detail=f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {request.model}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {allowed_models}",
        )

    if llm_service:
        try:
            llm_service.set_model(request.model)
            return {"status": "ok", "model": request.model}
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.delete("/admin/llm/history")
async def admin_clear_llm_history():
    """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ LLM"""
    if llm_service:
        count = len(llm_service.conversation_history)
        llm_service.reset_conversation()
        return {"status": "ok", "cleared_messages": count}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.get("/admin/llm/history")
async def admin_get_llm_history():
    """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ LLM"""
    if llm_service:
        return {
            "history": llm_service.conversation_history,
            "count": len(llm_service.conversation_history),
        }
    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== Voice Selection API ==============


class AdminVoiceRequest(BaseModel):
    voice: str  # gulya / lidia / dmitri / irina


@app.get("/admin/voices")
async def admin_get_voices():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤"""
    voices = []

    # XTTS –≥–æ–ª–æ—Å (–ì—É–ª—è) - —Ç—Ä–µ–±—É–µ—Ç GPU CC >= 7.0 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    if gulya_voice_service:
        voices.append(
            {
                "id": "gulya",
                "name": "–ì—É–ª—è (XTTS)",
                "engine": "xtts",
                "description": "–ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥–æ–ª–æ—Å –ì—É–ª–∏ (XTTS v2, GPU CC >= 7.0)",
                "available": True,
                "samples_count": len(gulya_voice_service.voice_samples),
                "default": True,
            }
        )

    # XTTS –≥–æ–ª–æ—Å (–õ–∏–¥–∏—è) - —Ç—Ä–µ–±—É–µ—Ç GPU CC >= 7.0
    if voice_service:
        voices.append(
            {
                "id": "lidia",
                "name": "–õ–∏–¥–∏—è (XTTS)",
                "engine": "xtts",
                "description": "–ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥–æ–ª–æ—Å –õ–∏–¥–∏–∏ (XTTS v2, GPU CC >= 7.0)",
                "available": True,
                "samples_count": len(voice_service.voice_samples),
            }
        )

    # OpenVoice –≥–æ–ª–æ—Å (–õ–∏–¥–∏—è) - —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ GPU CC 6.1+
    if openvoice_service:
        voices.append(
            {
                "id": "lidia_openvoice",
                "name": "–õ–∏–¥–∏—è (OpenVoice)",
                "engine": "openvoice",
                "description": "–ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥–æ–ª–æ—Å (OpenVoice v2, GPU CC 6.1+)",
                "available": True,
                "samples_count": len(openvoice_service.voice_samples)
                if openvoice_service.voice_samples
                else 0,
            }
        )

    # Piper –≥–æ–ª–æ—Å–∞ (CPU)
    if piper_service:
        piper_voices = piper_service.get_available_voices()
        for voice_id, info in piper_voices.items():
            voices.append(
                {
                    "id": voice_id,
                    "name": info["name"],
                    "engine": "piper",
                    "description": info["description"],
                    "available": info["available"],
                }
            )

    return {
        "voices": voices,
        "current": current_voice_config,
    }


@app.get("/admin/voice")
async def admin_get_current_voice():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π –≤—ã–±—Ä–∞–Ω–Ω—ã–π –≥–æ–ª–æ—Å"""
    return current_voice_config


@app.post("/admin/voice")
async def admin_set_voice(request: AdminVoiceRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π –≥–æ–ª–æ—Å"""
    global current_voice_config

    voice_id = request.voice.lower()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≥–æ–ª–æ—Å–∞
    if voice_id == "gulya":
        if not gulya_voice_service:
            raise HTTPException(
                status_code=503, detail="XTTS service (–ì—É–ª—è) not available (requires GPU CC >= 7.0)"
            )
        current_voice_config = {"engine": "xtts", "voice": "gulya"}
        logger.info("üé§ –ì–æ–ª–æ—Å –∏–∑–º–µ–Ω—ë–Ω –Ω–∞: –ì—É–ª—è (XTTS)")

    elif voice_id == "lidia":
        if not voice_service:
            raise HTTPException(
                status_code=503,
                detail="XTTS service (–õ–∏–¥–∏—è) not available (requires GPU CC >= 7.0)",
            )
        current_voice_config = {"engine": "xtts", "voice": "lidia"}
        logger.info("üé§ –ì–æ–ª–æ—Å –∏–∑–º–µ–Ω—ë–Ω –Ω–∞: –õ–∏–¥–∏—è (XTTS)")

    elif voice_id == "lidia_openvoice":
        if not openvoice_service:
            raise HTTPException(status_code=503, detail="OpenVoice service not available")
        current_voice_config = {"engine": "openvoice", "voice": "lidia_openvoice"}
        logger.info("üé§ –ì–æ–ª–æ—Å –∏–∑–º–µ–Ω—ë–Ω –Ω–∞: –õ–∏–¥–∏—è (OpenVoice)")

    elif voice_id in ["dmitri", "irina"]:
        if not piper_service:
            raise HTTPException(status_code=503, detail="Piper TTS service not available")
        piper_voices = piper_service.get_available_voices()
        if voice_id not in piper_voices or not piper_voices[voice_id]["available"]:
            raise HTTPException(status_code=400, detail=f"Voice model not found: {voice_id}")
        current_voice_config = {"engine": "piper", "voice": voice_id}
        logger.info(f"üé§ –ì–æ–ª–æ—Å –∏–∑–º–µ–Ω—ë–Ω –Ω–∞: {piper_voices[voice_id]['name']} (Piper)")

    else:
        raise HTTPException(
            status_code=400,
            detail=f"Unknown voice: {voice_id}. Available: gulya, lidia, lidia_openvoice, dmitri, irina",
        )

    # Sync with service container for modular routers
    from app.dependencies import get_container

    container = get_container()
    container.current_voice_config = current_voice_config

    return {"status": "ok", **current_voice_config}


@app.post("/admin/voice/test")
async def admin_test_voice(request: AdminVoiceRequest):
    """–¢–µ—Å—Ç–æ–≤—ã–π —Å–∏–Ω—Ç–µ–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –≥–æ–ª–æ—Å–æ–º"""
    voice_id = request.voice.lower()
    test_text = "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥–æ–ª–æ—Å–∞."

    output_path = TEMP_DIR / f"voice_test_{voice_id}_{int(time.time())}.wav"

    try:
        if voice_id == "gulya":
            if not gulya_voice_service:
                raise HTTPException(
                    status_code=503, detail="XTTS (–ì—É–ª—è) not available (requires GPU CC >= 7.0)"
                )
            gulya_voice_service.synthesize_to_file(test_text, str(output_path), preset="natural")

        elif voice_id == "lidia":
            if not voice_service:
                raise HTTPException(
                    status_code=503, detail="XTTS (–õ–∏–¥–∏—è) not available (requires GPU CC >= 7.0)"
                )
            voice_service.synthesize_to_file(test_text, str(output_path), preset="natural")

        elif voice_id == "lidia_openvoice":
            if not openvoice_service:
                raise HTTPException(status_code=503, detail="OpenVoice not available")
            openvoice_service.synthesize_to_file(test_text, str(output_path), language="ru")

        elif voice_id in ["dmitri", "irina"]:
            if not piper_service:
                raise HTTPException(status_code=503, detail="Piper not available")
            piper_service.synthesize_to_file(test_text, str(output_path), voice=voice_id)

        else:
            raise HTTPException(
                status_code=400,
                detail=f"Unknown voice: {voice_id}. Available: gulya, lidia, lidia_openvoice, dmitri, irina",
            )

        return FileResponse(output_path, media_type="audio/wav", filename=f"test_{voice_id}.wav")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def get_current_tts_service():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π TTS —Å–µ—Ä–≤–∏—Å –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    engine = current_voice_config["engine"]
    voice = current_voice_config["voice"]

    if engine == "xtts" and voice == "gulya":
        return gulya_voice_service, {"preset": "natural"}
    elif engine == "xtts" and voice == "lidia":
        return voice_service, {"preset": "natural"}
    elif engine == "piper":
        return piper_service, {"voice": voice}
    else:
        # Default to gulya if available
        return gulya_voice_service or voice_service, {"preset": "natural"}


# ============== Extended Admin API Endpoints ==============


# Pydantic models for new endpoints
class AdminBackendRequest(BaseModel):
    backend: str  # "vllm", "gemini", or "cloud:{provider_id}"
    stop_unused: bool = False  # –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Å–µ—Ä–≤–∏—Å (vLLM) –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è GPU


class CloudProviderCreate(BaseModel):
    """Create cloud LLM provider"""

    name: str
    provider_type: str  # gemini, kimi, openai, claude, deepseek, custom
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    model_name: str = ""
    enabled: bool = True
    is_default: bool = False
    config: Optional[Dict] = None
    description: Optional[str] = None


class CloudProviderUpdate(BaseModel):
    """Update cloud LLM provider"""

    name: Optional[str] = None
    provider_type: Optional[str] = None
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    model_name: Optional[str] = None
    enabled: Optional[bool] = None
    is_default: Optional[bool] = None
    config: Optional[Dict] = None
    description: Optional[str] = None


class AdminPersonaRequest(BaseModel):
    persona: str  # "gulya" or "lidia"


class AdminLLMParamsRequest(BaseModel):
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    repetition_penalty: Optional[float] = None


class AdminXTTSParamsRequest(BaseModel):
    temperature: Optional[float] = None
    repetition_penalty: Optional[float] = None
    top_k: Optional[int] = None
    top_p: Optional[float] = None
    speed: Optional[float] = None
    gpt_cond_len: Optional[int] = None
    gpt_cond_chunk_len: Optional[int] = None


class AdminPiperParamsRequest(BaseModel):
    speed: float = 1.0


class AdminCustomPresetRequest(BaseModel):
    name: str
    params: dict


class AdminFAQRequest(BaseModel):
    trigger: str
    response: str


class AdminFAQTestRequest(BaseModel):
    text: str


class AdminWidgetConfigRequest(BaseModel):
    enabled: bool = True
    title: str = "AI –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç"
    greeting: str = "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ö–æ–º–ø–∞–Ω–∏—è –®–∞–µ—Ä–≤—ç–π –î–∏-–ò–¥–∂–∏—Ç–∞–ª, —á–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?"
    placeholder: str = "–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ..."
    primary_color: str = "#6366f1"
    position: str = "right"  # "left" or "right"
    allowed_domains: List[str] = []
    tunnel_url: str = ""


class AdminTelegramConfigRequest(BaseModel):
    enabled: bool = False
    bot_token: str = ""
    api_url: str = "http://localhost:8002"
    allowed_users: List[int] = []
    admin_users: List[int] = []
    welcome_message: str = "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ–º–ø–∞–Ω–∏–∏ –®–∞–µ—Ä–≤—ç–π. –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?"
    unauthorized_message: str = "–ò–∑–≤–∏–Ω–∏—Ç–µ, —É –≤–∞—Å –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ —ç—Ç–æ–º—É –±–æ—Ç—É."
    error_message: str = "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
    typing_enabled: bool = True


class AdminFinetuneConfigRequest(BaseModel):
    lora_rank: Optional[int] = None
    lora_alpha: Optional[int] = None
    batch_size: Optional[int] = None
    gradient_accumulation_steps: Optional[int] = None
    learning_rate: Optional[float] = None
    num_epochs: Optional[int] = None
    max_seq_length: Optional[int] = None
    output_dir: Optional[str] = None


class AdminAdapterRequest(BaseModel):
    adapter: str


class AdminAuditQueryRequest(BaseModel):
    action: Optional[str] = None
    resource: Optional[str] = None
    user_id: Optional[str] = None
    from_date: Optional[str] = None  # ISO format
    to_date: Optional[str] = None  # ISO format
    limit: int = 100
    offset: int = 0


# ============== Chat Models & Manager ==============


class ChatMessageModel(BaseModel):
    id: str
    role: str
    content: str
    timestamp: str
    edited: bool = False


class ChatSessionModel(BaseModel):
    id: str
    title: str
    messages: List[ChatMessageModel]
    system_prompt: Optional[str] = None
    created: str
    updated: str


class CreateSessionRequest(BaseModel):
    title: Optional[str] = None
    system_prompt: Optional[str] = None


class UpdateSessionRequest(BaseModel):
    title: Optional[str] = None
    system_prompt: Optional[str] = None


class LLMOverrideConfig(BaseModel):
    llm_backend: Optional[str] = None  # "vllm", "gemini", or "cloud:provider-id"
    system_prompt: Optional[str] = None
    llm_params: Optional[dict] = None


class SendMessageRequest(BaseModel):
    content: str
    llm_override: Optional[LLMOverrideConfig] = None


class EditMessageRequest(BaseModel):
    content: str


# ============== Services Endpoints ==============


@app.get("/admin/services/status")
async def admin_services_status():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤"""
    manager = get_service_manager()
    status = manager.get_all_status()

    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –∏–∑ orchestrator
    status["services"]["xtts_gulya"]["is_running"] = gulya_voice_service is not None
    status["services"]["xtts_lidia"]["is_running"] = voice_service is not None
    status["services"]["piper"]["is_running"] = piper_service is not None
    status["services"]["openvoice"]["is_running"] = openvoice_service is not None
    status["services"]["orchestrator"]["is_running"] = True

    return status


@app.post("/admin/services/{service}/start")
async def admin_start_service(service: str):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–∏—Å"""
    manager = get_service_manager()
    return await manager.start_service(service)


@app.post("/admin/services/{service}/stop")
async def admin_stop_service(service: str):
    """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–µ—Ä–≤–∏—Å"""
    manager = get_service_manager()
    return await manager.stop_service(service)


@app.post("/admin/services/{service}/restart")
async def admin_restart_service(service: str):
    """–ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–∏—Å"""
    manager = get_service_manager()
    return await manager.restart_service(service)


@app.post("/admin/services/start-all")
async def admin_start_all_services():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ –≤–Ω–µ—à–Ω–∏–µ —Å–µ—Ä–≤–∏—Å—ã"""
    manager = get_service_manager()
    results = {}
    for service in ["vllm"]:  # –¢–æ–ª—å–∫–æ –≤–Ω–µ—à–Ω–∏–µ —Å–µ—Ä–≤–∏—Å—ã
        results[service] = await manager.start_service(service)
    return {"status": "ok", "results": results}


@app.post("/admin/services/stop-all")
async def admin_stop_all_services():
    """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Å–µ –≤–Ω–µ—à–Ω–∏–µ —Å–µ—Ä–≤–∏—Å—ã"""
    manager = get_service_manager()
    results = {}
    for service in ["vllm"]:
        results[service] = await manager.stop_service(service)
    return {"status": "ok", "results": results}


# ============== Logs Endpoints ==============


@app.get("/admin/logs")
async def admin_list_logs():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ª–æ–≥–æ–≤"""
    manager = get_service_manager()
    return {"logs": manager.get_available_logs()}


@app.get("/admin/logs/{logfile}")
async def admin_read_log(
    logfile: str, lines: int = 100, offset: int = 0, search: Optional[str] = None
):
    """–ü—Ä–æ—á–∏—Ç–∞—Ç—å –ª–æ–≥ —Ñ–∞–π–ª"""
    manager = get_service_manager()
    return manager.read_log(logfile, lines=lines, offset=offset, search=search)


@app.get("/admin/logs/stream/{logfile}")
async def admin_stream_log(logfile: str):
    """SSE streaming –ª–æ–≥–æ–≤"""
    manager = get_service_manager()

    async def generate():
        async for data in manager.stream_log(logfile):
            yield f"data: {data}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"},
    )


# ============== LLM Enhanced Endpoints ==============


@app.get("/admin/llm/backend")
async def admin_get_llm_backend():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π LLM backend"""
    if llm_service:
        # Detect backend type
        if isinstance(llm_service, CloudLLMService):
            backend = f"cloud:{llm_service.provider_id}"
        elif hasattr(llm_service, "api_url"):
            backend = "vllm"
        else:
            backend = "gemini"

        return {
            "backend": backend,
            "model": getattr(llm_service, "model_name", "unknown"),
            "api_url": getattr(llm_service, "api_url", None),
            "provider_type": getattr(llm_service, "provider_type", None),
        }
    return {"backend": "none", "error": "LLM service not initialized"}


@app.get("/admin/llm/models")
async def admin_get_llm_models():
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π vLLM –∏ —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ Qwen, Llama, DeepSeek –∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª—è—Ö.
    """
    from vllm_llm_service import AVAILABLE_MODELS

    result = {
        "available_models": AVAILABLE_MODELS,
        "current_model": None,
        "loaded_models": [],
        "backend": "none",
    }

    if llm_service:
        is_vllm = hasattr(llm_service, "api_url")
        result["backend"] = "vllm" if is_vllm else "gemini"

        if is_vllm and hasattr(llm_service, "get_current_model_info"):
            result["current_model"] = llm_service.get_current_model_info()
            result["loaded_models"] = llm_service.get_loaded_models()
        elif not is_vllm:
            # Gemini backend
            result["current_model"] = {
                "id": "gemini",
                "name": getattr(llm_service, "model_name", "gemini-2.0-flash"),
                "description": "Google Gemini API",
                "available": True,
            }

    return result


@app.post("/admin/llm/backend")
async def admin_set_llm_backend(
    request: AdminBackendRequest, user: User = Depends(get_current_user)
):
    """–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å LLM backend —Å –≥–æ—Ä—è—á–µ–π –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–æ–π —Å–µ—Ä–≤–∏—Å–∞"""
    global LLM_BACKEND, llm_service

    # Check if it's a cloud provider
    if request.backend.startswith("cloud:"):
        provider_id = request.backend.split(":", 1)[1]
        return await _switch_to_cloud_provider(provider_id, request.stop_unused, user)

    if request.backend not in ["vllm", "gemini"]:
        raise HTTPException(
            status_code=400,
            detail="Invalid backend. Use 'vllm', 'gemini', or 'cloud:{provider_id}'",
        )

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–∏–π –±—ç–∫–µ–Ω–¥
    current_backend = "vllm" if (llm_service and hasattr(llm_service, "api_url")) else "gemini"
    if request.backend == current_backend:
        return {
            "status": "ok",
            "backend": request.backend,
            "message": f"–£–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {request.backend}",
        }

    manager = get_service_manager()
    stop_vllm = request.stop_unused if hasattr(request, "stop_unused") else False

    try:
        if request.backend == "vllm":
            # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ vLLM
            logger.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ vLLM...")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–ø—É—â–µ–Ω –ª–∏ vLLM
            vllm_status = manager.get_service_status("vllm")

            if not vllm_status.get("is_running"):
                # –ó–∞–ø—É—Å–∫–∞–µ–º vLLM
                logger.info("üöÄ –ó–∞–ø—É—Å–∫ vLLM...")
                start_result = await manager.start_service("vllm")
                if start_result.get("status") != "ok":
                    raise HTTPException(
                        status_code=503,
                        detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å vLLM: {start_result.get('message', 'Unknown error')}",
                    )

                # –ñ–¥—ë–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ vLLM (–¥–æ 120 —Å–µ–∫—É–Ω–¥)
                logger.info("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ vLLM...")
                import httpx

                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL (—É–¥–∞–ª—è–µ–º trailing /v1)
                vllm_url = os.getenv("VLLM_API_URL", "http://localhost:11434").rstrip("/")
                if vllm_url.endswith("/v1"):
                    vllm_url = vllm_url[:-3]

                for i in range(60):  # 60 * 2 = 120 —Å–µ–∫—É–Ω–¥
                    await asyncio.sleep(2)
                    try:
                        async with httpx.AsyncClient() as client:
                            resp = await client.get(f"{vllm_url}/v1/models", timeout=5.0)
                            if resp.status_code == 200:
                                logger.info(f"‚úÖ vLLM –≥–æ—Ç–æ–≤ (–ø–æ–ø—ã—Ç–∫–∞ {i + 1})")
                                break
                    except Exception:
                        pass
                else:
                    raise HTTPException(
                        status_code=503, detail=f"vLLM –Ω–µ —Å—Ç–∞–ª –¥–æ—Å—Ç—É–ø–µ–Ω –∑–∞ 120 —Å–µ–∫—É–Ω–¥ ({vllm_url})"
                    )

            # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π vLLM —Å–µ—Ä–≤–∏—Å
            if VLLMLLMService is None:
                raise HTTPException(status_code=503, detail="VLLMLLMService –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")

            new_service = VLLMLLMService()
            if not new_service.is_available():
                raise HTTPException(status_code=503, detail="vLLM –∑–∞–ø—É—â–µ–Ω, –Ω–æ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ API")

            llm_service = new_service
            LLM_BACKEND = "vllm"
            os.environ["LLM_BACKEND"] = "vllm"

            logger.info("‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ vLLM")

            # Audit log
            await async_audit_logger.log(
                action="update",
                resource="config",
                resource_id="llm_backend",
                user_id=user.username,
                details={"backend": "vllm", "model": getattr(llm_service, "model_name", "unknown")},
            )

            return {
                "status": "ok",
                "backend": "vllm",
                "model": getattr(llm_service, "model_name", "unknown"),
                "message": "–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ vLLM",
            }

        else:
            # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ Gemini
            logger.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ Gemini...")

            new_service = LLMService()
            llm_service = new_service
            LLM_BACKEND = "gemini"
            os.environ["LLM_BACKEND"] = "gemini"

            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º vLLM –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è GPU
            if stop_vllm:
                vllm_status = manager.get_service_status("vllm")
                if vllm_status.get("is_running"):
                    logger.info("üõë –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º vLLM –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è GPU...")
                    await manager.stop_service("vllm")

            logger.info("‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ Gemini")

            # Audit log
            await async_audit_logger.log(
                action="update",
                resource="config",
                resource_id="llm_backend",
                user_id=user.username,
                details={
                    "backend": "gemini",
                    "model": getattr(llm_service, "model_name", "unknown"),
                },
            )

            return {
                "status": "ok",
                "backend": "gemini",
                "model": getattr(llm_service, "model_name", "unknown"),
                "message": "–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ Gemini" + (" (vLLM –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)" if stop_vllm else ""),
            }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –±—ç–∫–µ–Ω–¥–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def _switch_to_cloud_provider(provider_id: str, stop_unused: bool, user: User):
    """Helper function to switch to a cloud provider"""
    global LLM_BACKEND, llm_service

    provider_config = await async_cloud_provider_manager.get_provider_with_key(provider_id)
    if not provider_config:
        raise HTTPException(status_code=404, detail=f"Provider {provider_id} not found")

    if not provider_config.get("enabled"):
        raise HTTPException(status_code=400, detail=f"Provider {provider_id} is disabled")

    if not provider_config.get("api_key"):
        raise HTTPException(
            status_code=400, detail=f"Provider {provider_id} has no API key configured"
        )

    try:
        new_service = CloudLLMService(provider_config)
        if not new_service.is_available():
            raise HTTPException(status_code=503, detail=f"Provider {provider_id} is not responding")

        llm_service = new_service
        LLM_BACKEND = f"cloud:{provider_id}"
        os.environ["LLM_BACKEND"] = LLM_BACKEND

        # Optionally stop vLLM to free GPU
        if stop_unused:
            manager = get_service_manager()
            vllm_status = manager.get_service_status("vllm")
            if vllm_status.get("is_running"):
                logger.info("üõë Stopping vLLM to free GPU memory...")
                await manager.stop_service("vllm")

        logger.info(f"‚úÖ Switched to cloud provider: {provider_config.get('name')}")

        # Audit log
        await async_audit_logger.log(
            action="update",
            resource="config",
            resource_id="llm_backend",
            user_id=user.username,
            details={
                "backend": f"cloud:{provider_id}",
                "provider_type": provider_config.get("provider_type"),
                "model": provider_config.get("model_name"),
            },
        )

        return {
            "status": "ok",
            "backend": f"cloud:{provider_id}",
            "provider_id": provider_id,
            "provider_type": provider_config.get("provider_type"),
            "model": provider_config.get("model_name"),
            "message": f"Switched to cloud provider: {provider_config.get('name')}",
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error switching to cloud provider: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============== Cloud LLM Providers API ==============


@app.get("/admin/llm/providers")
async def admin_list_cloud_providers(enabled_only: bool = False):
    """List all cloud LLM providers"""
    providers = await async_cloud_provider_manager.list_providers(enabled_only)
    return {
        "providers": providers,
        "provider_types": PROVIDER_TYPES,
    }


@app.get("/admin/llm/providers/{provider_id}")
async def admin_get_cloud_provider(
    provider_id: str, include_key: bool = False, user: User = Depends(get_current_user)
):
    """Get cloud provider by ID"""
    if include_key:
        provider = await async_cloud_provider_manager.get_provider_with_key(provider_id)
    else:
        provider = await async_cloud_provider_manager.get_provider(provider_id)

    if not provider:
        raise HTTPException(status_code=404, detail="Provider not found")
    return {"provider": provider}


@app.post("/admin/llm/providers")
async def admin_create_cloud_provider(
    data: CloudProviderCreate, user: User = Depends(get_current_user)
):
    """Create new cloud LLM provider"""
    try:
        provider = await async_cloud_provider_manager.create_provider(
            name=data.name,
            provider_type=data.provider_type,
            api_key=data.api_key,
            base_url=data.base_url,
            model_name=data.model_name,
            enabled=data.enabled,
            is_default=data.is_default,
            config=data.config,
            description=data.description,
        )

        # Audit log
        await async_audit_logger.log(
            action="create",
            resource="cloud_provider",
            resource_id=provider["id"],
            user_id=user.username,
            details={"name": data.name, "provider_type": data.provider_type},
        )

        return {"status": "ok", "provider": provider}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@app.put("/admin/llm/providers/{provider_id}")
async def admin_update_cloud_provider(
    provider_id: str, data: CloudProviderUpdate, user: User = Depends(get_current_user)
):
    """Update cloud LLM provider"""
    # Filter out None values
    update_data = {k: v for k, v in data.model_dump().items() if v is not None}

    provider = await async_cloud_provider_manager.update_provider(provider_id, **update_data)
    if not provider:
        raise HTTPException(status_code=404, detail="Provider not found")

    # Audit log
    await async_audit_logger.log(
        action="update",
        resource="cloud_provider",
        resource_id=provider_id,
        user_id=user.username,
        details=update_data,
    )

    return {"status": "ok", "provider": provider}


@app.delete("/admin/llm/providers/{provider_id}")
async def admin_delete_cloud_provider(provider_id: str, user: User = Depends(get_current_user)):
    """Delete cloud LLM provider"""
    if not await async_cloud_provider_manager.delete_provider(provider_id):
        raise HTTPException(status_code=404, detail="Provider not found")

    # Audit log
    await async_audit_logger.log(
        action="delete",
        resource="cloud_provider",
        resource_id=provider_id,
        user_id=user.username,
    )

    return {"status": "ok", "message": f"Provider {provider_id} deleted"}


@app.post("/admin/llm/providers/{provider_id}/test")
async def admin_test_cloud_provider(provider_id: str, user: User = Depends(get_current_user)):
    """Test cloud provider connection"""
    provider_config = await async_cloud_provider_manager.get_provider_with_key(provider_id)
    if not provider_config:
        raise HTTPException(status_code=404, detail="Provider not found")

    if not provider_config.get("api_key"):
        return {
            "status": "error",
            "available": False,
            "message": "No API key configured",
        }

    try:
        service = CloudLLMService(provider_config)
        is_available = service.is_available()

        if is_available:
            # Quick test generation
            test_response = service.generate_response("–°–∫–∞–∂–∏ '—Ç–µ—Å—Ç –æ–∫'", use_history=False)
            return {
                "status": "ok",
                "available": True,
                "test_response": test_response[:200] if test_response else "",
            }
        else:
            return {
                "status": "error",
                "available": False,
                "message": "Provider not responding",
            }
    except Exception as e:
        return {
            "status": "error",
            "available": False,
            "message": str(e),
        }


@app.post("/admin/llm/providers/{provider_id}/set-default")
async def admin_set_default_cloud_provider(
    provider_id: str, user: User = Depends(get_current_user)
):
    """Set cloud provider as default"""
    if not await async_cloud_provider_manager.set_default(provider_id):
        raise HTTPException(status_code=404, detail="Provider not found or disabled")

    await async_audit_logger.log(
        action="update",
        resource="cloud_provider",
        resource_id=provider_id,
        user_id=user.username,
        details={"is_default": True},
    )

    return {"status": "ok", "message": f"Provider {provider_id} set as default"}


@app.get("/admin/llm/personas")
async def admin_get_personas():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω"""
    if llm_service and hasattr(llm_service, "get_available_personas"):
        return {"personas": llm_service.get_available_personas()}

    # Fallback –¥–ª—è Gemini LLM Service
    from vllm_llm_service import SECRETARY_PERSONAS

    return {
        "personas": {
            pid: {"name": p["name"], "full_name": p.get("full_name", p["name"])}
            for pid, p in SECRETARY_PERSONAS.items()
        }
    }


@app.get("/admin/llm/persona")
async def admin_get_current_persona():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –ø–µ—Ä—Å–æ–Ω—É"""
    if llm_service:
        persona_id = getattr(llm_service, "persona_id", "gulya")
        persona = getattr(llm_service, "persona", {})
        return {
            "id": persona_id,
            "name": persona.get("name", "Unknown"),
        }
    return {"id": "none", "error": "LLM service not initialized"}


@app.post("/admin/llm/persona")
async def admin_set_persona(request: AdminPersonaRequest, user: User = Depends(get_current_user)):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–µ—Ä—Å–æ–Ω—É"""
    if llm_service and hasattr(llm_service, "set_persona"):
        success = llm_service.set_persona(request.persona)
        if success:
            # Audit log
            await async_audit_logger.log(
                action="update",
                resource="config",
                resource_id="llm_persona",
                user_id=user.username,
                details={"persona": request.persona},
            )
            return {"status": "ok", "persona": request.persona}
        raise HTTPException(status_code=400, detail=f"Persona not found: {request.persona}")
    raise HTTPException(status_code=503, detail="LLM service does not support personas")


@app.get("/admin/llm/params")
async def admin_get_llm_params():
    """–ü–æ–ª—É—á–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM"""
    if llm_service and hasattr(llm_service, "runtime_params"):
        return {"params": llm_service.runtime_params}

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    return {
        "params": {"temperature": 0.7, "max_tokens": 512, "top_p": 0.9, "repetition_penalty": 1.1}
    }


@app.post("/admin/llm/params")
async def admin_set_llm_params(request: AdminLLMParamsRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM"""
    if llm_service and hasattr(llm_service, "set_params"):
        params = {k: v for k, v in request.dict().items() if v is not None}
        llm_service.set_params(**params)
        return {"status": "ok", "params": llm_service.runtime_params}

    # –î–ª—è vLLM —Å–µ—Ä–≤–∏—Å–∞ –±–µ–∑ set_params - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∞—Ç—Ä–∏–±—É—Ç–µ
    if llm_service:
        if not hasattr(llm_service, "runtime_params"):
            llm_service.runtime_params = {}
        params = {k: v for k, v in request.dict().items() if v is not None}
        llm_service.runtime_params.update(params)
        return {"status": "ok", "params": llm_service.runtime_params}

    raise HTTPException(status_code=503, detail="LLM service not initialized")


@app.get("/admin/llm/prompt/{persona}")
async def admin_get_persona_prompt(persona: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω—ã"""
    try:
        from vllm_llm_service import SECRETARY_PERSONAS

        if persona in SECRETARY_PERSONAS:
            return {"persona": persona, "prompt": SECRETARY_PERSONAS[persona]["prompt"]}
        raise HTTPException(status_code=404, detail=f"Persona not found: {persona}")
    except ImportError:
        raise HTTPException(status_code=503, detail="vLLM service not available")


@app.post("/admin/llm/prompt/{persona}")
async def admin_set_persona_prompt(persona: str, request: AdminLLMPromptRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω—ã"""
    try:
        from vllm_llm_service import SECRETARY_PERSONAS

        if persona not in SECRETARY_PERSONAS:
            raise HTTPException(status_code=404, detail=f"Persona not found: {persona}")

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–º–ø—Ç
        SECRETARY_PERSONAS[persona]["prompt"] = request.prompt

        # –ï—Å–ª–∏ —ç—Ç–æ —Ç–µ–∫—É—â–∞—è –ø–µ—Ä—Å–æ–Ω–∞ - –æ–±–Ω–æ–≤–ª—è–µ–º –≤ —Å–µ—Ä–≤–∏—Å–µ
        if llm_service and hasattr(llm_service, "persona_id") and llm_service.persona_id == persona:
            llm_service.system_prompt = request.prompt

        return {"status": "ok", "persona": persona}
    except ImportError:
        raise HTTPException(status_code=503, detail="vLLM service not available")


@app.post("/admin/llm/prompt/{persona}/reset")
async def admin_reset_persona_prompt(persona: str):
    """–°–±—Ä–æ—Å–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –ø–µ—Ä—Å–æ–Ω—ã –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
    # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
    raise HTTPException(status_code=501, detail="Not implemented yet")


# ============== Fine-tuning Endpoints ==============


@app.post("/admin/finetune/dataset/upload")
async def admin_upload_dataset(file: UploadFile = File(...)):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç (Telegram export JSON)"""
    manager = get_finetune_manager()
    content = await file.read()
    return await manager.upload_dataset(content, file.filename)


class DatasetProcessRequest(BaseModel):
    owner_name: Optional[str] = None
    transcribe_voice: Optional[bool] = None
    min_dialog_messages: Optional[int] = None
    max_message_length: Optional[int] = None
    max_dialog_length: Optional[int] = None
    include_groups: Optional[bool] = None
    output_name: Optional[str] = None


@app.post("/admin/finetune/dataset/process")
async def admin_process_dataset(request: Optional[DatasetProcessRequest] = None):
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç"""
    manager = get_finetune_manager()
    config = request.model_dump(exclude_none=True) if request else None
    return await manager.process_dataset(config)


@app.get("/admin/finetune/dataset/config")
async def admin_get_dataset_config():
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    manager = get_finetune_manager()
    return {"config": manager.get_dataset_config()}


@app.post("/admin/finetune/dataset/config")
async def admin_set_dataset_config(request: DatasetProcessRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    manager = get_finetune_manager()
    return manager.set_dataset_config(**request.model_dump(exclude_none=True))


@app.get("/admin/finetune/dataset/processing-status")
async def admin_get_processing_status():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    manager = get_finetune_manager()
    return {"status": manager.get_processing_status()}


@app.get("/admin/finetune/dataset/stats")
async def admin_get_dataset_stats():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    manager = get_finetune_manager()
    stats = manager.get_dataset_stats()
    return {
        "stats": {
            "total_sessions": stats.total_sessions,
            "total_messages": stats.total_messages,
            "total_tokens": stats.total_tokens,
            "avg_tokens_per_message": stats.avg_tokens_per_message,
            "file_path": stats.file_path,
            "file_size_mb": stats.file_size_mb,
            "modified": stats.modified,
        }
    }


@app.get("/admin/finetune/dataset/list")
async def admin_list_datasets():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    manager = get_finetune_manager()
    return {"datasets": manager.list_datasets()}


@app.post("/admin/finetune/dataset/augment")
async def admin_augment_dataset():
    """–ê—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç"""
    manager = get_finetune_manager()
    return await manager.augment_dataset()


class GenerateProjectDatasetRequest(BaseModel):
    include_tz: bool = True
    include_faq: bool = True
    include_docs: bool = True
    include_escalation: bool = True
    include_code: bool = True  # Python –∫–æ–¥ –∏ Markdown –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
    output_name: str = "project_dataset"


@app.post("/admin/finetune/dataset/generate-project")
async def admin_generate_project_dataset(request: GenerateProjectDatasetRequest):
    """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ –ø—Ä–æ–µ–∫—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–¢–ó, FAQ, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è, —ç—Å–∫–∞–ª–∞—Ü–∏–∏, –∫–æ–¥)"""
    manager = get_finetune_manager()
    return await manager.generate_project_dataset(
        include_tz=request.include_tz,
        include_faq=request.include_faq,
        include_docs=request.include_docs,
        include_escalation=request.include_escalation,
        include_code=request.include_code,
        output_name=request.output_name,
    )


@app.get("/admin/finetune/config")
async def admin_get_finetune_config():
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è"""
    manager = get_finetune_manager()
    config = manager.get_config()
    return {
        "config": {
            "base_model": config.base_model,
            "lora_rank": config.lora_rank,
            "lora_alpha": config.lora_alpha,
            "lora_dropout": config.lora_dropout,
            "batch_size": config.batch_size,
            "gradient_accumulation_steps": config.gradient_accumulation_steps,
            "learning_rate": config.learning_rate,
            "num_epochs": config.num_epochs,
            "warmup_ratio": config.warmup_ratio,
            "max_seq_length": config.max_seq_length,
            "output_dir": config.output_dir,
        },
        "presets": {
            name: {
                "lora_rank": p.lora_rank,
                "batch_size": p.batch_size,
                "num_epochs": p.num_epochs,
            }
            for name, p in manager.get_config_presets().items()
        },
    }


@app.post("/admin/finetune/config")
async def admin_set_finetune_config(request: AdminFinetuneConfigRequest):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è"""
    manager = get_finetune_manager()
    config = manager.get_config()

    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    if request.lora_rank is not None:
        config.lora_rank = request.lora_rank
    if request.lora_alpha is not None:
        config.lora_alpha = request.lora_alpha
    if request.batch_size is not None:
        config.batch_size = request.batch_size
    if request.gradient_accumulation_steps is not None:
        config.gradient_accumulation_steps = request.gradient_accumulation_steps
    if request.learning_rate is not None:
        config.learning_rate = request.learning_rate
    if request.num_epochs is not None:
        config.num_epochs = request.num_epochs
    if request.max_seq_length is not None:
        config.max_seq_length = request.max_seq_length
    if request.output_dir is not None:
        config.output_dir = request.output_dir

    return manager.set_config(config)


@app.post("/admin/finetune/train/start")
async def admin_start_training():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ"""
    manager = get_finetune_manager()
    return await manager.start_training()


@app.post("/admin/finetune/train/stop")
async def admin_stop_training():
    """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ"""
    manager = get_finetune_manager()
    return await manager.stop_training()


@app.get("/admin/finetune/train/status")
async def admin_get_training_status():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è"""
    manager = get_finetune_manager()
    status = manager.get_training_status()
    return {
        "status": {
            "is_running": status.is_running,
            "current_step": status.current_step,
            "total_steps": status.total_steps,
            "current_epoch": status.current_epoch,
            "total_epochs": status.total_epochs,
            "loss": status.loss,
            "learning_rate": status.learning_rate,
            "elapsed_seconds": status.elapsed_seconds,
            "eta_seconds": status.eta_seconds,
            "error": status.error,
        }
    }


@app.get("/admin/finetune/train/log")
async def admin_stream_training_log():
    """SSE streaming –ª–æ–≥–∞ –æ–±—É—á–µ–Ω–∏—è"""
    manager = get_finetune_manager()

    async def generate():
        async for data in manager.stream_training_log():
            yield f"data: {data}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"},
    )


@app.get("/admin/finetune/adapters")
async def admin_list_adapters():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤"""
    manager = get_finetune_manager()
    adapters = manager.list_adapters()
    return {
        "adapters": [
            {
                "name": a.name,
                "path": a.path,
                "size_mb": a.size_mb,
                "modified": a.modified,
                "active": a.active,
                "config": a.config,
            }
            for a in adapters
        ],
        "active": manager.active_adapter,
    }


@app.post("/admin/finetune/adapters/activate")
async def admin_activate_adapter(request: AdminAdapterRequest):
    """–ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å LoRA –∞–¥–∞–ø—Ç–µ—Ä"""
    manager = get_finetune_manager()
    return await manager.activate_adapter(request.adapter)


@app.delete("/admin/finetune/adapters/{name}")
async def admin_delete_adapter(name: str):
    """–£–¥–∞–ª–∏—Ç—å LoRA –∞–¥–∞–ø—Ç–µ—Ä"""
    manager = get_finetune_manager()
    return await manager.delete_adapter(name)


# ============== TTS Finetune Endpoints ==============


@app.get("/admin/tts-finetune/config")
async def admin_get_tts_finetune_config():
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é TTS fine-tuning"""
    manager = get_tts_finetune_manager()
    return {"config": manager.get_config()}


@app.post("/admin/tts-finetune/config")
async def admin_set_tts_finetune_config(config: dict):
    """–û–±–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é TTS fine-tuning"""
    manager = get_tts_finetune_manager()
    return {"status": "ok", "config": manager.set_config(config)}


@app.get("/admin/tts-finetune/samples")
async def admin_get_tts_samples():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–∑—Ü–æ–≤ –≥–æ–ª–æ—Å–∞"""
    manager = get_tts_finetune_manager()
    return {"samples": manager.get_samples()}


@app.post("/admin/tts-finetune/samples/upload")
async def admin_upload_tts_sample(file: UploadFile = File(...)):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–±—Ä–∞–∑–µ—Ü –≥–æ–ª–æ—Å–∞"""
    manager = get_tts_finetune_manager()
    content = await file.read()
    sample = manager.add_sample(file.filename, content)
    return {
        "status": "ok",
        "sample": {
            "filename": sample.filename,
            "path": sample.path,
            "duration_sec": sample.duration_sec,
            "size_kb": sample.size_kb,
        },
    }


@app.delete("/admin/tts-finetune/samples/{filename}")
async def admin_delete_tts_sample(filename: str):
    """–£–¥–∞–ª–∏—Ç—å –æ–±—Ä–∞–∑–µ—Ü –≥–æ–ª–æ—Å–∞"""
    manager = get_tts_finetune_manager()
    if manager.delete_sample(filename):
        return {"status": "ok", "message": f"Sample {filename} deleted"}
    raise HTTPException(status_code=404, detail="Sample not found")


@app.put("/admin/tts-finetune/samples/{filename}/transcript")
async def admin_update_tts_transcript(filename: str, request: dict):
    """–û–±–Ω–æ–≤–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –æ–±—Ä–∞–∑—Ü–∞"""
    manager = get_tts_finetune_manager()
    transcript = request.get("transcript", "")
    sample = manager.update_transcript(filename, transcript)
    if not sample:
        raise HTTPException(status_code=404, detail="Sample not found")
    return {
        "status": "ok",
        "sample": {
            "filename": sample.filename,
            "transcript": sample.transcript,
            "transcript_edited": sample.transcript_edited,
        },
    }


@app.post("/admin/tts-finetune/transcribe")
async def admin_transcribe_tts_samples():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é –æ–±—Ä–∞–∑—Ü–æ–≤ —á–µ—Ä–µ–∑ Whisper"""
    manager = get_tts_finetune_manager()
    if manager.transcribe_samples():
        return {"status": "ok", "message": "Transcription started"}
    return {"status": "error", "message": "Already running or no samples to transcribe"}


@app.post("/admin/tts-finetune/prepare")
async def admin_prepare_tts_dataset():
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç (–∏–∑–≤–ª–µ—á—å audio_codes)"""
    manager = get_tts_finetune_manager()
    if manager.prepare_dataset():
        return {"status": "ok", "message": "Dataset preparation started"}
    return {"status": "error", "message": "Already running or no samples with transcripts"}


@app.get("/admin/tts-finetune/processing-status")
async def admin_get_tts_processing_status():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    manager = get_tts_finetune_manager()
    return {"status": manager.get_processing_status()}


@app.post("/admin/tts-finetune/train/start")
async def admin_start_tts_training():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ TTS"""
    manager = get_tts_finetune_manager()
    if manager.start_training():
        return {"status": "ok", "message": "Training started"}
    return {"status": "error", "message": "Already running or dataset not prepared"}


@app.post("/admin/tts-finetune/train/stop")
async def admin_stop_tts_training():
    """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ TTS"""
    manager = get_tts_finetune_manager()
    if manager.stop_training():
        return {"status": "ok", "message": "Training stopped"}
    return {"status": "error", "message": "Training not running"}


@app.get("/admin/tts-finetune/train/status")
async def admin_get_tts_training_status():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è TTS"""
    manager = get_tts_finetune_manager()
    return {"status": manager.get_training_status()}


@app.get("/admin/tts-finetune/train/log")
async def admin_get_tts_training_log():
    """–ü–æ–ª—É—á–∏—Ç—å –ª–æ–≥ –æ–±—É—á–µ–Ω–∏—è TTS"""
    manager = get_tts_finetune_manager()
    return {"log": manager.get_training_log()}


@app.get("/admin/tts-finetune/models")
async def admin_get_tts_trained_models():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –æ–±—É—á–µ–Ω–Ω—ã—Ö TTS –º–æ–¥–µ–ª–µ–π"""
    manager = get_tts_finetune_manager()
    return {"models": manager.get_trained_models()}


# ============== Monitoring Endpoints ==============


@app.get("/admin/monitor/gpu")
async def admin_get_gpu_stats():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É GPU"""
    import torch

    if not torch.cuda.is_available():
        return {"available": False, "gpus": []}

    gpus = []
    for i in range(torch.cuda.device_count()):
        try:
            name = torch.cuda.get_device_name(i)
            props = torch.cuda.get_device_properties(i)
            total_memory = props.total_memory / (1024**3)
            allocated = torch.cuda.memory_allocated(i) / (1024**3)
            reserved = torch.cuda.memory_reserved(i) / (1024**3)

            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —É—Ç–∏–ª–∏–∑–∞—Ü–∏—é —á–µ—Ä–µ–∑ nvidia-smi
            utilization = None
            temperature = None
            try:
                import subprocess

                result = subprocess.run(
                    [
                        "nvidia-smi",
                        f"--id={i}",
                        "--query-gpu=utilization.gpu,temperature.gpu",
                        "--format=csv,noheader,nounits",
                    ],
                    capture_output=True,
                    text=True,
                    timeout=5,
                )
                if result.returncode == 0:
                    parts = result.stdout.strip().split(",")
                    if len(parts) >= 2:
                        utilization = int(parts[0].strip())
                        temperature = int(parts[1].strip())
            except Exception:
                pass

            gpus.append(
                {
                    "id": i,
                    "name": name,
                    "total_memory_gb": round(total_memory, 2),
                    "allocated_gb": round(allocated, 2),
                    "reserved_gb": round(reserved, 2),
                    "free_gb": round(total_memory - reserved, 2),
                    "utilization_percent": utilization,
                    "temperature_c": temperature,
                    "compute_capability": f"{props.major}.{props.minor}",
                }
            )
        except Exception as e:
            logger.warning(f"Error getting GPU {i} stats: {e}")

    return {"available": True, "gpus": gpus}


@app.get("/admin/monitor/gpu/stream")
async def admin_stream_gpu_stats():
    """SSE streaming —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ GPU"""
    import torch

    async def generate():
        while True:
            if torch.cuda.is_available():
                gpus = []
                for i in range(torch.cuda.device_count()):
                    try:
                        allocated = torch.cuda.memory_allocated(i) / (1024**3)
                        reserved = torch.cuda.memory_reserved(i) / (1024**3)
                        gpus.append(
                            {
                                "id": i,
                                "allocated_gb": round(allocated, 2),
                                "reserved_gb": round(reserved, 2),
                            }
                        )
                    except Exception:
                        pass

                yield f"data: {json.dumps({'gpus': gpus, 'timestamp': datetime.now().isoformat()})}\n\n"
            else:
                yield f"data: {json.dumps({'available': False})}\n\n"

            await asyncio.sleep(5)

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"},
    )


@app.get("/admin/monitor/health")
async def admin_get_health():
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
    manager = get_service_manager()
    health = {"timestamp": datetime.now().isoformat(), "overall": "healthy", "components": {}}

    # Orchestrator
    health["components"]["orchestrator"] = {"status": "healthy", "uptime": "running"}

    # LLM
    if llm_service:
        try:
            if hasattr(llm_service, "is_available") and llm_service.is_available():
                health["components"]["llm"] = {"status": "healthy", "backend": "vllm"}
            else:
                health["components"]["llm"] = {"status": "healthy", "backend": "gemini"}
        except Exception as e:
            health["components"]["llm"] = {"status": "unhealthy", "error": str(e)}
            health["overall"] = "degraded"
    else:
        health["components"]["llm"] = {"status": "unavailable"}
        health["overall"] = "degraded"

    # TTS
    if gulya_voice_service or voice_service:
        health["components"]["tts_xtts"] = {"status": "healthy"}
    else:
        health["components"]["tts_xtts"] = {"status": "unavailable"}

    if piper_service:
        health["components"]["tts_piper"] = {"status": "healthy"}
    else:
        health["components"]["tts_piper"] = {"status": "unavailable"}

    # vLLM external process
    vllm_status = manager.get_service_status("vllm")
    if vllm_status["is_running"]:
        health["components"]["vllm_process"] = {"status": "healthy", "pid": vllm_status["pid"]}
    else:
        health["components"]["vllm_process"] = {"status": "stopped"}

    return health


@app.get("/admin/monitor/metrics")
async def admin_get_metrics():
    """–ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
    import psutil

    metrics = {
        "timestamp": datetime.now().isoformat(),
        "system": {
            "cpu_percent": psutil.cpu_percent(interval=0.1),
            "memory_percent": psutil.virtual_memory().percent,
            "memory_used_gb": round(psutil.virtual_memory().used / (1024**3), 2),
            "memory_total_gb": round(psutil.virtual_memory().total / (1024**3), 2),
        },
        "streaming_tts": streaming_tts_manager.get_stats() if streaming_tts_manager else None,
    }

    # LLM –º–µ—Ç—Ä–∏–∫–∏
    if llm_service:
        metrics["llm"] = {
            "history_length": len(getattr(llm_service, "conversation_history", [])),
            "faq_count": len(getattr(llm_service, "faq", {})),
        }

    return metrics


@app.get("/admin/monitor/errors")
async def admin_get_errors():
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –æ—à–∏–±–∫–∏"""
    manager = get_service_manager()
    return {"errors": manager.last_errors, "timestamp": datetime.now().isoformat()}


@app.post("/admin/monitor/metrics/reset")
async def admin_reset_metrics():
    """–°–±—Ä–æ—Å–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏"""
    # –û—á–∏—â–∞–µ–º –∫—ç—à TTS
    if streaming_tts_manager:
        with streaming_tts_manager._cache_lock:
            streaming_tts_manager._cache.clear()

    return {"status": "ok", "message": "Metrics reset"}


@app.get("/admin/monitor/system")
async def admin_get_system_status():
    """–ü–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ: GPU, CPU, RAM, –¥–∏—Å–∫–∏, Docker, —Å–µ—Ç—å"""
    monitor = get_system_monitor()
    return monitor.get_full_status()


# ============== Model Management API ==============


@app.get("/admin/models/list")
async def admin_list_models():
    """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    manager = get_model_manager()
    return {"models": manager.get_cached_models()}


@app.post("/admin/models/scan")
async def admin_scan_models(request: Request):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"""
    data = await request.json() if request.headers.get("content-type") == "application/json" else {}
    include_system = data.get("include_system", False)

    manager = get_model_manager()
    if manager.scan_all_models(include_system=include_system):
        return {"status": "ok", "message": "Scan started"}
    else:
        return {"status": "error", "message": "Scan already in progress"}


@app.post("/admin/models/scan/cancel")
async def admin_cancel_scan():
    """–û—Ç–º–µ–Ω–∏—Ç—å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ"""
    manager = get_model_manager()
    manager.cancel_scan()
    return {"status": "ok", "message": "Scan cancelled"}


@app.get("/admin/models/scan/status")
async def admin_scan_status():
    """–°—Ç–∞—Ç—É—Å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è"""
    manager = get_model_manager()
    return {"status": manager.get_scan_progress()}


@app.post("/admin/models/download")
async def admin_download_model(request: Request):
    """–°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å —Å HuggingFace"""
    data = await request.json()
    repo_id = data.get("repo_id")
    revision = data.get("revision", "main")

    if not repo_id:
        raise HTTPException(status_code=400, detail="repo_id required")

    manager = get_model_manager()
    if manager.download_model(repo_id, revision):
        return {"status": "ok", "message": f"Download started: {repo_id}"}
    else:
        return {"status": "error", "message": "Download already in progress"}


@app.post("/admin/models/download/cancel")
async def admin_cancel_download():
    """–û—Ç–º–µ–Ω–∏—Ç—å –∑–∞–≥—Ä—É–∑–∫—É"""
    manager = get_model_manager()
    manager.cancel_download()
    return {"status": "ok", "message": "Download cancelled"}


@app.get("/admin/models/download/status")
async def admin_download_status():
    """–°—Ç–∞—Ç—É—Å –∑–∞–≥—Ä—É–∑–∫–∏"""
    manager = get_model_manager()
    return {"status": manager.get_download_progress()}


@app.delete("/admin/models/delete")
async def admin_delete_model(path: str):
    """–£–¥–∞–ª–∏—Ç—å –º–æ–¥–µ–ª—å"""
    manager = get_model_manager()
    result = manager.delete_model(path)
    if result["status"] == "ok":
        return result
    else:
        raise HTTPException(status_code=400, detail=result.get("error", "Delete failed"))


@app.get("/admin/models/search")
async def admin_search_huggingface(query: str, limit: int = 20):
    """–ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π –Ω–∞ HuggingFace"""
    manager = get_model_manager()
    results = manager.search_huggingface(query, limit)
    return {"results": results}


@app.get("/admin/models/details/{repo_id:path}")
async def admin_get_model_details(repo_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª–∏ –º–æ–¥–µ–ª–∏ —Å HuggingFace"""
    manager = get_model_manager()
    details = manager.get_model_details(repo_id)
    if details:
        return {"details": details}
    else:
        raise HTTPException(status_code=404, detail="Model not found")


# ============== Widget Script Endpoint ==============


def _escape_js_string(s: str) -> str:
    """Escape a string for safe use in JavaScript."""
    return s.replace("'", "\\'")


@app.get("/widget.js")
async def get_widget_script(request: Request, instance: Optional[str] = None):
    """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–π —Å–∫—Ä–∏–ø—Ç –≤–∏–¥–∂–µ—Ç–∞.

    Args:
        instance: Optional widget instance ID. If not provided, uses legacy config or 'default' instance.
    """
    config = None

    # Try to load from widget instance if specified
    if instance:
        instance_data = await async_widget_instance_manager.get_instance(instance)
        if instance_data:
            config = instance_data
        else:
            return Response(
                content=f"// Widget instance '{instance}' not found",
                media_type="application/javascript",
                status_code=404,
            )
    else:
        # Try default instance first, fallback to legacy config
        instance_data = await async_widget_instance_manager.get_instance("default")
        if instance_data:
            config = instance_data
        else:
            # Fallback to legacy config
            config = await async_config_manager.get_widget()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∫–ª—é—á–µ–Ω –ª–∏ –≤–∏–¥–∂–µ—Ç
    if not config.get("enabled", False):
        return Response(content="// Widget is disabled", media_type="application/javascript")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–µ)
    origin = request.headers.get("origin", "") or request.headers.get("referer", "")
    allowed_domains = config.get("allowed_domains", [])
    if allowed_domains and origin:
        origin_domain = (
            origin.replace("https://", "").replace("http://", "").split("/")[0].split(":")[0]
        )
        if not any(d in origin_domain for d in allowed_domains):
            return Response(
                content=f"// Widget not allowed for domain: {origin_domain}",
                media_type="application/javascript",
            )

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º API URL
    api_url = config.get("tunnel_url", "").strip()
    if not api_url:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–π —Ö–æ—Å—Ç –µ—Å–ª–∏ tunnel_url –Ω–µ —É–∫–∞–∑–∞–Ω
        api_url = str(request.base_url).rstrip("/")

    # –ß–∏—Ç–∞–µ–º –±–∞–∑–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –≤–∏–¥–∂–µ—Ç–∞
    widget_path = Path(__file__).parent / "web-widget" / "ai-chat-widget.js"
    if not widget_path.exists():
        return Response(
            content="// Widget script not found",
            media_type="application/javascript",
            status_code=404,
        )

    widget_js = widget_path.read_text(encoding="utf-8")

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∫—Ä–∏–ø—Ç —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    instance_id = instance or config.get("id", "default")
    settings_js = f"""
// Auto-generated widget settings
// Instance: {instance_id}
window.aiChatSettings = {{
  apiUrl: '{api_url}',
  instanceId: '{instance_id}',
  title: '{config.get("title", "AI –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç")}',
  greeting: '{_escape_js_string(config.get("greeting") or "")}',
  placeholder: '{_escape_js_string(config.get("placeholder") or "")}',
  primaryColor: '{config.get("primary_color", "#6366f1")}',
  position: '{config.get("position", "right")}'
}};

"""
    full_script = settings_js + widget_js

    return Response(
        content=full_script,
        media_type="application/javascript",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Access-Control-Allow-Origin": "*",
        },
    )


# ============== Static Files for Vue Admin ==============

DEV_MODE = os.getenv("DEV_MODE", "").lower() in ("1", "true", "yes")
VITE_DEV_URL = os.getenv("VITE_DEV_URL", "http://localhost:5173")

if DEV_MODE:
    # Dev mode: proxy to Vite dev server for hot reload
    import httpx

    @app.api_route("/admin/{path:path}", methods=["GET", "HEAD"])
    async def proxy_to_vite(path: str, request: Request):
        """Proxy static files to Vite dev server"""
        async with httpx.AsyncClient() as client:
            url = f"{VITE_DEV_URL}/admin/{path}"
            try:
                resp = await client.get(url, headers=dict(request.headers))
                return Response(
                    content=resp.content, status_code=resp.status_code, headers=dict(resp.headers)
                )
            except httpx.ConnectError:
                return Response(
                    content=b"Vite dev server not running. Start with: cd admin && npm run dev",
                    status_code=503,
                )

    @app.get("/admin")
    async def proxy_admin_root():
        """Redirect /admin to /admin/"""
        return RedirectResponse(url="/admin/")

    logger.info(f"üîß DEV MODE: Proxying /admin/* to Vite at {VITE_DEV_URL}")
else:
    # Production: serve built Vue app
    admin_dist_path = Path(__file__).parent / "admin" / "dist"
    if admin_dist_path.exists():
        from fastapi.staticfiles import StaticFiles

        app.mount("/admin", StaticFiles(directory=str(admin_dist_path), html=True), name="admin")
        logger.info(f"üìÇ Vue admin mounted at /admin from {admin_dist_path}")


if __name__ == "__main__":
    from dotenv import load_dotenv

    load_dotenv()
    port = int(os.getenv("ORCHESTRATOR_PORT", 8002))
    logger.info(f"üéØ –ó–∞–ø—É—Å–∫ Orchestrator –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    uvicorn.run("orchestrator:app", host="0.0.0.0", port=port, reload=False, log_level="info")
