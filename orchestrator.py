#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä - –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤—Å–µ —Å–µ—Ä–≤–∏—Å—ã
STT (Whisper) -> LLM (Gemini) -> TTS (XTTS v2)
"""
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import logging
from pathlib import Path
import os
from typing import Optional, List
from pydantic import BaseModel
from datetime import datetime
import json
import time

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
from voice_clone_service import VoiceCloneService
from stt_service import STTService
from llm_service import LLMService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="AI Secretary Orchestrator", version="1.0.0")

# CORS –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã
voice_service: Optional[VoiceCloneService] = None
stt_service: Optional[STTService] = None
llm_service: Optional[LLMService] = None

# –ü–∞–ø–∫–∞ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
TEMP_DIR = Path("./temp")
TEMP_DIR.mkdir(exist_ok=True)

# –ü–∞–ø–∫–∞ –¥–ª—è –ª–æ–≥–æ–≤ –∑–≤–æ–Ω–∫–æ–≤
CALLS_LOG_DIR = Path("./calls_log")
CALLS_LOG_DIR.mkdir(exist_ok=True)


class ConversationRequest(BaseModel):
    text: str
    session_id: Optional[str] = None


class TTSRequest(BaseModel):
    text: str
    language: str = "ru"


class OpenAISpeechRequest(BaseModel):
    """OpenAI-compatible TTS request for OpenWebUI integration"""
    model: str = "lidia-voice"
    input: str
    voice: str = "lidia"
    response_format: str = "wav"
    speed: float = 1.0


class ChatMessage(BaseModel):
    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    """OpenAI-compatible chat completion request"""
    model: str = "lidia-gemini"
    messages: List[ChatMessage]
    stream: bool = False
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None


@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ"""
    global voice_service, stt_service, llm_service

    logger.info("üöÄ –ó–∞–ø—É—Å–∫ AI Secretary Orchestrator")

    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
        logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ Voice Clone Service...")
        voice_service = VoiceCloneService()

        logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ LLM Service...")
        llm_service = LLMService()

        # STT –æ—Ç–∫–ª—é—á—ë–Ω –≤—Ä–µ–º–µ–Ω–Ω–æ - –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞—Ç–∞ –Ω–µ –Ω—É–∂–µ–Ω
        # –ú–æ–¥–µ–ª—å faster-whisper –∑–∞–≤–∏—Å–∞–µ—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
        logger.info("‚è≠Ô∏è STT –æ—Ç–∫–ª—é—á—ë–Ω (–¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞—Ç–∞ –Ω–µ –Ω—É–∂–µ–Ω)")
        stt_service = None

        logger.info("‚úÖ –û—Å–Ω–æ–≤–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        raise


@app.get("/")
async def root():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏"""
    return {
        "status": "ok",
        "service": "AI Secretary Orchestrator",
        "endpoints": {
            "health": "/health",
            "process_call": "/process_call (POST)",
            "tts": "/tts (POST)",
            "stt": "/stt (POST)",
            "chat": "/chat (POST)",
        }
    }


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤"""
    services_status = {
        "voice_clone": voice_service is not None,
        "stt": stt_service is not None,
        "llm": llm_service is not None,
    }

    all_ok = all(services_status.values())

    return {
        "status": "healthy" if all_ok else "degraded",
        "services": services_status,
        "timestamp": datetime.now().isoformat()
    }


@app.post("/tts")
async def text_to_speech(request: TTSRequest):
    """
    –°–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏ —Å –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≥–æ–ª–æ—Å–æ–º –õ–∏–¥–∏–∏
    """
    if not voice_service:
        raise HTTPException(status_code=503, detail="Voice service not initialized")

    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        output_file = TEMP_DIR / f"tts_{datetime.now().timestamp()}.wav"

        # –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º
        voice_service.synthesize_to_file(
            text=request.text,
            output_path=str(output_file),
            language=request.language
        )

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∞–π–ª
        return FileResponse(
            path=output_file,
            media_type="audio/wav",
            filename="response.wav"
        )

    except Exception as e:
        logger.error(f"‚ùå TTS Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/stt")
async def speech_to_text(audio: UploadFile = File(...)):
    """
    –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∏–∑ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞
    """
    if not stt_service:
        raise HTTPException(status_code=503, detail="STT service not initialized")

    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        temp_audio = TEMP_DIR / f"stt_{datetime.now().timestamp()}_{audio.filename}"

        with open(temp_audio, "wb") as f:
            content = await audio.read()
            f.write(content)

        # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º
        result = stt_service.transcribe(temp_audio, language="ru")

        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        temp_audio.unlink()

        return {
            "text": result["text"],
            "language": result["language"],
            "segments_count": len(result.get("segments", []))
        }

    except Exception as e:
        logger.error(f"‚ùå STT Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat")
async def chat(request: ConversationRequest):
    """
    –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç LLM (Gemini)
    """
    if not llm_service:
        raise HTTPException(status_code=503, detail="LLM service not initialized")

    try:
        response = llm_service.generate_response(request.text)

        return {
            "response": response,
            "session_id": request.session_id
        }

    except Exception as e:
        logger.error(f"‚ùå LLM Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/process_call")
async def process_call(audio: UploadFile = File(...)):
    """
    –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–≤–æ–Ω–∫–∞:
    1. STT - —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
    2. LLM - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    3. TTS - —Å–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞—É–¥–∏–æ —Å –æ—Ç–≤–µ—Ç–æ–º —Å–µ–∫—Ä–µ—Ç–∞—Ä—è
    """
    call_id = f"call_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    logger.info(f"üìû –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–≤–æ–Ω–∫–∞ {call_id}")

    try:
        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ö–æ–¥—è—â–∏–π –∞—É–¥–∏–æ
        input_audio = CALLS_LOG_DIR / f"{call_id}_input.wav"
        with open(input_audio, "wb") as f:
            content = await audio.read()
            f.write(content)

        # 2. –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Ä–µ—á—å (STT)
        logger.info(f"üéß STT –¥–ª—è {call_id}")
        stt_result = stt_service.transcribe(input_audio, language="ru")
        recognized_text = stt_result["text"]
        logger.info(f"üìù –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {recognized_text}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
        with open(CALLS_LOG_DIR / f"{call_id}_transcript.txt", "w") as f:
            f.write(f"USER: {recognized_text}\n")

        # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç (LLM)
        logger.info(f"ü§ñ LLM –¥–ª—è {call_id}")
        llm_response = llm_service.generate_response(recognized_text)
        logger.info(f"üí¨ –û—Ç–≤–µ—Ç: {llm_response}")

        # –î–æ–ø–æ–ª–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
        with open(CALLS_LOG_DIR / f"{call_id}_transcript.txt", "a") as f:
            f.write(f"ASSISTANT: {llm_response}\n")

        # 4. –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç (TTS)
        logger.info(f"üéôÔ∏è  TTS –¥–ª—è {call_id}")
        output_audio = CALLS_LOG_DIR / f"{call_id}_output.wav"
        voice_service.synthesize_to_file(
            text=llm_response,
            output_path=str(output_audio),
            language="ru"
        )

        logger.info(f"‚úÖ –ó–≤–æ–Ω–æ–∫ {call_id} –æ–±—Ä–∞–±–æ—Ç–∞–Ω")

        # 5. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∞—É–¥–∏–æ –æ—Ç–≤–µ—Ç
        return FileResponse(
            path=output_audio,
            media_type="audio/wav",
            filename=f"{call_id}_response.wav",
            headers={
                "X-Call-ID": call_id,
                "X-Recognized-Text": recognized_text,
                "X-Response-Text": llm_response
            }
        )

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–≤–æ–Ω–∫–∞ {call_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/reset_conversation")
async def reset_conversation():
    """–°–±—Ä–æ—Å –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞"""
    if llm_service:
        llm_service.reset_conversation()
        return {"status": "ok", "message": "Conversation history reset"}
    raise HTTPException(status_code=503, detail="LLM service not initialized")


# ============== OpenAI-Compatible Endpoints for OpenWebUI ==============

@app.get("/v1/models")
@app.get("/v1/models/")
async def list_models():
    """OpenAI-compatible models list for OpenWebUI"""
    return {
        "object": "list",
        "data": [
            {
                "id": "lidia-gemini",
                "object": "model",
                "created": 1700000000,
                "owned_by": "ai-secretary",
                "permission": [],
                "root": "lidia-gemini",
                "parent": None
            },
            {
                "id": "lidia-voice",
                "object": "model",
                "created": 1700000000,
                "owned_by": "ai-secretary",
                "permission": [],
                "root": "lidia-voice",
                "parent": None
            }
        ]
    }


@app.get("/v1/voices")
async def list_voices():
    """List available voices"""
    return {
        "voices": [
            {"voice_id": "lidia", "name": "–õ–∏–¥–∏—è", "language": "ru"}
        ]
    }


@app.post("/v1/audio/speech")
async def openai_speech(request: OpenAISpeechRequest):
    """
    OpenAI-compatible TTS endpoint for OpenWebUI integration
    POST /v1/audio/speech
    """
    if not voice_service:
        raise HTTPException(status_code=503, detail="Voice service not initialized")

    try:
        output_file = TEMP_DIR / f"speech_{datetime.now().timestamp()}.wav"

        voice_service.synthesize_to_file(
            text=request.input,
            output_path=str(output_file),
            language="ru"
        )

        return FileResponse(
            path=output_file,
            media_type="audio/wav",
            filename="speech.wav"
        )

    except Exception as e:
        logger.error(f"‚ùå OpenAI TTS Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """
    OpenAI-compatible chat completions endpoint for OpenWebUI
    Supports both streaming and non-streaming responses
    """
    if not llm_service:
        raise HTTPException(status_code=503, detail="LLM service not initialized")

    logger.info(f"üí¨ Chat completions request: stream={request.stream}, messages={len(request.messages)}")

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Pydantic –º–æ–¥–µ–ª–∏ –≤ dict
    messages = [{"role": m.role, "content": m.content} for m in request.messages]

    if request.stream:
        # Streaming response (SSE)
        async def generate_stream():
            created = int(time.time())
            chunk_id = f"chatcmpl-{created}"

            try:
                for text_chunk in llm_service.generate_response_from_messages(messages, stream=True):
                    chunk_data = {
                        "id": chunk_id,
                        "object": "chat.completion.chunk",
                        "created": created,
                        "model": request.model,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": text_chunk},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"

                # Final chunk
                final_chunk = {
                    "id": chunk_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                yield "data: [DONE]\n\n"

            except Exception as e:
                logger.error(f"‚ùå Streaming error: {e}")
                error_chunk = {
                    "error": {"message": str(e), "type": "server_error"}
                }
                yield f"data: {json.dumps(error_chunk)}\n\n"

        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            }
        )
    else:
        # Non-streaming response
        try:
            response_text = llm_service.generate_response_from_messages(messages, stream=False)

            # Consume generator if it returns one
            if hasattr(response_text, '__iter__') and not isinstance(response_text, str):
                response_text = "".join(response_text)

            return {
                "id": f"chatcmpl-{int(time.time())}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": request.model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response_text
                    },
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0
                }
            }
        except Exception as e:
            logger.error(f"‚ùå Chat completions error: {e}")
            raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv()
    port = int(os.getenv("ORCHESTRATOR_PORT", 8002))
    logger.info(f"üéØ –ó–∞–ø—É—Å–∫ Orchestrator –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    uvicorn.run(
        "orchestrator:app",
        host="0.0.0.0",
        port=port,
        reload=False,
        log_level="info"
    )
