# CLI-OpenAI Bridge Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# Server Configuration
# =============================================================================

BRIDGE_HOST=0.0.0.0
BRIDGE_PORT=8000
BRIDGE_DEBUG=false

# =============================================================================
# Authentication (Optional)
# =============================================================================
# If set, authentication is required:
#   - API endpoints: Authorization: Bearer <api_key>
#   - Dashboard: HTTP Basic Auth (any username, api_key as password)
#   - Public (no auth): /, /health, /docs

# BRIDGE_API_KEY=your-secret-api-key

# =============================================================================
# CLI Paths
# =============================================================================
# Paths to CLI executables (if not in system PATH)

CLAUDE_CLI_PATH=claude
GEMINI_CLI_PATH=gemini
GPT_CLI_PATH=sgpt

# =============================================================================
# Timeouts (seconds)
# =============================================================================

# Global timeouts (used as defaults)
CLI_TIMEOUT=300
STREAM_TIMEOUT=600

# Per-provider timeouts (override global if set)
# CLAUDE_TIMEOUT=300
# GEMINI_TIMEOUT=300
# GPT_TIMEOUT=300

# =============================================================================
# Retry Configuration
# =============================================================================

RETRY_ENABLED=true
RETRY_MAX_ATTEMPTS=3
RETRY_DELAY=1.0
RETRY_BACKOFF=2.0

# =============================================================================
# Response Caching
# =============================================================================
# Cache identical non-streaming requests

CACHE_ENABLED=false
CACHE_TTL=3600
CACHE_MAX_SIZE=1000

# =============================================================================
# Rate Limiting
# =============================================================================
# Token bucket rate limiting per API key or IP

RATE_LIMIT_ENABLED=false
RATE_LIMIT_REQUESTS=60
RATE_LIMIT_WINDOW=60

# =============================================================================
# Request Queue
# =============================================================================
# Manages concurrent CLI requests per provider to prevent overload

QUEUE_ENABLED=true
QUEUE_MAX_SIZE=50
QUEUE_TIMEOUT=300

# Per-provider concurrency limits
QUEUE_DEFAULT_CONCURRENT=2
QUEUE_CLAUDE_CONCURRENT=2
QUEUE_GEMINI_CONCURRENT=3
QUEUE_GPT_CONCURRENT=2

# =============================================================================
# CLI Permission Levels
# =============================================================================
# Control what operations each CLI provider can perform
# Levels:
#   - chat: NO local operations - pure LLM text completion only
#           (no file access, no shell, no web - ideal for Aider/Cursor/LLM clients)
#   - readonly: Only read operations (view files, search)
#   - edit: Read + file edits, no shell commands or web access
#   - full: All operations allowed (default) - no restrictions

CLAUDE_PERMISSION_LEVEL=full
GEMINI_PERMISSION_LEVEL=full
GPT_PERMISSION_LEVEL=full

# =============================================================================
# Custom Model Lists
# =============================================================================
# Override default model lists (comma-separated). If empty, uses built-in defaults.
# Useful for adding new models or removing ones you don't have access to.

# CLAUDE_MODELS=sonnet,opus,haiku,claude-opus-4-5-20251101,claude-sonnet-4-5-20250929
# GEMINI_MODELS=gemini-3-flash-preview,gemini-2.5-pro,gemini-2.5-flash
# GPT_MODELS=gpt-4o,gpt-4o-mini,o1,o3-mini

# =============================================================================
# Conversation Summarization
# =============================================================================
# For providers without native session support (Gemini, GPT), long conversations
# can be summarized to reduce token usage. Older messages are compressed into
# a summary while keeping recent messages verbatim.
# Note: Claude has native session support (--resume), so summarization is skipped.

SUMMARIZE_ENABLED=false
SUMMARIZE_THRESHOLD=50
SUMMARIZE_KEEP_RECENT=4
# Provider for summarization: "auto" (same as request) or specific: claude, gemini, gpt
SUMMARIZE_PROVIDER=auto

# =============================================================================
# Metrics
# =============================================================================

METRICS_ENABLED=true

# =============================================================================
# Logging
# =============================================================================

LOG_LEVEL=INFO
LOG_REQUESTS=true

# =============================================================================
# Health Checks
# =============================================================================

HEALTH_CHECK_ON_STARTUP=true

# Hide models from CLIs that are not available (default: false)
# When enabled, /v1/models only returns models from accessible CLIs
HIDE_UNAVAILABLE_MODELS=false

# =============================================================================
# API Keys (Reserved for future direct API fallback)
# =============================================================================
# These are NOT used by the CLI bridge, but reserved for potential
# direct API provider implementations

# ANTHROPIC_API_KEY=
# GOOGLE_API_KEY=
# OPENAI_API_KEY=
